import{_ as l}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as a,c as i,f as o,a as e,b as n,e as d}from"./app-taMZPyBp.js";const t={},s=e("h1",{id:"ollama-user-guide",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#ollama-user-guide","aria-hidden":"true"},"#"),n(" Ollama User Guide")],-1),r=d(`<h2 id="_1-about" tabindex="-1"><a class="header-anchor" href="#_1-about" aria-hidden="true">#</a> 1. About</h2><p>Official Website: https://ollama.com/</p><p>Github: https://github.com/ollama/ollama</p><p>Model Library: https://ollama.com/search</p><p>Model Library Information:</p><ul><li>Model Categories: All/Embedding/Vision/Tools</li><li>Model Sorting: Popular/Newest</li><li>Model Name</li><li>Model Description</li><li>Available Parameter Scales</li><li>Model Size, Unique Identifier, Upload Time, and Download Command for Different Scales and Variants</li><li>Download Count (Indicating Popularity)</li><li>Number of Tags (Different Scales and Variant Versions Available for Download)</li><li>Update Date</li></ul><h2 id="_2-what" tabindex="-1"><a class="header-anchor" href="#_2-what" aria-hidden="true">#</a> 2. What</h2><p>Used for local deployment of open-source large models.</p><p>Developers can test and debug models locally.</p><p>Users can use language models offline to ensure data privacy, fast response times, and complete freedom, all for free.</p><p>Features:</p><ul><li>Simple operation, simplified model deployment and management, with a low usage threshold.</li><li>Supports multiple platforms: macOS, Linux, and Windows.</li><li>The model library is comprehensive and updated quickly. In addition to models in the library, users can download GGUF models from HuggingFace, deploy locally downloaded GGUF model files, or create custom models.</li></ul><h2 id="_3-how-to-use-macos-platform" tabindex="-1"><a class="header-anchor" href="#_3-how-to-use-macos-platform" aria-hidden="true">#</a> 3. How To Use (macOS Platform)</h2><h3 id="_3-1-installing-ollama" tabindex="-1"><a class="header-anchor" href="#_3-1-installing-ollama" aria-hidden="true">#</a> 3.1. Installing Ollama</h3><ol><li>Download from the official website to get Ollama-darwin.zip. Double-click to get Ollama.app, and move it to Applications.</li><li>The first time you open it, you will be prompted to install the command-line tools. Enter the password to authorize, and installation will be complete.</li></ol><h3 id="_3-2-downloading-models" tabindex="-1"><a class="header-anchor" href="#_3-2-downloading-models" aria-hidden="true">#</a> 3.2. Downloading Models</h3><h4 id="_3-2-1-from-the-model-library" tabindex="-1"><a class="header-anchor" href="#_3-2-1-from-the-model-library" aria-hidden="true">#</a> 3.2.1. From the Model Library</h4><ol><li>Find the model you want to download locally from the model library (it will be downloaded to the hidden .ollama folder in the user directory).</li><li>Copy the download command to the command line to start the download (usually a <code>run</code> or <code>pull</code> command).</li></ol><h4 id="_3-2-2-from-huggingface" tabindex="-1"><a class="header-anchor" href="#_3-2-2-from-huggingface" aria-hidden="true">#</a> 3.2.2. From HuggingFace</h4><ol><li>Search for GGUF models on HuggingFace (https://huggingface.co/models).</li><li>Execute the command in the terminal:</li></ol><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>ollama run hf.co/{username}/{repository}
ollama run hf.co/{username}/{repository}:{quantization} 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_3-3-creating-a-custom-model-and-pushing-to-the-model-library" tabindex="-1"><a class="header-anchor" href="#_3-3-creating-a-custom-model-and-pushing-to-the-model-library" aria-hidden="true">#</a> 3.3. Creating a Custom Model and Pushing to the Model Library</h3><ol><li>Create the model file <code>Modelfile</code></li></ol><ul><li>FROM: Base Model <ul><li>FROM ./vicuna-33b.Q4_0.gguf # Ollama supports importing GGUF models in the Modelfile by pointing to the local file path of the GGUF model to import.</li><li>FROM llama3.2</li></ul></li><li>PARAMETER: Parameter Configuration</li><li>SYSTEM: System Prompt</li></ul><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>FROM llama3.2

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# set the system message
SYSTEM &quot;&quot;&quot;
You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.
&quot;&quot;&quot;
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="2"><li>Execute the command to create the custom model</li></ol><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>ollama create my_model_name -f ./Modelfile
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><ol start="3"><li>Run the custom model</li></ol><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>ollama run my_model_name
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><ol start="4"><li><p>Upload to the official model library:</p><ol><li>Register and log in to the official website, where the name will be the namespace for uploading the model (<code>my_namespace</code>).</li><li>Go to the &quot;My models&quot; section and create a new model. The model name should match the custom <code>my_model_name</code>. Follow the prompt to set up the public key in the website Settings.</li><li>Copy the model to the namespace:</li></ol><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>ollama cp my_model_name my_namespace/my_model_name
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><ol start="4"><li>Push the model to the model library:</li></ol><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>ollama push my_namespace/my_model_name
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><ol start="5"><li>Once the upload is successful, if you choose to make the model public, it will appear in the model library.</li></ol></li></ol><h3 id="_3-4-managing-and-using-models" tabindex="-1"><a class="header-anchor" href="#_3-4-managing-and-using-models" aria-hidden="true">#</a> 3.4. Managing and Using Models</h3><ul><li>Add: <ul><li>pull</li><li>run</li><li>cp</li><li>create</li></ul></li><li>Remove: <ul><li>rm</li></ul></li><li>Check: <ul><li>List models: list</li><li>List running models: ps</li><li>Show details: show</li></ul></li><li>Usage: <ul><li>run</li><li>Exit chat: Use Ctrl + d or /bye to exit</li><li>stop</li></ul></li></ul><h3 id="_3-5-more-commands" tabindex="-1"><a class="header-anchor" href="#_3-5-more-commands" aria-hidden="true">#</a> 3.5. More Commands</h3><p>Type <code>ollama</code> in the command window to see all usage options.</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama （和运行桌面应用是一样的）
  create      Create a model from a Modelfile
  show        Show information for a model
  run         Run a model
  stop        Stop a running model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models
  ps          List running models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information

Use &quot;ollama [command] --help&quot; for more information about a command.
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,35);function m(c,u){return a(),i("div",null,[s,o(" more "),r])}const v=l(t,[["render",m],["__file","019_ollama.html.vue"]]);export{v as default};
