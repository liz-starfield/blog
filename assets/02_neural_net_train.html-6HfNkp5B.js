import{_ as e}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as r,c as a,f as n,e as t}from"./app-KJHtdF_7.js";const d={},i=t('<h1 id="神经网络训练要点解读" tabindex="-1"><a class="header-anchor" href="#神经网络训练要点解读" aria-hidden="true">#</a> 神经网络训练要点解读</h1><ul><li><ol><li>整体目标</li></ol></li><li><ol start="2"><li>损失函数：量化模型的有效性</li></ol></li><li><ol start="3"><li>优化算法（梯度下降,gradient descent）：调整模型参数以优化目标函数的算法</li></ol></li><li><ol start="4"><li>超参数</li></ol></li></ul>',2),o=t('<h2 id="_1-整体目标" tabindex="-1"><a class="header-anchor" href="#_1-整体目标" aria-hidden="true">#</a> 1. 整体目标</h2><p>用数据不断调整神经网络模型的参数，使得模型行为更符合预期。</p><h2 id="_2-损失函数-量化模型的有效性" tabindex="-1"><a class="header-anchor" href="#_2-损失函数-量化模型的有效性" aria-hidden="true">#</a> 2. 损失函数：量化模型的有效性</h2><table><thead><tr><th>场景</th><th>最常见损失函数</th><th>特点</th></tr></thead><tbody><tr><td>回归</td><td>平方误差SE，squared error，即预测值与实际值之差的平方</td><td>很容易被优化</td></tr><tr><td>分类</td><td>交叉熵cross-entropy最小化错误率，即预测与实际情况不符的样本比例</td><td>难以直接优化，通常会优化替代目标</td></tr></tbody></table><h2 id="_3-优化算法-梯度下降-gradient-descent-调整模型参数以优化目标函数的算法" tabindex="-1"><a class="header-anchor" href="#_3-优化算法-梯度下降-gradient-descent-调整模型参数以优化目标函数的算法" aria-hidden="true">#</a> 3. 优化算法（梯度下降,gradient descent）：调整模型参数以优化目标函数的算法</h2><p>梯度下降（gradient descent）， 这种方法几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。</p><p>梯度，是对每一个特征求偏导组成的向量</p><p>梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做<strong>小批量随机梯度下降</strong>（minibatch stochastic gradient descent）。</p><h2 id="_4-超参数" tabindex="-1"><a class="header-anchor" href="#_4-超参数" aria-hidden="true">#</a> 4. 超参数</h2><p>|B|表示每个小批量中的样本数，这也称为<strong>批量大小</strong>（batch size）。</p><p>η表示<em>学习率</em>（learning rate）。</p><p>批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。 这些可以调整但不在训练过程中更新的参数称为<strong>超参数</strong>（hyperparameter）。</p><p><strong>调参</strong>（hyperparameter tuning）是选择超参数的过程。 超参数通常是我们根据训练迭代结果来调整的， 而训练迭代结果是在独立的<strong>验证数据集</strong>（validation dataset）上评估得到的。</p><p>深度学习实践者很少会去花费大力气寻找这样一组参数，使得在<strong>训练集</strong>上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为<strong>泛化</strong>（generalization）。</p>',14);function s(h,l){return r(),a("div",null,[i,n(" more "),o])}const p=e(d,[["render",s],["__file","02_neural_net_train.html.vue"]]);export{p as default};
