import{_ as o,a as p,b as i,c as l,d as c}from"./029_training-YPL3rOVm.js";import{_ as r}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as u,o as d,c as m,f as k,a as n,b as s,d as e,e as t}from"./app-cafaW3Tc.js";const h={},g=n("h1",{id:"grpo-unsloth-vllm",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#grpo-unsloth-vllm","aria-hidden":"true"},"#"),s(" GRPO + Unsloth + vLLM")],-1),v=n("ul",null,[n("li",null,"GRPO (Group Relative Policy Optimization): A reinforcement learning method focused on optimizing model performance based on a specific reward function."),n("li",null,"Unsloth: A framework for efficiently fine-tuning large language models."),n("li",null,"vllm: An inference framework optimized for large language models.")],-1),b=n("h2",{id:"_1-grpo",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#_1-grpo","aria-hidden":"true"},"#"),s(" 1. GRPO")],-1),f=n("p",null,"GRPO (Group Relative Policy Optimization)",-1),w={href:"https://arxiv.org/pdf/2402.03300",target:"_blank",rel:"noopener noreferrer"},y={href:"https://arxiv.org/pdf/2501.12948",target:"_blank",rel:"noopener noreferrer"},_=t('<h3 id="_1-1-what-grpo-can-do" tabindex="-1"><a class="header-anchor" href="#_1-1-what-grpo-can-do" aria-hidden="true">#</a> 1.1. What GRPO Can Do</h3><p>GRPO can transform standard models into fully functional inference models.</p><p>The main goal of GRPO is to maximize rewards and learn how answers are derived, rather than simply memorizing and reproducing answers from training data.</p><p>Conventional fine-tuning (without GRPO) only maximizes the prediction probability of the next word, without optimizing for rewards. GRPO optimizes the reward function, not just the next word prediction.</p><p>Initially, a large amount of data was needed to fill in the reasoning process or chain of thought. But GRPO or other reinforcement learning algorithms can guide the model to automatically exhibit reasoning abilities and generate reasoning trajectories, relying on GRPO or other reinforcement learning algorithms to create good reward functions or validators.</p><p>The application scenarios of GRPO are not limited to code or mathematics. Its reasoning process can also enhance tasks such as email automation, database retrieval, legal, and medical tasks, greatly improving accuracy based on the datasets and reward functions used in training!</p><h3 id="_1-2-how-grpo-works" tabindex="-1"><a class="header-anchor" href="#_1-2-how-grpo-works" aria-hidden="true">#</a> 1.2. How GRPO Works</h3><ol><li>For each question-answer pair, the model generates multiple responses as a group (e.g., 8 different responses).</li><li>Each response is scored based on the reward function.</li><li>The average score of the group of responses is calculated as a baseline.</li><li>Each response&#39;s score is compared to the average score, and each response&#39;s advantage value is determined by the difference between its score and the baseline.</li><li>The model is enhanced to favor higher-scoring responses.</li></ol><h3 id="_1-3-grpo-vs-ppo" tabindex="-1"><a class="header-anchor" href="#_1-3-grpo-vs-ppo" aria-hidden="true">#</a> 1.3. GRPO vs PPO</h3><h4 id="_1-3-1-the-double-teacher-dilemma-of-ppo" tabindex="-1"><a class="header-anchor" href="#_1-3-1-the-double-teacher-dilemma-of-ppo" aria-hidden="true">#</a> 1.3.1. The &quot;Double Teacher Dilemma&quot; of PPO</h4><p>In traditional reinforcement learning methods, PPO (Proximal Policy Optimization) is widely used, where the training system requires two &quot;teachers&quot; to work together: the policy model (student) is responsible for generating answers, and the value model (scoring teacher) is responsible for evaluating quality. This architecture has three fundamental flaws:</p><ul><li>Resource consumption black hole: The parameter size of the value model is often comparable to the policy model, requiring additional storage of gradient parameters during training, doubling memory usage.</li><li>Evaluation standard drift: Asynchronous updates of the two models can easily lead to inconsistent &quot;teaching standards.&quot;</li><li>Absolute scoring trap: The absolute score of a single output is difficult to reflect the relative merits of answers. These problems are particularly prominent in complex reasoning tasks. When dealing with multi-step mathematical proofs, traditional methods are like using the same ruler to measure answers of different dimensions, easily causing evaluation bias.</li></ul><h4 id="_1-3-2-grpo-s-improvements-over-ppo" tabindex="-1"><a class="header-anchor" href="#_1-3-2-grpo-s-improvements-over-ppo" aria-hidden="true">#</a> 1.3.2. GRPO&#39;s Improvements Over PPO</h4><p>Imagine you are teaching a student to solve math problems. Traditional methods may require another teacher (value function model) to evaluate the student&#39;s performance. GRPO adopts a smarter approach: letting the student generate multiple answers and then guiding learning by comparing the merits of these answers. This method is not only more intuitive but also greatly improves learning efficiency. It is closer to the human cognitive way of &quot;comparative learning,&quot; where the merits of answers are no longer determined by absolute scores but are generated through group comparison.</p><p>GRPO is a reinforcement learning method developed on the popular PPO (Proximal Policy Optimization). Its biggest innovation is the introduction of the &quot;intra-group relative evaluation&quot; mechanism, while removing the need for a value function model in traditional methods, making the entire training process more efficient and stable.</p><figure><img src="'+o+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_1-4-three-revolutionary-designs-of-grpo" tabindex="-1"><a class="header-anchor" href="#_1-4-three-revolutionary-designs-of-grpo" aria-hidden="true">#</a> 1.4. Three Revolutionary Designs of GRPO</h3><h4 id="_1-4-1-from-decisive-evaluation-mechanism-to-relative-evaluation-mechanism" tabindex="-1"><a class="header-anchor" href="#_1-4-1-from-decisive-evaluation-mechanism-to-relative-evaluation-mechanism" aria-hidden="true">#</a> 1.4.1. From Decisive Evaluation Mechanism to Relative Evaluation Mechanism</h4><p>This relative evaluation mechanism brings three major advantages:</p><ul><li>Evaluation dimension normalization: Automatically eliminates the impact of differences in question difficulty.</li><li>Error compensation effect: Random fluctuations are naturally smoothed in group comparisons.</li><li>Implicit knowledge mining: The model learns implicit rules beyond the scoring standard through comparison.</li></ul><h4 id="_1-4-2-removal-of-the-value-function-model" tabindex="-1"><a class="header-anchor" href="#_1-4-2-removal-of-the-value-function-model" aria-hidden="true">#</a> 1.4.2. Removal of the Value Function Model</h4><ul><li>GRPO&#39;s success validates the &quot;less is more&quot; technical philosophy.</li><li>GRPO, a reinforcement learning technology, efficiently optimizes responses without the need for a value function model, reducing memory and computational costs compared to PPO.</li><li>By replacing complex model inference with simple matrix operations, training speed is increased by 40%, and memory usage is reduced by 55%. This design is especially suitable for training today&#39;s large models with billions of parameters.</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># Traditional PPO advantage calculation</span>
advantage <span class="token operator">=</span> reward <span class="token operator">-</span> value_model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>state<span class="token punctuation">)</span>

<span class="token comment"># GRPO advantage calculation</span>
group_rewards <span class="token operator">=</span> <span class="token punctuation">[</span>r1<span class="token punctuation">,</span> r2<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> rn<span class="token punctuation">]</span>
baseline <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>group_rewards<span class="token punctuation">)</span>
advantages <span class="token operator">=</span> <span class="token punctuation">[</span>r <span class="token operator">-</span> baseline <span class="token keyword">for</span> r <span class="token keyword">in</span> group_rewards<span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>In the GSM8K math benchmark test, models empowered by GRPO showed amazing breakthroughs:</p><figure><img src="`+p+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="_1-4-3-kl-intelligent-constraint" tabindex="-1"><a class="header-anchor" href="#_1-4-3-kl-intelligent-constraint" aria-hidden="true">#</a> 1.4.3. KL Intelligent Constraint</h4><p>GRPO directly integrates the KL divergence constraint into the loss function, creatively solving the &quot;catastrophic forgetting&quot; problem in reinforcement learning.</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>Loss = -E[log(π(a|s)) * A] + β*KL(π||π_ref)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>Where the β parameter is dynamically adjusted through an adaptive algorithm, achieving a subtle balance between exploration and convergence. Experiments show that this design improves the training stability of mathematical reasoning tasks by 70%.</p><h3 id="_1-5-comparison-of-different-llm-training-methods" tabindex="-1"><a class="header-anchor" href="#_1-5-comparison-of-different-llm-training-methods" aria-hidden="true">#</a> 1.5. Comparison of Different LLM Training Methods</h3><ul><li>SFT <ul><li>Standardizes model output format (with reasoning and answer tags).</li><li>Difficult to learn the mathematical rules and meta-thinking behind the data, still only learns the generation probability of the next token.</li><li>Poor generalization ability, somewhat rote memorization.</li></ul></li><li>Traditional RL <ul><li>A large amount of high-quality data containing problem-solving steps and precise reward functions, then training with brute force.</li><li>Has long-cot, belongs to the type with standard answers, so the model completely fits and approaches the long-cot, essentially learning according to the standard answers and problem-solving process of the training data.</li><li>No mutual comparison between multiple answers (no distinction between the merits of answers), somewhat like cramming education.</li></ul></li><li>GRPO <ul><li>Encourages the model to learn the rules behind reasoning in the process of maximizing rewards through trial and error.</li><li>GRPO only looks at the result, the process is explored and attempted by the model itself. There is no standard cot answer (only the final answer), requiring the model to do a lot of exploration to find the optimal cot, so the model has an aha moment, with better generalization.</li><li>Flexible rewards, generating multiple responses for each question, finding the optimal ones, guiding the model towards the optimal direction.</li><li>Doing so results in a very chaotic reasoning format in the first 100+ steps, so R1 first uses long-cot for SFT on the basis of R1-zero, allowing the model&#39;s response to output according to a predetermined template, appropriately reducing some exploration steps, and improving training efficiency.</li><li>Higher upper bound on generalization and reasoning performance.</li></ul></li></ul><h2 id="_2-efficient-grpo-training-with-unsloth" tabindex="-1"><a class="header-anchor" href="#_2-efficient-grpo-training-with-unsloth" aria-hidden="true">#</a> 2. Efficient GRPO Training with Unsloth</h2>`,32),q=n("li",null,"With the help of 15GB of VRAM, Unsloth can transform any model with up to 17B parameters, such as Llama 3.1 (8B), Phi-4 (14B), Mistral (7B), or Qwen2.5 (7B), into an inference model.",-1),x=n("li",null,'In extreme cases, only 5G of VRAM is needed to train your own inference model locally, reaching the "aha" moment (suitable for any model with 1.5B parameters or less).',-1),R=n("li",null,"Previously, GRPO only supported full fine-tuning, Unsloth AI enables compatibility with QLoRA and LoRA.",-1),P=n("li",null,"Unsloth x vLLM: vLLM achieves fast inference, can increase throughput (up to 20 times), allows fine-tuning and inference to occur simultaneously, and magically eliminates the double memory consumption when loading vLLM and Unsloth simultaneously.",-1),L=n("li",null,"Unsloth's new memory-efficient linear kernel for GRPO reduces memory usage by 8 times or more. This cuts 68.5GB of memory while achieving num_generations = 8 and 20K context length with torch.compile, actually faster.",-1),O={href:"https://unsloth.ai/blog/long-context",target:"_blank",rel:"noopener noreferrer"},G=n("li",null,"Unsloth also uses the same GPU/CUDA memory space as the underlying inference engine (vLLM), unlike implementations from other packages, cutting 16GB of VRAM.",-1),M=t('<figure><img src="'+i+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_3-grpo-training-tips" tabindex="-1"><a class="header-anchor" href="#_3-grpo-training-tips" aria-hidden="true">#</a> 3. GRPO Training Tips</h2><ul><li>Wait at least 300 steps to see substantial reward growth.</li><li>Train the model with at least 500 rows of data.</li><li>A standard model with at least 1.5B parameters is required to generate a chain of thought with GRPO; too small a model may not generate a chain of thought.</li><li>For GRPO&#39;s GPU VRAM requirements in QLoRA 4-bit mode, the general rule is that the model parameter size equals the required VRAM size.</li><li>The longer the context length set, the more VRAM is required. LoRA 16-bit will use at least 4 times more VRAM.</li><li>One major advantage of GRPO is that you don&#39;t even need a lot of data. You only need a good reward function/validator, and the longer the training time, the better the model becomes. The reward value increases with the number of training steps.</li><li>Reward functions and validators: <ul><li>Reward function: scoring <ul><li>Correctness verification is not necessary.</li><li>Reward functions can use validators.</li></ul></li><li>Validator: correctness verification <ul><li>Does not score.</li><li>Validators can also execute code to verify logic or syntax and other correctness.</li></ul></li></ul></li><li>There is no single correct way to design reward functions or validators—the possibilities are endless. However, they must be well-designed and meaningful, as poorly designed rewards may inadvertently degrade model performance.</li></ul><h2 id="_4-code-implementation" tabindex="-1"><a class="header-anchor" href="#_4-code-implementation" aria-hidden="true">#</a> 4. Code Implementation</h2><h3 id="_4-1-install-required-libraries" tabindex="-1"><a class="header-anchor" href="#_4-1-install-required-libraries" aria-hidden="true">#</a> 4.1. Install Required Libraries</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token operator">%</span><span class="token operator">%</span>capture <span class="token comment"># Jupyter magic command to capture cell output to avoid displaying lengthy installation processes</span>
<span class="token keyword">import</span> sys<span class="token punctuation">;</span> modules <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>sys<span class="token punctuation">.</span>modules<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># Get the names of all currently loaded modules</span>
<span class="token keyword">for</span> x <span class="token keyword">in</span> modules<span class="token punctuation">:</span> sys<span class="token punctuation">.</span>modules<span class="token punctuation">.</span>pop<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token string">&quot;PIL&quot;</span> <span class="token keyword">in</span> x <span class="token keyword">or</span> <span class="token string">&quot;google&quot;</span> <span class="token keyword">in</span> x <span class="token keyword">else</span> <span class="token boolean">None</span> <span class="token comment"># Remove cached modules related to PIL (Pillow) and google</span>

!pip install unsloth vllm <span class="token comment"># Install unsloth and vllm</span>
!pip install <span class="token operator">-</span><span class="token operator">-</span>upgrade pillow <span class="token comment"># Upgrade pillow</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4-2-load-llama-3-1-8b-instruct-model" tabindex="-1"><a class="header-anchor" href="#_4-2-load-llama-3-1-8b-instruct-model" aria-hidden="true">#</a> 4.2. Load Llama-3.1-8B-Instruct Model</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># Import core libraries</span>
<span class="token keyword">from</span> unsloth <span class="token keyword">import</span> FastLanguageModel <span class="token comment"># Library for efficiently loading models</span>
<span class="token keyword">import</span> torch <span class="token comment"># PyTorch deep learning framework</span>

<span class="token comment"># Model configuration parameters</span>
max_seq_length <span class="token operator">=</span> <span class="token number">1024</span> <span class="token comment"># Maximum input sequence length (affects VRAM usage)</span>
lora_rank <span class="token operator">=</span> <span class="token number">32</span> <span class="token comment"># LoRA rank, the larger the value, the stronger the model capability, but also slower</span>

<span class="token comment"># Load the base model</span>
model<span class="token punctuation">,</span> tokenizer <span class="token operator">=</span> FastLanguageModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_name <span class="token operator">=</span> <span class="token string">&quot;meta-llama/meta-Llama-3.1-8B-Instruct&quot;</span><span class="token punctuation">,</span> <span class="token comment"># Base model is the 8B parameter Llama3 instruction fine-tuned version</span>
    max_seq_length <span class="token operator">=</span> max_seq_length<span class="token punctuation">,</span>
    load_in_4bit <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment"># True for 4bit quantization (VRAM optimization), False for 16bit</span>
    fast_inference <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment"># Enable vLLM accelerated inference</span>
    max_lora_rank <span class="token operator">=</span> lora_rank<span class="token punctuation">,</span> <span class="token comment"># Maximum LoRA rank limit</span>
    gpu_memory_utilization <span class="token operator">=</span> <span class="token number">0.6</span><span class="token punctuation">,</span> <span class="token comment"># GPU VRAM utilization (can be adjusted lower if OOM)</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4-3-inference-before-grpo-training" tabindex="-1"><a class="header-anchor" href="#_4-3-inference-before-grpo-training" aria-hidden="true">#</a> 4.3. Inference Before GRPO Training</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># Create chat template</span>
text <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>apply_chat_template<span class="token punctuation">(</span><span class="token punctuation">[</span> 
    <span class="token punctuation">{</span><span class="token string">&quot;role&quot;</span> <span class="token punctuation">:</span> <span class="token string">&quot;user&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;content&quot;</span> <span class="token punctuation">:</span> <span class="token string">&quot;Calculate pi.&quot;</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span><span class="token punctuation">,</span> 
tokenize <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token comment"># Indicates not to tokenize the input</span>
add_generation_prompt <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># Set sampling parameters</span>
<span class="token keyword">from</span> vllm <span class="token keyword">import</span> SamplingParams
sampling_params <span class="token operator">=</span> SamplingParams<span class="token punctuation">(</span> 
    temperature <span class="token operator">=</span> <span class="token number">0.8</span><span class="token punctuation">,</span>
    top_p <span class="token operator">=</span> <span class="token number">0.95</span><span class="token punctuation">,</span>
    max_tokens <span class="token operator">=</span> <span class="token number">1024</span><span class="token punctuation">,</span> <span class="token comment"># Maximum length of generated text</span>
<span class="token punctuation">)</span>

<span class="token comment"># Generate text</span>
output <span class="token operator">=</span> model<span class="token punctuation">.</span>fast_generate<span class="token punctuation">(</span> 
    <span class="token punctuation">[</span>text<span class="token punctuation">]</span><span class="token punctuation">,</span>
    sampling_params <span class="token operator">=</span> sampling_params<span class="token punctuation">,</span>
    lora_request <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token comment"># Indicates not to use LoRA (low-rank adaptation) request</span>
<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text

output
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4-4-load-and-prepare-dataset" tabindex="-1"><a class="header-anchor" href="#_4-4-load-and-prepare-dataset" aria-hidden="true">#</a> 4.4. Load and Prepare Dataset</h3><p>OpenAI&#39;s GSM8K dataset</p><figure><img src="`+l+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> re <span class="token comment"># Regular expressions</span>
<span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset<span class="token punctuation">,</span> Dataset

SYSTEM_PROMPT <span class="token operator">=</span> <span class="token triple-quoted-string string">&quot;&quot;&quot;
Respond in the following format:
&lt;reasoning&gt;
...
&lt;/reasoning&gt;
&lt;answer&gt;
...
&lt;/answer&gt;
&quot;&quot;&quot;</span>

<span class="token keyword">def</span> <span class="token function">extract_hash_answer</span><span class="token punctuation">(</span>text<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">str</span> <span class="token operator">|</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token string">&quot;####&quot;</span> <span class="token keyword">not</span> <span class="token keyword">in</span> text<span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token boolean">None</span>
    <span class="token keyword">return</span> text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">&quot;####&quot;</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># uncomment middle messages for 1-shot prompting</span>
<span class="token keyword">def</span> <span class="token function">get_gsm8k_questions</span><span class="token punctuation">(</span>split <span class="token operator">=</span> <span class="token string">&quot;train&quot;</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Dataset<span class="token punctuation">:</span>
    data <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">&#39;openai/gsm8k&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;main&#39;</span><span class="token punctuation">)</span><span class="token punctuation">[</span>split<span class="token punctuation">]</span> <span class="token comment"># Load dataset</span>
    data <span class="token operator">=</span> data<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">{</span> <span class="token comment"># Construct as dict format</span>
        <span class="token string">&#39;prompt&#39;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{</span><span class="token string">&#39;role&#39;</span><span class="token punctuation">:</span> <span class="token string">&#39;system&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;content&#39;</span><span class="token punctuation">:</span> SYSTEM_PROMPT<span class="token punctuation">}</span><span class="token punctuation">,</span>
            <span class="token punctuation">{</span><span class="token string">&#39;role&#39;</span><span class="token punctuation">:</span> <span class="token string">&#39;user&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;content&#39;</span><span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">&#39;question&#39;</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
        <span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">&#39;answer&#39;</span><span class="token punctuation">:</span> extract_hash_answer<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token string">&#39;answer&#39;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span> <span class="token comment"># type: ignore</span>
    <span class="token keyword">return</span> data <span class="token comment"># type: ignore</span>

dataset <span class="token operator">=</span> get_gsm8k_questions<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4-5-set-lora-fine-tuning-configuration" tabindex="-1"><a class="header-anchor" href="#_4-5-set-lora-fine-tuning-configuration" aria-hidden="true">#</a> 4.5. Set LoRA Fine-tuning Configuration</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>model <span class="token operator">=</span> FastLanguageModel<span class="token punctuation">.</span>get_peft_model<span class="token punctuation">(</span>
    model<span class="token punctuation">,</span>
    r <span class="token operator">=</span> lora_rank<span class="token punctuation">,</span> 
    target_modules <span class="token operator">=</span> <span class="token punctuation">[</span> <span class="token comment"># Modules to apply LoRA (can remove QKVO if OOM)</span>
        <span class="token string">&quot;q_proj&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;k_proj&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;v_proj&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;o_proj&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;gate_proj&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;up_proj&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;down_proj&quot;</span><span class="token punctuation">,</span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span> 
    lora_alpha <span class="token operator">=</span> lora_rank<span class="token punctuation">,</span> <span class="token comment"># LoRA scaling factor, usually the same as rank, used to adjust the impact of LoRA</span>
    use_gradient_checkpointing <span class="token operator">=</span> <span class="token string">&quot;unsloth&quot;</span><span class="token punctuation">,</span> <span class="token comment"># Apply activation recomputation/gradient checkpointing for long text fine-tuning, set to &quot;unsloth&quot;, possibly referring to a library-specific gradient checkpointing implementation (VRAM optimization technique)</span>
    random_state <span class="token operator">=</span> <span class="token number">3407</span><span class="token punctuation">,</span> <span class="token comment"># Random seed (ensures experiment reproducibility)</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4-6-define-reward-functions" tabindex="-1"><a class="header-anchor" href="#_4-6-define-reward-functions" aria-hidden="true">#</a> 4.6. Define Reward Functions</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># Reward if LLM-generated answer matches the standard answer</span>
<span class="token keyword">def</span> <span class="token function">correctness_reward_func</span><span class="token punctuation">(</span>prompts<span class="token punctuation">,</span> completions<span class="token punctuation">,</span> answer<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">float</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    responses <span class="token operator">=</span> <span class="token punctuation">[</span>completion<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&#39;content&#39;</span><span class="token punctuation">]</span> <span class="token keyword">for</span> completion <span class="token keyword">in</span> completions<span class="token punctuation">]</span> <span class="token comment"># List of LLM-generated results</span>
    q <span class="token operator">=</span> prompts<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&#39;content&#39;</span><span class="token punctuation">]</span> <span class="token comment"># Question</span>
    extracted_responses <span class="token operator">=</span> <span class="token punctuation">[</span>extract_xml_answer<span class="token punctuation">(</span>r<span class="token punctuation">)</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> responses<span class="token punctuation">]</span> <span class="token comment"># List of LLM-generated answers</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;-&#39;</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f&quot;Question:\\n</span><span class="token interpolation"><span class="token punctuation">{</span>q<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f&quot;\\nAnswer:\\n</span><span class="token interpolation"><span class="token punctuation">{</span>answer<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f&quot;\\nResponse:\\n</span><span class="token interpolation"><span class="token punctuation">{</span>responses<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f&quot;\\nExtracted:\\n</span><span class="token interpolation"><span class="token punctuation">{</span>extracted_responses<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token number">2.0</span> <span class="token keyword">if</span> r <span class="token operator">==</span> a <span class="token keyword">else</span> <span class="token number">0.0</span> <span class="token keyword">for</span> r<span class="token punctuation">,</span> a <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>extracted_responses<span class="token punctuation">,</span> answer<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment"># Compare each LLM-generated answer with the standard answer</span>

<span class="token comment"># Reward if LLM-generated answer is a number</span>
<span class="token keyword">def</span> <span class="token function">int_reward_func</span><span class="token punctuation">(</span>completions<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">float</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    responses <span class="token operator">=</span> <span class="token punctuation">[</span>completion<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&#39;content&#39;</span><span class="token punctuation">]</span> <span class="token keyword">for</span> completion <span class="token keyword">in</span> completions<span class="token punctuation">]</span>
    extracted_responses <span class="token operator">=</span> <span class="token punctuation">[</span>extract_xml_answer<span class="token punctuation">(</span>r<span class="token punctuation">)</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> responses<span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token number">0.5</span> <span class="token keyword">if</span> r<span class="token punctuation">.</span>isdigit<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token number">0.0</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> extracted_responses<span class="token punctuation">]</span>

<span class="token comment"># Reward if LLM-generated result meets the format requirements of the system prompt (strict version)</span>
<span class="token keyword">def</span> <span class="token function">strict_format_reward_func</span><span class="token punctuation">(</span>completions<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">float</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;Reward function that checks if the completion has a specific format.&quot;&quot;&quot;</span>
    pattern <span class="token operator">=</span> <span class="token string">r&quot;^&lt;reasoning&gt;\\n.*?\\n&lt;/reasoning&gt;\\n&lt;answer&gt;\\n.*?\\n&lt;/answer&gt;\\n$&quot;</span>
    responses <span class="token operator">=</span> <span class="token punctuation">[</span>completion<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&quot;content&quot;</span><span class="token punctuation">]</span> <span class="token keyword">for</span> completion <span class="token keyword">in</span> completions<span class="token punctuation">]</span>
    matches <span class="token operator">=</span> <span class="token punctuation">[</span>re<span class="token punctuation">.</span><span class="token keyword">match</span><span class="token punctuation">(</span>pattern<span class="token punctuation">,</span> r<span class="token punctuation">)</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> responses<span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token number">0.5</span> <span class="token keyword">if</span> <span class="token keyword">match</span> <span class="token keyword">else</span> <span class="token number">0.0</span> <span class="token keyword">for</span> <span class="token keyword">match</span> <span class="token keyword">in</span> matches<span class="token punctuation">]</span>

<span class="token comment"># Reward if LLM-generated result meets the format requirements of the system prompt (lenient version)</span>
<span class="token keyword">def</span> <span class="token function">soft_format_reward_func</span><span class="token punctuation">(</span>completions<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">float</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;Reward function that checks if the completion has a specific format.&quot;&quot;&quot;</span>
    pattern <span class="token operator">=</span> <span class="token string">r&quot;&lt;reasoning&gt;.*?&lt;/reasoning&gt;\\s*&lt;answer&gt;.*?&lt;/answer&gt;&quot;</span>
    responses <span class="token operator">=</span> <span class="token punctuation">[</span>completion<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&quot;content&quot;</span><span class="token punctuation">]</span> <span class="token keyword">for</span> completion <span class="token keyword">in</span> completions<span class="token punctuation">]</span>
    matches <span class="token operator">=</span> <span class="token punctuation">[</span>re<span class="token punctuation">.</span><span class="token keyword">match</span><span class="token punctuation">(</span>pattern<span class="token punctuation">,</span> r<span class="token punctuation">)</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> responses<span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token number">0.5</span> <span class="token keyword">if</span> <span class="token keyword">match</span> <span class="token keyword">else</span> <span class="token number">0.0</span> <span class="token keyword">for</span> <span class="token keyword">match</span> <span class="token keyword">in</span> matches<span class="token punctuation">]</span>

<span class="token comment"># Reward if LLM-generated result meets the format requirements of the system prompt (reward by point), and the shorter the content between &lt;answer&gt; and &lt;/answer&gt; tags, the higher the reward</span>
<span class="token keyword">def</span> <span class="token function">xmlcount_reward_func</span><span class="token punctuation">(</span>completions<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">float</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    contents <span class="token operator">=</span> <span class="token punctuation">[</span>completion<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&quot;content&quot;</span><span class="token punctuation">]</span> <span class="token keyword">for</span> completion <span class="token keyword">in</span> completions<span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>count_xml<span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token keyword">for</span> c <span class="token keyword">in</span> contents<span class="token punctuation">]</span>

<span class="token keyword">def</span> <span class="token function">count_xml</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">float</span><span class="token punctuation">:</span>
    count <span class="token operator">=</span> <span class="token number">0.0</span>
    <span class="token keyword">if</span> text<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token string">&quot;&lt;reasoning&gt;\\n&quot;</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        count <span class="token operator">+=</span> <span class="token number">0.125</span>
    <span class="token keyword">if</span> text<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token string">&quot;\\n&lt;/reasoning&gt;\\n&quot;</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        count <span class="token operator">+=</span> <span class="token number">0.125</span>
    <span class="token keyword">if</span> text<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token string">&quot;\\n&lt;answer&gt;\\n&quot;</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        count <span class="token operator">+=</span> <span class="token number">0.125</span>
        count <span class="token operator">-=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">&quot;\\n&lt;/answer&gt;\\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.001</span>
    <span class="token keyword">if</span> text<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token string">&quot;\\n&lt;/answer&gt;&quot;</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        count <span class="token operator">+=</span> <span class="token number">0.125</span>
        count <span class="token operator">-=</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">&quot;\\n&lt;/answer&gt;&quot;</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.001</span>
    <span class="token keyword">return</span> count

<span class="token keyword">def</span> <span class="token function">extract_xml_answer</span><span class="token punctuation">(</span>text<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">str</span><span class="token punctuation">:</span>
    answer <span class="token operator">=</span> text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">&quot;&lt;answer&gt;&quot;</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
    answer <span class="token operator">=</span> answer<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">&quot;&lt;/answer&gt;&quot;</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> answer<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4-7-configure-grpo-parameters" tabindex="-1"><a class="header-anchor" href="#_4-7-configure-grpo-parameters" aria-hidden="true">#</a> 4.7. Configure GRPO Parameters</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>max_prompt_length <span class="token operator">=</span> <span class="token number">256</span> <span class="token comment"># Maximum prompt length for model input</span>

<span class="token keyword">from</span> trl <span class="token keyword">import</span> GRPOConfig<span class="token punctuation">,</span> GRPOTrainer

training_args <span class="token operator">=</span> GRPOConfig<span class="token punctuation">(</span>
    learning_rate <span class="token operator">=</span> <span class="token number">5e-6</span><span class="token punctuation">,</span> <span class="token comment"># Learning rate, the step size for optimizer to adjust model parameters at each update</span>
    adam_beta1 <span class="token operator">=</span> <span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token comment"># Beta parameter for Adam optimizer, used to control momentum calculation</span>
    adam_beta2 <span class="token operator">=</span> <span class="token number">0.99</span><span class="token punctuation">,</span> <span class="token comment"># Beta parameter for Adam optimizer, used to control momentum calculation</span>
    weight_decay <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token comment"># Weight decay, used to prevent overfitting by reducing the size of weights at each update</span>
    warmup_ratio <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token comment"># Learning rate warm-up ratio, indicating that the learning rate will gradually increase to the set learning rate at the beginning of training</span>
    lr_scheduler_type <span class="token operator">=</span> <span class="token string">&quot;cosine&quot;</span><span class="token punctuation">,</span> <span class="token comment"># Learning rate scheduler type, set to &quot;cosine,&quot; indicating that the learning rate will gradually decrease according to a cosine function</span>
    optim <span class="token operator">=</span> <span class="token string">&quot;paged_adamw_8bit&quot;</span><span class="token punctuation">,</span> <span class="token comment"># Optimizer, &quot;paged_adamw_8bit&quot; is a variant of the optimizer, possibly used to reduce memory usage</span>
    logging_steps <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment"># Set logging steps to 1, indicating that logs are recorded at every step</span>
    per_device_train_batch_size <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment"># Set the training batch size per device to 1</span>
    gradient_accumulation_steps <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment"># Gradient accumulation steps, can be increased to 4 for smoother training</span>
    num_generations <span class="token operator">=</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token comment"># Number of generations (can be reduced if OOM)</span>
    max_prompt_length <span class="token operator">=</span> max_prompt_length<span class="token punctuation">,</span>
    max_completion_length <span class="token operator">=</span> max_seq_length <span class="token operator">-</span> max_prompt_length<span class="token punctuation">,</span> <span class="token comment"># Maximum completion length, ensuring that the generated text does not exceed the model&#39;s maximum sequence length</span>
    <span class="token comment"># num_train_epochs = 1, # Set to 1 for a full training run</span>
    max_steps <span class="token operator">=</span> <span class="token number">250</span><span class="token punctuation">,</span> <span class="token comment"># Maximum training steps</span>
    save_steps <span class="token operator">=</span> <span class="token number">250</span><span class="token punctuation">,</span> <span class="token comment"># Steps to save the model, indicating that the model is saved every 250 steps</span>
    max_grad_norm <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token comment"># Maximum gradient norm, used for gradient clipping to prevent gradient explosion</span>
    report_to <span class="token operator">=</span> <span class="token string">&quot;none&quot;</span><span class="token punctuation">,</span> <span class="token comment"># Can use Weights &amp; Biases</span>
    output_dir <span class="token operator">=</span> <span class="token string">&quot;outputs&quot;</span><span class="token punctuation">,</span> <span class="token comment"># Output directory for storing training results and model checkpoints</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4-8-define-grpotrainer" tabindex="-1"><a class="header-anchor" href="#_4-8-define-grpotrainer" aria-hidden="true">#</a> 4.8. Define GRPOTrainer</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>trainer <span class="token operator">=</span> GRPOTrainer<span class="token punctuation">(</span>
    model <span class="token operator">=</span> model<span class="token punctuation">,</span> <span class="token comment"># Base model</span>
    processing_class <span class="token operator">=</span> tokenizer<span class="token punctuation">,</span> <span class="token comment"># Embedding model</span>
    reward_funcs <span class="token operator">=</span> <span class="token punctuation">[</span> <span class="token comment"># Reward functions</span>
        xmlcount_reward_func<span class="token punctuation">,</span>
        soft_format_reward_func<span class="token punctuation">,</span>
        strict_format_reward_func<span class="token punctuation">,</span>
        int_reward_func<span class="token punctuation">,</span>
        correctness_reward_func<span class="token punctuation">,</span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span>
    args <span class="token operator">=</span> training_args<span class="token punctuation">,</span> <span class="token comment"># Training parameters</span>
    train_dataset <span class="token operator">=</span> dataset<span class="token punctuation">,</span> <span class="token comment"># Dataset</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4-9-start-grpo-training" tabindex="-1"><a class="header-anchor" href="#_4-9-start-grpo-training" aria-hidden="true">#</a> 4.9. Start GRPO Training</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><figure><img src="`+c+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_4-10-save-lora-weights" tabindex="-1"><a class="header-anchor" href="#_4-10-save-lora-weights" aria-hidden="true">#</a> 4.10. Save LoRA Weights</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>model<span class="token punctuation">.</span>save_lora<span class="token punctuation">(</span><span class="token string">&quot;grpo_saved_lora&quot;</span><span class="token punctuation">)</span> <span class="token comment"># Save LoRA weights to grpo_saved_lora file</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="_4-11-inference-after-grpo-training" tabindex="-1"><a class="header-anchor" href="#_4-11-inference-after-grpo-training" aria-hidden="true">#</a> 4.11. Inference After GRPO Training</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>text <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>apply_chat_template<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token punctuation">{</span><span class="token string">&quot;role&quot;</span> <span class="token punctuation">:</span> <span class="token string">&quot;system&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;content&quot;</span> <span class="token punctuation">:</span> SYSTEM_PROMPT<span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token comment"># Difference from pre-training inference 1: Added system prompt</span>
    <span class="token punctuation">{</span><span class="token string">&quot;role&quot;</span> <span class="token punctuation">:</span> <span class="token string">&quot;user&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;content&quot;</span> <span class="token punctuation">:</span> <span class="token string">&quot;Calculate pi.&quot;</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span><span class="token punctuation">,</span> tokenize <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> add_generation_prompt <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">from</span> vllm <span class="token keyword">import</span> SamplingParams
sampling_params <span class="token operator">=</span> SamplingParams<span class="token punctuation">(</span>
    temperature <span class="token operator">=</span> <span class="token number">0.8</span><span class="token punctuation">,</span>
    top_p <span class="token operator">=</span> <span class="token number">0.95</span><span class="token punctuation">,</span>
    max_tokens <span class="token operator">=</span> <span class="token number">1024</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
output <span class="token operator">=</span> model<span class="token punctuation">.</span>fast_generate<span class="token punctuation">(</span>
    text<span class="token punctuation">,</span>
    sampling_params <span class="token operator">=</span> sampling_params<span class="token punctuation">,</span>
    lora_request <span class="token operator">=</span> model<span class="token punctuation">.</span>load_lora<span class="token punctuation">(</span><span class="token string">&quot;grpo_saved_lora&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># Difference from pre-training inference 2: Load LoRA weights</span>
<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text

output
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_5-references" tabindex="-1"><a class="header-anchor" href="#_5-references" aria-hidden="true">#</a> 5. References</h2><p>https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb</p><p>https://www.kaggle.com/code/kingabzpro/fine-tuning-deepseek-r1-reasoning-model</p><p>https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb</p><p>https://www.51cto.com/aigc/4216.html</p><p>https://unsloth.ai/blog/grpo</p>`,35),z={href:"https://blog.csdn.net/simoncool23/article/details/145400144",target:"_blank",rel:"noopener noreferrer"},T={href:"https://www.cnblogs.com/theseventhson/p/18696408",target:"_blank",rel:"noopener noreferrer"};function A(B,S){const a=u("ExternalLinkIcon");return d(),m("div",null,[g,k(" more "),v,b,f,n("p",null,[s("GRPO was first introduced in "),n("a",w,[s("DeepSeek's Math paper"),e(a)]),s(" from February to April 2024, and later utilized by DeepSeek in creating "),n("a",y,[s("DeepSeek R1"),e(a)]),s(" as described in their paper.")]),_,n("ul",null,[q,x,R,P,n("li",null,[s("Unsloth cleverly reduces VRAM usage by over 90% compared to standard implementations (HuggingFace TRL + Flash Attention 2), significantly optimizing. For example, with a 20K context length, generating 8 times per prompt, Unsloth uses only 54.3GB of VRAM for Llama 3.1 8B, while the standard implementation requires 510.8GB (Unsloth saves 90% of VRAM). "),n("ul",null,[L,n("li",null,[s("Unsloth uses the "),n("a",O,[s("smart Unsloth gradient checkpoint algorithm"),e(a)]),s(", intelligently offloading intermediate activations to system memory asynchronously, slowing down by only 1%. Since we need num_generations = 8, up to 372GB of VRAM can be saved. We can further reduce this memory usage through intermediate gradient accumulation.")]),G])])]),M,n("p",null,[n("a",z,[s("What is the GRPO technology behind the popular Deepseek"),e(a)])]),n("p",null,[n("a",T,[s("LLM Large Model: Shallow Analysis of Deepseek (Part 2): The Principle of R1's GRPO"),e(a)])])])}const E=r(h,[["render",A],["__file","029_unsloth_grpo.html.vue"]]);export{E as default};
