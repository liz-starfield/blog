const e=JSON.parse('{"key":"v-a1ef33a6","path":"/zh/posts/llm/015_fine_tune.html","title":"微调","lang":"zh-CN","frontmatter":{"icon":"lightbulb","sidebar":false,"date":"2024-11-05T00:00:00.000Z","prev":"./016_multimodal","next":"./014_rag_evaluation","category":["LLM"],"tag":["Fine-tuning"],"description":"微调 模型微调流程 LoRA Llama-factory 基础开源模型 MoE 混合专家模型 RLHF 基于人类反馈的强化学习","head":[["link",{"rel":"alternate","hreflang":"en-us","href":"https://liz-in-tech.github.io/blog/posts/llm/015_fine_tune.html"}],["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/blog/zh/posts/llm/015_fine_tune.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"微调"}],["meta",{"property":"og:description","content":"微调 模型微调流程 LoRA Llama-factory 基础开源模型 MoE 混合专家模型 RLHF 基于人类反馈的强化学习"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:locale:alternate","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-11-13T05:26:25.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:tag","content":"Fine-tuning"}],["meta",{"property":"article:published_time","content":"2024-11-05T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-11-13T05:26:25.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"微调\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-11-05T00:00:00.000Z\\",\\"dateModified\\":\\"2024-11-13T05:26:25.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. 微调 Fine-tuning","slug":"_1-微调-fine-tuning","link":"#_1-微调-fine-tuning","children":[{"level":3,"title":"1.1. 指令微调/有监督微调","slug":"_1-1-指令微调-有监督微调","link":"#_1-1-指令微调-有监督微调","children":[]},{"level":3,"title":"1.2. 多任务微调 Multi-tasking FT","slug":"_1-2-多任务微调-multi-tasking-ft","link":"#_1-2-多任务微调-multi-tasking-ft","children":[]}]},{"level":2,"title":"2. 模型微调流程","slug":"_2-模型微调流程","link":"#_2-模型微调流程","children":[]},{"level":2,"title":"3. 数据构造","slug":"_3-数据构造","link":"#_3-数据构造","children":[]},{"level":2,"title":"4. 微调策略分类","slug":"_4-微调策略分类","link":"#_4-微调策略分类","children":[{"level":3,"title":"4.1. 基于微调范围：全量微调和部分参数微调","slug":"_4-1-基于微调范围-全量微调和部分参数微调","link":"#_4-1-基于微调范围-全量微调和部分参数微调","children":[]},{"level":3,"title":"4.2. 基于任务：SFT，RLHF，RLAIF","slug":"_4-2-基于任务-sft-rlhf-rlaif","link":"#_4-2-基于任务-sft-rlhf-rlaif","children":[]},{"level":3,"title":"4.3. 低资源微调","slug":"_4-3-低资源微调","link":"#_4-3-低资源微调","children":[]},{"level":3,"title":"4.4. deepspeed","slug":"_4-4-deepspeed","link":"#_4-4-deepspeed","children":[]}]},{"level":2,"title":"5. LoRA 与 QLoRA","slug":"_5-lora-与-qlora","link":"#_5-lora-与-qlora","children":[{"level":3,"title":"5.1. LoRA","slug":"_5-1-lora","link":"#_5-1-lora","children":[]},{"level":3,"title":"5.2. QLoRA","slug":"_5-2-qlora","link":"#_5-2-qlora","children":[]}]},{"level":2,"title":"6. 微调实践","slug":"_6-微调实践","link":"#_6-微调实践","children":[{"level":3,"title":"6.1. 3个关键部分","slug":"_6-1-3个关键部分","link":"#_6-1-3个关键部分","children":[]},{"level":3,"title":"6.2. 微调高级设置","slug":"_6-2-微调高级设置","link":"#_6-2-微调高级设置","children":[]},{"level":3,"title":"6.3. github: tloen/alpaca-lora","slug":"_6-3-github-tloen-alpaca-lora","link":"#_6-3-github-tloen-alpaca-lora","children":[]},{"level":3,"title":"6.4. LLaMA-Factory","slug":"_6-4-llama-factory","link":"#_6-4-llama-factory","children":[]},{"level":3,"title":"6.5. More","slug":"_6-5-more","link":"#_6-5-more","children":[]}]},{"level":2,"title":"7. 开源模型","slug":"_7-开源模型","link":"#_7-开源模型","children":[{"level":3,"title":"7.1. Mistral-7B","slug":"_7-1-mistral-7b","link":"#_7-1-mistral-7b","children":[]}]},{"level":2,"title":"8. 混合专家模型 MoE，Mixture of Experts","slug":"_8-混合专家模型-moe-mixture-of-experts","link":"#_8-混合专家模型-moe-mixture-of-experts","children":[]},{"level":2,"title":"9. RLHF 基于人类反馈的强化学习","slug":"_9-rlhf-基于人类反馈的强化学习","link":"#_9-rlhf-基于人类反馈的强化学习","children":[]},{"level":2,"title":"10. References","slug":"_10-references","link":"#_10-references","children":[]}],"git":{"createdTime":1731475585000,"updatedTime":1731475585000,"contributors":[{"name":"unknown","email":"15721607377@163.com","commits":1}]},"readingTime":{"minutes":11.63,"words":3489},"filePathRelative":"zh/posts/llm/015_fine_tune.md","localizedDate":"2024年11月5日","excerpt":"<h1> 微调</h1>\\n<ul>\\n<li>模型微调流程</li>\\n<li>LoRA</li>\\n<li>Llama-factory</li>\\n<li>基础开源模型</li>\\n<li>MoE 混合专家模型</li>\\n<li>RLHF 基于人类反馈的强化学习</li>\\n</ul>\\n","autoDesc":true}');export{e as data};
