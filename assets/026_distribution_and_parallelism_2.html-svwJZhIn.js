import{_ as i,a as o,b as r,c as p,d as l,e as c,f as u,g as d,h as m,i as k,j as h,k as g,l as v,m as b,n as f}from"./026_zero3_3-skr0K_0Q.js";import{_}from"./024_mixed_precision_training_list-dG4PWcKt.js";import{_ as y}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as w,o as q,c as z,f as P,a,b as n,d as e,e as t}from"./app-MbMw1XaM.js";const x={},D=a("h1",{id:"distributed-training-part-3-data-parallelism",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#distributed-training-part-3-data-parallelism","aria-hidden":"true"},"#"),n(" Distributed Training Part 3: Data Parallelism")],-1),O=t('<h2 id="_1-overview-of-data-parallelism-dp" tabindex="-1"><a class="header-anchor" href="#_1-overview-of-data-parallelism-dp" aria-hidden="true">#</a> 1. Overview of Data Parallelism (DP)</h2><p>Data parallelism (DP)</p><ul><li>Naive DP (naive DDP approach)</li><li>DP Optimization 1: Attach an all-reduce hook function to each parameter</li><li>DP Optimization 2: Bucketing gradients</li><li>ZeRO (Zero Redundancy Optimizer) <ul><li>ZeRO-1: optimizer state partitioning</li><li>ZeRO-2: optimizer state + gradient partitioning</li><li>ZeRO-3 / FSDP (Fully-Sharded Data Parallelism): optimizer state + gradient + parameter partitioning</li></ul></li></ul><h2 id="_2-data-parallelism-dp" tabindex="-1"><a class="header-anchor" href="#_2-data-parallelism-dp" aria-hidden="true">#</a> 2. Data Parallelism (DP)</h2><p>Different micro-batches are processed in parallel on different GPUs (only gradient computation on micro-batch, model parameters are updated once per global batch)</p><ul><li>A copy of the model instance is replicated on each GPU</li><li>Multiple GPUs compute forward and backward passes of micro-batches in parallel, then compute gradients for each micro-batch</li><li>Gradients of micro-batches are accumulated (averaged) -&gt; all-reduce</li><li>The optimizer updates model parameters using the average gradient</li></ul><figure><img src="'+i+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Involved distributed communication</p><ul><li>all-reduce: Used here to average gradients computed on different GPUs and update each GPU&#39;s gradients to the average, thus synchronizing gradients</li></ul><p>Gradient computation and synchronization</p><ul><li>Gradient computation (GPU computation): Forward and backward gradient computation</li><li>Gradient synchronization (GPU communication): Trigger distributed communication all-reduce operation for gradient synchronization</li></ul><h3 id="_2-1-naive-dp-naive-ddp-approach" tabindex="-1"><a class="header-anchor" href="#_2-1-naive-dp-naive-ddp-approach" aria-hidden="true">#</a> 2.1. Naive DP (naive DDP approach)</h3><p>Principle: Perform GPU communication after GPU computation is complete, then wait for GPU communication to complete before proceeding with GPU computation</p><ul><li>After each GPU completes gradient computation, trigger a distributed communication all-reduce operation for gradient synchronization, then wait for gradient synchronization to complete before the optimizer updates parameters</li></ul><figure><img src="'+o+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Improvement space: During communication, GPUs are idle, and we should try to overlap communication and computation as much as possible to make them occur simultaneously</p><h3 id="_2-2-dp-optimization" tabindex="-1"><a class="header-anchor" href="#_2-2-dp-optimization" aria-hidden="true">#</a> 2.2. DP Optimization</h3><p>Overlap gradient computation&#39;s backward pass and gradient synchronization</p><ul><li>DP Optimization 1: Once a parameter&#39;s gradient is computed on multiple GPUs, start the all-reduce operation for that parameter&#39;s gradient</li><li>DP Optimization 2: Once a layer&#39;s gradient is computed on multiple GPUs, start the all-reduce operation for that layer&#39;s gradient</li></ul><h4 id="_2-2-1-dp-optimization-1-attach-an-all-reduce-hook-function-to-each-parameter" tabindex="-1"><a class="header-anchor" href="#_2-2-1-dp-optimization-1-attach-an-all-reduce-hook-function-to-each-parameter" aria-hidden="true">#</a> 2.2.1. DP Optimization 1: Attach an all-reduce hook function to each parameter</h4><p>Principle: Do not wait for all layers&#39; backward passes to complete before starting gradient synchronization. Instead, start synchronizing gradients of computed layers as soon as they are available, significantly speeding up data parallelism and reducing the wait time for entire model gradient synchronization (e.g., in llama, start synchronizing the gradient of layer 32 while GPU is still computing the gradient of layer 31)</p><p>Note: Optimization 1 does not even wait for a decoding layer&#39;s gradient to be fully computed before starting gradient synchronization. Instead, it starts synchronizing a parameter&#39;s gradient as soon as it is computed on each GPU</p><figure><img src="'+r+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>PyTorch implementation: Attach an all-reduce hook function to each parameter. Once a parameter&#39;s gradient is ready, it immediately triggers the all-reduce operation, even if other parameters&#39; gradients are still being computed</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">register_backward_hook</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hook<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Registers a backward hook for all parameters of the model that 
    require gradients.
    &quot;&quot;&quot;</span>
    <span class="token keyword">for</span> p <span class="token keyword">in</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> p<span class="token punctuation">.</span>requires_grad <span class="token keyword">is</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
            p<span class="token punctuation">.</span>register_post_accumulate_grad_hook<span class="token punctuation">(</span>hook<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-2-2-dp-optimization-2-bucketing-gradients" tabindex="-1"><a class="header-anchor" href="#_2-2-2-dp-optimization-2-bucketing-gradients" aria-hidden="true">#</a> 2.2.2. DP Optimization 2: Bucketing gradients</h4><p>Preliminary theory: GPU operations are generally more efficient when processing large tensors than performing multiple operations on many small tensors. This is also true for communication operations.</p><p>Principle: Bucket gradients (e.g., bucket by layer, treating each decoding layer as a whole), and initiate a single all-reduce operation for all gradients in the same bucket, rather than performing an all-reduce operation for each parameter&#39;s gradient individually. By performing a single all-reduce operation per bucket, we can significantly reduce communication overhead and speed up the communication process.</p><figure><img src="`+p+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_2-3-dp-practice" tabindex="-1"><a class="header-anchor" href="#_2-3-dp-practice" aria-hidden="true">#</a> 2.3. DP Practice</h3><p>Terminology</p><ul><li>bs: batch size</li><li>mbs: micro batch size</li><li>gbs: global batch size</li><li>grad_acc: the number of gradient accumulation steps (refers to serially computing several mbs on a single GPU when parallelism is not possible, accumulating these serial computations)</li><li>dp: the number of parallel instances used for data parallelism</li><li>gbst: global batch size tokens</li></ul><p>Formula</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>bs = gbs = mbs * grad_acc * dp
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>gbst = batch_size * sequence_length 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>Initial data parallelism plan</p><ul><li><ol><li>Determine the optimal global batch size tokens (gbst)</li></ol><ul><li>Determine through literature review or experiments (measuring model convergence)</li></ul></li><li><ol start="2"><li>Choose the training sequence length</li></ol><ul><li>Also determined through literature review or experiments</li><li>Typically, 2-8k tokens are reliable for our evaluations today</li></ul></li><li><ol start="3"><li>Find the maximum local batch size (mbs)</li></ol><ul><li>Can be found by continuously increasing the local batch size on a single GPU (mbs) until memory is exhausted</li></ul></li><li><ol start="4"><li>Determine the number of GPUs available for data parallelism (DP)</li></ol><ul><li>The value of gbs/dp will tell us how many gradient accumulation steps are needed to achieve the desired gbs.</li></ul></li></ul><p>Specific case</p><ul><li>gbst (tokens) = 4M tokens</li><li>sequence_length=4K tokens</li><li>bs=1024 samples</li><li>observe assume: a single GPU can only fit mbs=2 in memory (meaning a single GPU can only accommodate 2 samples with a sequence length of 4k tokens at a time), mbs needs to be less than or equal to 2</li><li>we have 128 GPUs available for training</li><li>This means with 4 gradient accumulation steps we&#39;ll achieve our goal of 1024 samples or 4M tokens per training step. <ul><li>grad_acc = bs / (dp * mbs) = 1024 / (128 * 2) = 4</li></ul></li><li>Now what if we suddenly have 512 GPUs available? We can achieve the same GBS and thus identical training by keeping MBS=2 and setting gradient accumulation steps to 1 and achieve faster training! <ul><li>grad_acc = bs / (dp * mbs) = 1024 / (512 * 2) = 1</li></ul></li></ul><h3 id="_2-4-performance-with-dp" tabindex="-1"><a class="header-anchor" href="#_2-4-performance-with-dp" aria-hidden="true">#</a> 2.4. Performance with DP</h3><figure><img src="`+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>As DP parallelism increases</p><ul><li>Throughput decreases</li><li>Memory usage remains stable</li></ul><p>The prerequisite for using this is that mbs is at least 1, meaning a GPU can support at least one input sample&#39;s forward pass, but this is not always the case, even with activation recomputation</p><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Tip: Quick estimation of the minimum memory required for model parameters: model parameters * 2 (e.g., 70B → 140GB (=133GiB))</p><p>For extremely large models or larger batch token sizes, do we have other options?</p><ul><li>There are two main splitting methods: parallelism (tensor parallelism, context parallelism, or pipeline parallelism) and sharding (DeepSpeed Zero or PyTorch FSDP). These two methods are somewhat independent and can actually be combined!</li></ul><h2 id="_3-deepspeed-zero" tabindex="-1"><a class="header-anchor" href="#_3-deepspeed-zero" aria-hidden="true">#</a> 3. DeepSpeed ZeRO</h2><h3 id="_3-1-zero" tabindex="-1"><a class="header-anchor" href="#_3-1-zero" aria-hidden="true">#</a> 3.1. ZeRO</h3>',50),G={href:"https://www.deepspeed.ai/tutorials/zero/",target:"_blank",rel:"noopener noreferrer"},T=t('<p>ZeRO</p><ul><li>(<strong>Ze</strong>ro <strong>R</strong>edundancy <strong>O</strong>ptimizer)</li><li>A memory optimization technique aimed at reducing memory redundancy in LLM training</li><li>ZeRO reduces memory redundancy by partitioning optimizer states, gradients, and parameters along the <strong>data parallel DP dimension</strong>, which sometimes requires more GPU communication that may or may not overlap with GPU computation</li><li>When we mention &quot;partitioning,&quot; it refers to partitioning along the DP axis, as ZeRO is part of data parallelism</li></ul><p>Three optimization strategies of ZeRO</p><ul><li>ZeRO-1: optimizer state partitioning</li><li>ZeRO-2: optimizer state + gradient partitioning</li><li>ZeRO-3 / FSDP (also called FSDP for &quot;Fully-Sharded Data Parallelism&quot;): optimizer state + gradient + parameter partitioning</li></ul><figure><img src="'+u+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Note: k=12 for Adam</p><p>Note: Why not shard activations? Because each DP receives different micro-batches, the activations they compute are naturally different, they are not redundant, not shared or duplicated data, so they do not need synchronization or sharding, and they have already been used to compute the gradients of each DP rank, completing their main task, and are no longer needed afterward</p><p>Memory usage of different DP strategies:</p><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Limitations of ZeRO:</p><p>Data parallelism (DP) only works properly when a single layer of the model can fit on a single GPU, and ZeRO can only partition parameters, gradients, and optimizer states, not activation memory! We recall from the previous discussion on activation memory that this part of activation memory increases linearly with sequence length and batch size. Naturally, we can cope by limiting sequence length and batch size, but in practice, we do not want to train with shorter sequence lengths due to hardware limitations.</p><h3 id="_3-2-review-of-mixed-precision-training" tabindex="-1"><a class="header-anchor" href="#_3-2-review-of-mixed-precision-training" aria-hidden="true">#</a> 3.2. Review of Mixed Precision Training</h3><p>Summary of known methods for mixed precision training:</p><figure><img src="'+_+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Number of parameters: Ψ</p><ul><li>BF16+FP32 mixed precision baseline: 2Ψ + 6Ψ + 12Ψ = 20Ψ <ul><li>Model parameters (half precision): 2 bytes</li><li>Gradients (half precision) + FP32 gradients (accumulated in FP32 precision): 2 + 4 = 6 bytes</li><li>FP32 model parameters and optimizer states: 4 + (4 + 4) = 12 bytes</li></ul></li><li>BF16+FP32 mixed precision without FP32 gradients: 2Ψ + 2Ψ + 12Ψ = 16Ψ <ul><li>Model parameters (half precision): 2 bytes</li><li>Gradients (half precision): 2 bytes</li><li>FP32 model parameters and optimizer states: 4 + (4 + 4) = 12 bytes</li></ul></li></ul><h3 id="_3-3-zero-1" tabindex="-1"><a class="header-anchor" href="#_3-3-zero-1" aria-hidden="true">#</a> 3.3. ZeRO-1</h3><figure><img src="'+m+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+k+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Execution process of a single training step</p><ul><li>Forward pass: Each replica holds the same complete model parameters (BF16) but processes different micro-batches (although parameters are the same, the activations and gradients computed will differ due to different micro-batches)</li><li>Backward pass: Each replica computes complete gradients, but due to different micro-batches, each replica&#39;s gradients are different</li><li>Perform reduce-scatter operation on gradients: Each replica accumulates only the part of the gradient corresponding to the optimizer state shard, other parts of the gradient remain unchanged</li><li>Local optimization: Each replica performs parameter update steps on its local optimizer state, updating only the part of the parameters corresponding to the optimizer state shard (converted back to BF16 after updating from FP32), other parameters remain unchanged</li><li>Perform all-gather operation on parameters: Collect the updated parameter (BF16) parts from each replica so that each replica has the complete updated parameters</li></ul><p>Compared to naive DP</p><ul><li>Gradient accumulation changes from naive DP&#39;s all-reduce operation to reduce-scatter operation (reduce-scatter is 2 times faster than all-reduce!)</li><li>Added all-gather operation after optimizer step</li></ul><p>Further optimization of ZeRO: Overlap all-gather with preceding and following operations</p><ul><li>Overlap all-gather with optimizer step: Start all-gather operation after optimizer updates part of the parameters</li><li>Overlap all-gather with forward pass: Start forward pass of a layer after all-gather completes parameter collection for that layer</li></ul><h3 id="_3-4-zero-2" tabindex="-1"><a class="header-anchor" href="#_3-4-zero-2" aria-hidden="true">#</a> 3.4. ZeRO-2</h3><figure><img src="'+h+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_3-5-zero-3" tabindex="-1"><a class="header-anchor" href="#_3-5-zero-3" aria-hidden="true">#</a> 3.5. ZeRO-3</h3><p>ZeRO-3 is called FSDP (Fully Shared Data Parallelism) in PyTorch implementation</p><figure><img src="'+v+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+b+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+f+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Only gather model parameters when needed</p><ul><li>Perform all-gather operation to collect all parameters before each layer&#39;s forward pass, and flush unnecessary parameters from memory after each layer&#39;s forward pass (32 times from layer 1 to layer 32)</li><li>Perform all-gather operation to collect all parameters before each layer&#39;s backward pass, and flush unnecessary parameters from memory after computing gradients for each layer&#39;s backward pass (32 times from layer 32 to layer 1)</li><li>Optimization: Combine forward and backward passes of layer 32, perform all-gather operation to collect all parameters before layer 32&#39;s forward pass, and flush unnecessary parameters from memory after computing gradients for layer 32&#39;s backward pass</li><li>Therefore, the total number of all-gather operations and flushing unnecessary parameters from memory is num_layers + num_layers - 1 = 32 + 32 - 1 = 63 times, as shown in the third figure with some latency overhead</li><li>ZeRO-3 heavily relies on parameter communication</li></ul><h2 id="_4-extended-links" tabindex="-1"><a class="header-anchor" href="#_4-extended-links" aria-hidden="true">#</a> 4. Extended Links</h2>',36),F={href:"https://siboehm.com/articles/22/data-parallel-training",target:"_blank",rel:"noopener noreferrer"},R={href:"https://www.harmdevries.com/post/context-length/",target:"_blank",rel:"noopener noreferrer"},U=t(`<h2 id="_5-code-implementation" tabindex="-1"><a class="header-anchor" href="#_5-code-implementation" aria-hidden="true">#</a> 5. Code Implementation</h2><h3 id="_5-1-naive-dp-implementation-with-overlap-in-picotron" tabindex="-1"><a class="header-anchor" href="#_5-1-naive-dp-implementation-with-overlap-in-picotron" aria-hidden="true">#</a> 5.1. Naive DP implementation with overlap in Picotron</h3><p>Complete code</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">DataParallelNaive</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Naive Data Parallelism. Not used in practice. But it is a good starting point to understand how data parallelism works.
    It implements a simple all-reduce operation to synchronize gradients across multiple processes.
    And \`no_sync\` context manager to disable gradient synchronization.
    &quot;&quot;&quot;</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> module<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Initializes the DataParallel wrapper for a given module.

        Args:
            module (nn.Module): The model to be wrapped for data parallelism.
            process_group (torch.distributed.ProcessGroup): The process group used for gradient synchronization. 
                                                            It could be a data parallel or context parallel group.
        &quot;&quot;&quot;</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>module <span class="token operator">=</span> module
        self<span class="token punctuation">.</span>require_backward_grad_sync <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token comment"># whether to synchronize gradients during backward pass. Set to False when using gradient accumulation</span>
        self<span class="token punctuation">.</span>register_backward_hook<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_allreduce_grads<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>module<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">register_backward_hook</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hook<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Registers a backward hook for all parameters of the model that require gradients.    
        &quot;&quot;&quot;</span>
        <span class="token keyword">for</span> p <span class="token keyword">in</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> p<span class="token punctuation">.</span>requires_grad <span class="token keyword">is</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
                p<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span>hook<span class="token punctuation">)</span>
                
    <span class="token keyword">def</span> <span class="token function">_allreduce_grads</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> grad<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Performs an all-reduce operation to synchronize gradients across multiple processes.    
        &quot;&quot;&quot;</span>
        <span class="token comment"># No synchronization needed during gradient accumulation, except at the final accumulation step.</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>require_backward_grad_sync<span class="token punctuation">:</span>
            dist<span class="token punctuation">.</span>all_reduce<span class="token punctuation">(</span>grad<span class="token punctuation">,</span> op<span class="token operator">=</span>dist<span class="token punctuation">.</span>ReduceOp<span class="token punctuation">.</span>SUM<span class="token punctuation">,</span> group<span class="token operator">=</span>pgm<span class="token punctuation">.</span>process_group_manager<span class="token punctuation">.</span>cp_dp_group<span class="token punctuation">)</span>
            grad <span class="token operator">/=</span> pgm<span class="token punctuation">.</span>process_group_manager<span class="token punctuation">.</span>cp_dp_world_size
        <span class="token keyword">return</span> grad 
    
    <span class="token decorator annotation punctuation">@contextlib<span class="token punctuation">.</span>contextmanager</span>
    <span class="token keyword">def</span> <span class="token function">no_sync</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        A context manager to temporarily disable gradient synchronization. 
        This is useful for performing multiple backward passes during gradient accumulation without synchronizing 
        gradients in between.
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>require_backward_grad_sync <span class="token operator">=</span> <span class="token boolean">False</span>
        <span class="token keyword">yield</span>
        self<span class="token punctuation">.</span>require_backward_grad_sync <span class="token operator">=</span> <span class="token boolean">True</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_5-2-bucket-dp-implementation-in-picotron" tabindex="-1"><a class="header-anchor" href="#_5-2-bucket-dp-implementation-in-picotron" aria-hidden="true">#</a> 5.2. Bucket DP implementation in Picotron</h3><p>Complete code</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">DataParallelBucket</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Data Parallelism with gradient grouped into buckets to reduce the communication overhead.
    &quot;&quot;&quot;</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> module<span class="token punctuation">,</span> bucket_cap_mb<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">,</span> grad_type <span class="token operator">=</span> torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Initialize the DataParallelBucket module.
        
        Args:
            module (nn.Module): The model to be parallelized.
            process_group: The process group for gradient synchronization, which can be either 
                           a data parallel group or a context parallel group.
            bucket_cap_mb (int, optional): The maximum size of each gradient synchronization bucket in megabytes. 
                                           Defaults to 25 MB.
            grad_type (torch.dtype, optional): The data type of gradients, defaulting to float32.
        &quot;&quot;&quot;</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>module <span class="token operator">=</span> module
        self<span class="token punctuation">.</span>require_backward_grad_sync <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token comment"># whether to synchronize gradients during backward pass. Set to False when using gradient accumulation</span>
        grad_size <span class="token operator">=</span> <span class="token number">2</span> <span class="token keyword">if</span> grad_type <span class="token operator">==</span> torch<span class="token punctuation">.</span>bfloat16 <span class="token keyword">else</span> <span class="token number">4</span> <span class="token comment"># float32 gradient: 4 bytes</span>
        bucket_size <span class="token operator">=</span> bucket_cap_mb <span class="token operator">*</span> <span class="token number">1024</span> <span class="token operator">*</span> <span class="token number">1024</span> <span class="token operator">//</span> grad_size <span class="token comment"># number of gradients in one bucket</span>
        self<span class="token punctuation">.</span>bucket_manager <span class="token operator">=</span> BucketManager<span class="token punctuation">(</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> pgm<span class="token punctuation">.</span>process_group_manager<span class="token punctuation">.</span>cp_dp_group<span class="token punctuation">,</span> bucket_size<span class="token punctuation">,</span> grad_type<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_backward_hook<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_post_backward_callback_set <span class="token operator">=</span> <span class="token boolean">False</span> <span class="token comment"># whether the callback for wait gradient synchronization is set</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>module<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_tensor<span class="token punctuation">,</span> output_tensor<span class="token punctuation">,</span> output_tensor_grad<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> output_tensor<span class="token punctuation">,</span> output_tensor_grad<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">register_backward_hook</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Registers a backward hook to manually accumulate and synchronize gradients.
        
        This hook serves two main purposes:
        1. PyTorch does not natively support gradient accumulation with mixed precision.
        2. After gradient accumulation, it flags parameters as ready for synchronization.
        
        The gradient accumulation functions are stored to prevent them from going out of scope.
        
        References:
        - https://github.com/NVIDIA/Megatron-LM/issues/690
        - https://pytorch.org/docs/stable/generated/torch.autograd.graph.Node.register_hook.html
        - https://arxiv.org/abs/2006.15704 (page 5)
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>grad_accs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad<span class="token punctuation">:</span>
                <span class="token comment"># Expand so we get access to grad_fn.</span>
                param_tmp <span class="token operator">=</span> param<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>param<span class="token punctuation">)</span>
                <span class="token comment"># Get the gradient accumulator function.</span>
                grad_acc_fn <span class="token operator">=</span> param_tmp<span class="token punctuation">.</span>grad_fn<span class="token punctuation">.</span>next_functions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
                grad_acc_fn<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_make_param_hook<span class="token punctuation">(</span>param<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bucket_manager<span class="token punctuation">)</span><span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>grad_accs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>grad_acc_fn<span class="token punctuation">)</span>
                
    <span class="token keyword">def</span> <span class="token function">_make_param_hook</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> param<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">,</span>bucket_manager<span class="token punctuation">:</span> BucketManager<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Creates the a hook for each parameter to handle gradient accumulation and synchronization.
        &quot;&quot;&quot;</span>
        <span class="token keyword">def</span> <span class="token function">param_hook</span><span class="token punctuation">(</span><span class="token operator">*</span>unused<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token triple-quoted-string string">&quot;&quot;&quot;
            The hook called after the gradient is ready. It performs the following:
            1. Accumulates the gradient into the main gradient.
            2. Adds a post-backward callback to wait for gradient synchronization completion.
            3. Marks the parameter as ready for synchronization.
            &quot;&quot;&quot;</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad<span class="token punctuation">:</span>
                <span class="token keyword">assert</span> param<span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
                param<span class="token punctuation">.</span>main_grad<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>param<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">)</span> <span class="token comment"># accumulate the gradients</span>
                param<span class="token punctuation">.</span>grad <span class="token operator">=</span> <span class="token boolean">None</span>
                
                <span class="token comment"># skip the gradient synchronization (gradient accumulation/PP micro batches)</span>
                <span class="token keyword">if</span> self<span class="token punctuation">.</span>require_backward_grad_sync<span class="token punctuation">:</span>
                    <span class="token comment"># Add a callback to wait for gradient synchronization. Ensures the callback is added only once.</span>
                    <span class="token comment"># Callback is executed after the backward pass. It should be added per backward pass.</span>
                    <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>_post_backward_callback_set<span class="token punctuation">:</span>
                        Variable<span class="token punctuation">.</span>_execution_engine<span class="token punctuation">.</span>queue_callback<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_post_backward<span class="token punctuation">)</span>
                        self<span class="token punctuation">.</span>_post_backward_callback_set <span class="token operator">=</span> <span class="token boolean">True</span>
                        
                    <span class="token comment"># mark the parameter as ready for gradient synchronization. </span>
                    bucket_manager<span class="token punctuation">.</span>mark_param_as_ready<span class="token punctuation">(</span>param<span class="token punctuation">)</span> 
        <span class="token keyword">return</span> param_hook
    
    <span class="token decorator annotation punctuation">@contextlib<span class="token punctuation">.</span>contextmanager</span>
    <span class="token keyword">def</span> <span class="token function">no_sync</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;A context manager to disable gradient synchronization.&quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>require_backward_grad_sync <span class="token operator">=</span> <span class="token boolean">False</span>
        <span class="token keyword">yield</span>
        self<span class="token punctuation">.</span>require_backward_grad_sync <span class="token operator">=</span> <span class="token boolean">True</span>
        
    <span class="token keyword">def</span> <span class="token function">_post_backward</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        A post-backward callback that waits for gradient synchronization to finish, then copies 
        the synchronized gradients back to the parameters&#39; grad attribute.
        
        This method is called after the backward pass and before the optimizer step.
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>bucket_manager<span class="token punctuation">.</span>wait<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_post_backward_callback_set <span class="token operator">=</span> <span class="token boolean">False</span>
        <span class="token comment"># copy to params.grad so we can use the optimizer to update the parameters</span>
        <span class="token keyword">for</span> p <span class="token keyword">in</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> p<span class="token punctuation">.</span>requires_grad<span class="token punctuation">:</span>
                p<span class="token punctuation">.</span>grad <span class="token operator">=</span> p<span class="token punctuation">.</span>main_grad<span class="token punctuation">.</span>to<span class="token punctuation">(</span>p<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span> <span class="token comment"># In PyTorch, you cannot assign a gradient with one data type to a tensor of another data type.</span>

    <span class="token keyword">def</span> <span class="token function">reset</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Reset the bucket manager and zero out gradients in the model
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>bucket_manager<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span> </code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,7);function B(Z,N){const s=w("ExternalLinkIcon");return q(),z("div",null,[D,P(" more "),O,a("p",null,[n("DeepSpeed ZeRO official documentation: "),a("a",G,[n("https://www.deepspeed.ai/tutorials/zero/"),e(s)])]),T,a("p",null,[a("a",F,[n("https://siboehm.com/articles/22/data-parallel-training"),e(s)])]),a("p",null,[a("a",R,[n("https://www.harmdevries.com/post/context-length/"),e(s)])]),U])}const C=y(x,[["render",B],["__file","026_distribution_and_parallelism_2.html.vue"]]);export{C as default};
