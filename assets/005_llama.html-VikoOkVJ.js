import{_ as s,a,b as t,c as p,d as e,e as o,f as c,g as i}from"./005_inheritance-RaLAnK7d.js";import{_ as l}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as u,c as r,f as k,e as n}from"./app-taMZPyBp.js";const d={},v=n('<h1 id="llama-source-code-exploration" tabindex="-1"><a class="header-anchor" href="#llama-source-code-exploration" aria-hidden="true">#</a> Llama Source Code Exploration</h1><ul><li><ol><li>About</li></ol></li><li><ol start="2"><li>Llama Overall Architecture</li></ol></li><li><ol start="3"><li>Hyperparameters</li></ol></li><li><ol start="4"><li>Tensor Dimensionality Transformation</li></ol></li><li><ol start="5"><li>Number of Trainable Parameters</li></ol></li><li><ol start="6"><li>Source Code</li></ol></li></ul>',2),m=n('<h2 id="_1-about" tabindex="-1"><a class="header-anchor" href="#_1-about" aria-hidden="true">#</a> 1. About</h2><p>Source：https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct</p><p>Date：2024.04.18</p><p>Company：Meta</p><p>Source Code：</p><p>https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py</p><h2 id="_2-llama-overall-architecture" tabindex="-1"><a class="header-anchor" href="#_2-llama-overall-architecture" aria-hidden="true">#</a> 2. Llama Overall Architecture</h2><figure><img src="'+s+'" alt="Llama Overall Architecture" tabindex="0" loading="lazy"><figcaption>Llama Overall Architecture</figcaption></figure><figure><img src="'+a+'" alt="Source Code Corresponding to Model Architecture" tabindex="0" loading="lazy"><figcaption>Source Code Corresponding to Model Architecture</figcaption></figure><figure><img src="'+t+'" alt="Llama vs Transformer" tabindex="0" loading="lazy"><figcaption>Llama vs Transformer</figcaption></figure><h2 id="_3-hyperparameters" tabindex="-1"><a class="header-anchor" href="#_3-hyperparameters" aria-hidden="true">#</a> 3. Hyperparameters</h2><figure><img src="'+p+'" alt="Hyperparameters" tabindex="0" loading="lazy"><figcaption>Hyperparameters</figcaption></figure><h2 id="_4-tensor-dimensionality-transformation" tabindex="-1"><a class="header-anchor" href="#_4-tensor-dimensionality-transformation" aria-hidden="true">#</a> 4. Tensor Dimensionality Transformation</h2><figure><img src="'+e+'" alt="Tensor Dimension Transformation" tabindex="0" loading="lazy"><figcaption>Tensor Dimension Transformation</figcaption></figure><figure><img src="'+o+'" alt="Tensor Dimension Transformation Details" tabindex="0" loading="lazy"><figcaption>Tensor Dimension Transformation Details</figcaption></figure><h2 id="_5-number-of-trainable-parameters" tabindex="-1"><a class="header-anchor" href="#_5-number-of-trainable-parameters" aria-hidden="true">#</a> 5. Number of Trainable Parameters</h2><figure><img src="'+c+`" alt="Number of Trainable Parameters" tabindex="0" loading="lazy"><figcaption>Number of Trainable Parameters</figcaption></figure><h2 id="_6-source-code" tabindex="-1"><a class="header-anchor" href="#_6-source-code" aria-hidden="true">#</a> 6. Source Code</h2><h3 id="_6-1-entrance" tabindex="-1"><a class="header-anchor" href="#_6-1-entrance" aria-hidden="true">#</a> 6.1. Entrance</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># Download the vocabulary file tokenizer.json from the model_id path and instantiate the tokenizer class</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&quot;&quot;&quot;
The main steps are divided into two parts:
1.Download the configuration file config.json from the model_id path and instantiate the LlamaConfig class
2.Download the model-related information from the model_id path and instantiate the LlamaForCausalLM class
&quot;&quot;&quot;</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_id<span class="token punctuation">,</span>
    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16<span class="token punctuation">,</span>
    device_map<span class="token operator">=</span><span class="token string">&quot;auto&quot;</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># prompt</span>
messages <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{</span><span class="token string">&quot;role&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;system&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;content&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;You are a pirate chatbot who always responds in pirate speak!&quot;</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span><span class="token string">&quot;role&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;user&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;content&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;Who are you?&quot;</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>

<span class="token comment"># Convert messages into tokens</span>
input_ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>apply_chat_template<span class="token punctuation">(</span>
    messages<span class="token punctuation">,</span>
    add_generation_prompt<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    return_tensors<span class="token operator">=</span><span class="token string">&quot;pt&quot;</span>
<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>model<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

terminators <span class="token operator">=</span> <span class="token punctuation">[</span>
    tokenizer<span class="token punctuation">.</span>eos_token_id<span class="token punctuation">,</span>
    tokenizer<span class="token punctuation">.</span>convert_tokens_to_ids<span class="token punctuation">(</span><span class="token string">&quot;&lt;|eot_id|&gt;&quot;</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span>

<span class="token comment"># GenerationMixin&#39;s generate</span>
<span class="token comment"># Generation Strategy：*multinomial sampling* if \`num_beams=1\` and \`do_sample=True\`</span>
outputs <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>
    input_ids<span class="token punctuation">,</span>
    max_new_tokens<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>
    eos_token_id<span class="token operator">=</span>terminators<span class="token punctuation">,</span>
    do_sample<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    temperature<span class="token operator">=</span><span class="token number">0.6</span><span class="token punctuation">,</span> <span class="token comment"># The value used to modulate the next token probabilities.defaults to 1.0</span>
    top_p<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token comment"># defaults to 1.0</span>
<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">)</span>
response <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>input_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment"># Retrieve the part of the outputs that excludes the original output of input_ids (prompt)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>response<span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># Convert tokens back into characters, ignoring special tokens</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-2-generationmixin" tabindex="-1"><a class="header-anchor" href="#_6-2-generationmixin" aria-hidden="true">#</a> 6.2. GenerationMixin</h3><figure><img src="`+i+`" alt="Inherit Relation of LlamaForCausalLM and GenerationMixin" tabindex="0" loading="lazy"><figcaption>Inherit Relation of LlamaForCausalLM and GenerationMixin</figcaption></figure><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">GenerationMixin</span><span class="token punctuation">:</span>

    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">generate</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        inputs<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        generation_config<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>GenerationConfig<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        logits_processor<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>LogitsProcessorList<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        stopping_criteria<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>StoppingCriteriaList<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        prefix_allowed_tokens_fn<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Callable<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        synced_gpus<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        assistant_model<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token string">&quot;PreTrainedModel&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        streamer<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token string">&quot;BaseStreamer&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        negative_prompt_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        negative_prompt_attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Union<span class="token punctuation">[</span>GenerateOutput<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span><span class="token punctuation">:</span>

        <span class="token comment"># 13. run sample</span>
        result <span class="token operator">=</span> self<span class="token punctuation">.</span>_sample<span class="token punctuation">(</span>
            input_ids<span class="token punctuation">,</span>
            logits_processor<span class="token operator">=</span>prepared_logits_processor<span class="token punctuation">,</span>
            logits_warper<span class="token operator">=</span>prepared_logits_warper<span class="token punctuation">,</span>
            stopping_criteria<span class="token operator">=</span>prepared_stopping_criteria<span class="token punctuation">,</span>
            generation_config<span class="token operator">=</span>generation_config<span class="token punctuation">,</span>
            synced_gpus<span class="token operator">=</span>synced_gpus<span class="token punctuation">,</span>
            streamer<span class="token operator">=</span>streamer<span class="token punctuation">,</span>
            <span class="token operator">**</span>model_kwargs<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_sample</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        input_ids<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">,</span>
        logits_processor<span class="token punctuation">:</span> LogitsProcessorList<span class="token punctuation">,</span>
        stopping_criteria<span class="token punctuation">:</span> StoppingCriteriaList<span class="token punctuation">,</span>
        generation_config<span class="token punctuation">:</span> GenerationConfig<span class="token punctuation">,</span>
        synced_gpus<span class="token punctuation">:</span> <span class="token builtin">bool</span><span class="token punctuation">,</span>
        streamer<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token string">&quot;BaseStreamer&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        logits_warper<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>LogitsProcessorList<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        <span class="token operator">**</span>model_kwargs<span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Union<span class="token punctuation">[</span>GenerateNonBeamOutput<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span><span class="token punctuation">:</span>

        <span class="token keyword">while</span> self<span class="token punctuation">.</span>_has_unfinished_sequences<span class="token punctuation">(</span>this_peer_finished<span class="token punctuation">,</span> synced_gpus<span class="token punctuation">,</span> device<span class="token operator">=</span>input_ids<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">:</span>            

            <span class="token comment"># forward pass to get next token</span>
            outputs <span class="token operator">=</span> self<span class="token punctuation">(</span>
                <span class="token operator">**</span>model_inputs<span class="token punctuation">,</span>
                return_dict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span>
                output_hidden_states<span class="token operator">=</span>output_hidden_states<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

            next_token_logits <span class="token operator">=</span> outputs<span class="token punctuation">.</span>logits<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>

            next_token_scores <span class="token operator">=</span> logits_warper<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> next_token_scores<span class="token punctuation">)</span>

            <span class="token comment"># token selection        </span>
            probs <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>next_token_scores<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            next_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>multinomial<span class="token punctuation">(</span>probs<span class="token punctuation">,</span> num_samples<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>

            input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>input_ids<span class="token punctuation">,</span> next_tokens<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> GenerateDecoderOnlyOutput<span class="token punctuation">(</span>
                    sequences<span class="token operator">=</span>input_ids<span class="token punctuation">,</span>
                    scores<span class="token operator">=</span>scores<span class="token punctuation">,</span>
                    logits<span class="token operator">=</span>raw_logits<span class="token punctuation">,</span>
                    attentions<span class="token operator">=</span>decoder_attentions<span class="token punctuation">,</span>
                    hidden_states<span class="token operator">=</span>decoder_hidden_states<span class="token punctuation">,</span>
                    past_key_values<span class="token operator">=</span>model_kwargs<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">&quot;past_key_values&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-3-llamaforcausallm" tabindex="-1"><a class="header-anchor" href="#_6-3-llamaforcausallm" aria-hidden="true">#</a> 6.3. LlamaForCausalLM</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LlamaForCausalLM</span><span class="token punctuation">(</span>LlamaPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> LlamaModel<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> config<span class="token punctuation">.</span>vocab_size
        self<span class="token punctuation">.</span>lm_head <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

        <span class="token comment"># Initialize weights and apply final processing</span>
        self<span class="token punctuation">.</span>post_init<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        input_ids<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>LongTensor <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        past_key_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>Cache<span class="token punctuation">,</span> List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        inputs_embeds<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        labels<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        output_attentions<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        output_hidden_states<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        return_dict<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        cache_position<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Union<span class="token punctuation">[</span>Tuple<span class="token punctuation">,</span> CausalLMOutputWithPast<span class="token punctuation">]</span><span class="token punctuation">:</span>

        <span class="token comment"># decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)</span>
        outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>
            input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">,</span>
            attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
            position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
            inputs_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span>
            use_cache<span class="token operator">=</span>use_cache<span class="token punctuation">,</span>
            output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span>
            output_hidden_states<span class="token operator">=</span>output_hidden_states<span class="token punctuation">,</span>
            return_dict<span class="token operator">=</span>return_dict<span class="token punctuation">,</span>
            cache_position<span class="token operator">=</span>cache_position<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

        hidden_states <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

        logits <span class="token operator">=</span> self<span class="token punctuation">.</span>lm_head<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

        <span class="token keyword">return</span> CausalLMOutputWithPast<span class="token punctuation">(</span>
            loss<span class="token operator">=</span>loss<span class="token punctuation">,</span>
            logits<span class="token operator">=</span>logits<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>outputs<span class="token punctuation">.</span>past_key_values<span class="token punctuation">,</span>
            hidden_states<span class="token operator">=</span>outputs<span class="token punctuation">.</span>hidden_states<span class="token punctuation">,</span>
            attentions<span class="token operator">=</span>outputs<span class="token punctuation">.</span>attentions<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-4-llamamodel" tabindex="-1"><a class="header-anchor" href="#_6-4-llamamodel" aria-hidden="true">#</a> 6.4. LlamaModel</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LlamaModel</span><span class="token punctuation">(</span>LlamaPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> LlamaConfig<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>padding_idx <span class="token operator">=</span> config<span class="token punctuation">.</span>pad_token_id
        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> config<span class="token punctuation">.</span>vocab_size

        self<span class="token punctuation">.</span>embed_tokens <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>padding_idx<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>
            <span class="token punctuation">[</span>LlamaDecoderLayer<span class="token punctuation">(</span>config<span class="token punctuation">,</span> layer_idx<span class="token punctuation">)</span> <span class="token keyword">for</span> layer_idx <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>num_hidden_layers<span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> LlamaRMSNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>rms_norm_eps<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gradient_checkpointing <span class="token operator">=</span> <span class="token boolean">False</span>

        <span class="token comment"># Initialize weights and apply final processing</span>
        self<span class="token punctuation">.</span>post_init<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        input_ids<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>LongTensor <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        past_key_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>Cache<span class="token punctuation">,</span> List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        inputs_embeds<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        output_attentions<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        output_hidden_states<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        return_dict<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        cache_position<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Union<span class="token punctuation">[</span>Tuple<span class="token punctuation">,</span> BaseModelOutputWithPast<span class="token punctuation">]</span><span class="token punctuation">:</span>

        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>embed_tokens<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>

        <span class="token keyword">for</span> decoder_layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
            layer_outputs <span class="token operator">=</span> decoder_layer<span class="token punctuation">(</span>
                    hidden_states<span class="token punctuation">,</span>
                    attention_mask<span class="token operator">=</span>causal_mask<span class="token punctuation">,</span>
                    position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">,</span>
                    past_key_value<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
                    output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span>
                    use_cache<span class="token operator">=</span>use_cache<span class="token punctuation">,</span>
                    cache_position<span class="token operator">=</span>cache_position<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
        
        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>layer_outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 

        <span class="token keyword">return</span> BaseModelOutputWithPast<span class="token punctuation">(</span>
            last_hidden_state<span class="token operator">=</span>hidden_states<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>next_cache<span class="token punctuation">,</span>
            hidden_states<span class="token operator">=</span>all_hidden_states<span class="token punctuation">,</span>
            attentions<span class="token operator">=</span>all_self_attns<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-5-llamadecoderlayer" tabindex="-1"><a class="header-anchor" href="#_6-5-llamadecoderlayer" aria-hidden="true">#</a> 6.5. LlamaDecoderLayer</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LlamaDecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> LlamaConfig<span class="token punctuation">,</span> layer_idx<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size

        self<span class="token punctuation">.</span>self_attn <span class="token operator">=</span> LLAMA_ATTENTION_CLASSES<span class="token punctuation">[</span>config<span class="token punctuation">.</span>_attn_implementation<span class="token punctuation">]</span><span class="token punctuation">(</span>config<span class="token operator">=</span>config<span class="token punctuation">,</span> layer_idx<span class="token operator">=</span>layer_idx<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> LlamaMLP<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>input_layernorm <span class="token operator">=</span> LlamaRMSNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>rms_norm_eps<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>post_attention_layernorm <span class="token operator">=</span> LlamaRMSNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>rms_norm_eps<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>
        attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        past_key_value<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Cache<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        output_attentions<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        cache_position<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>

        residual <span class="token operator">=</span> hidden_states

        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>input_layernorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

        <span class="token comment"># Self Attention</span>
        hidden_states<span class="token punctuation">,</span> self_attn_weights<span class="token punctuation">,</span> present_key_value <span class="token operator">=</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>
            hidden_states<span class="token operator">=</span>hidden_states<span class="token punctuation">,</span>
            attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
            position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">,</span>
            past_key_value<span class="token operator">=</span>past_key_value<span class="token punctuation">,</span>
            output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span>
            use_cache<span class="token operator">=</span>use_cache<span class="token punctuation">,</span>
            cache_position<span class="token operator">=</span>cache_position<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

        hidden_states <span class="token operator">=</span> residual <span class="token operator">+</span> hidden_states

        <span class="token comment"># Fully Connected</span>
        residual <span class="token operator">=</span> hidden_states
        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
        hidden_states <span class="token operator">=</span> residual <span class="token operator">+</span> hidden_states

        outputs <span class="token operator">=</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>
            outputs <span class="token operator">+=</span> <span class="token punctuation">(</span>present_key_value<span class="token punctuation">,</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> outputs
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-6-llamarmsnorm" tabindex="-1"><a class="header-anchor" href="#_6-6-llamarmsnorm" aria-hidden="true">#</a> 6.6. LlamaRMSNorm</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LlamaRMSNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        LlamaRMSNorm is equivalent to T5LayerNorm
        &quot;&quot;&quot;</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>variance_epsilon <span class="token operator">=</span> eps

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_states<span class="token punctuation">)</span><span class="token punctuation">:</span>
        input_dtype <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>dtype
        hidden_states <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
        variance <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        hidden_states <span class="token operator">=</span> hidden_states <span class="token operator">*</span> torch<span class="token punctuation">.</span>rsqrt<span class="token punctuation">(</span>variance <span class="token operator">+</span> self<span class="token punctuation">.</span>variance_epsilon<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>weight <span class="token operator">*</span> hidden_states<span class="token punctuation">.</span>to<span class="token punctuation">(</span>input_dtype<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-7-llamasdpaattention" tabindex="-1"><a class="header-anchor" href="#_6-7-llamasdpaattention" aria-hidden="true">#</a> 6.7. LlamaSdpaAttention</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LlamaSdpaAttention</span><span class="token punctuation">(</span>LlamaAttention<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from
    \`LlamaAttention\` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to
    SDPA API.
    &quot;&quot;&quot;</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> LlamaConfig<span class="token punctuation">,</span> layer_idx<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config
        self<span class="token punctuation">.</span>layer_idx <span class="token operator">=</span> layer_idx
        self<span class="token punctuation">.</span>attention_dropout <span class="token operator">=</span> config<span class="token punctuation">.</span>attention_dropout
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> config<span class="token punctuation">.</span>num_attention_heads
        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> self<span class="token punctuation">.</span>hidden_size <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads
        self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">=</span> config<span class="token punctuation">.</span>num_key_value_heads
        self<span class="token punctuation">.</span>num_key_value_groups <span class="token operator">=</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">//</span> self<span class="token punctuation">.</span>num_key_value_heads
        self<span class="token punctuation">.</span>max_position_embeddings <span class="token operator">=</span> config<span class="token punctuation">.</span>max_position_embeddings
        self<span class="token punctuation">.</span>rope_theta <span class="token operator">=</span> config<span class="token punctuation">.</span>rope_theta
        self<span class="token punctuation">.</span>is_causal <span class="token operator">=</span> <span class="token boolean">True</span>

        self<span class="token punctuation">.</span>q_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>k_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>v_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>o_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>rotary_emb <span class="token operator">=</span> LlamaRotaryEmbedding<span class="token punctuation">(</span>
                self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>
                max_position_embeddings<span class="token operator">=</span>self<span class="token punctuation">.</span>max_position_embeddings<span class="token punctuation">,</span>
                base<span class="token operator">=</span>self<span class="token punctuation">.</span>rope_theta<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

    <span class="token comment"># Adapted from LlamaAttention.forward</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>
        attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        past_key_value<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Cache<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        output_attentions<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        use_cache<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        cache_position<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        

        bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> _ <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>

        query_states <span class="token operator">=</span> self<span class="token punctuation">.</span>q_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
        key_states <span class="token operator">=</span> self<span class="token punctuation">.</span>k_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
        value_states <span class="token operator">=</span> self<span class="token punctuation">.</span>v_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

        query_states <span class="token operator">=</span> query_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        key_states <span class="token operator">=</span> key_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        value_states <span class="token operator">=</span> value_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

        cos<span class="token punctuation">,</span> sin <span class="token operator">=</span> self<span class="token punctuation">.</span>rotary_emb<span class="token punctuation">(</span>value_states<span class="token punctuation">,</span> position_ids<span class="token punctuation">)</span>
        query_states<span class="token punctuation">,</span> key_states <span class="token operator">=</span> apply_rotary_pos_emb<span class="token punctuation">(</span>query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">,</span> cos<span class="token punctuation">,</span> sin<span class="token punctuation">)</span>

        <span class="token keyword">if</span> past_key_value <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token comment"># sin and cos are specific to RoPE models; cache_position needed for the static cache</span>
            cache_kwargs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&quot;sin&quot;</span><span class="token punctuation">:</span> sin<span class="token punctuation">,</span> <span class="token string">&quot;cos&quot;</span><span class="token punctuation">:</span> cos<span class="token punctuation">,</span> <span class="token string">&quot;cache_position&quot;</span><span class="token punctuation">:</span> cache_position<span class="token punctuation">}</span>
            key_states<span class="token punctuation">,</span> value_states <span class="token operator">=</span> past_key_value<span class="token punctuation">.</span>update<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> value_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>layer_idx<span class="token punctuation">,</span> cache_kwargs<span class="token punctuation">)</span>

        key_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_groups<span class="token punctuation">)</span>
        value_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>value_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_groups<span class="token punctuation">)</span>

        causal_mask <span class="token operator">=</span> attention_mask
        <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            causal_mask <span class="token operator">=</span> causal_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span> key_states<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span>

        <span class="token comment"># SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,</span>
        <span class="token comment"># Reference: https://github.com/pytorch/pytorch/issues/112577.</span>
        <span class="token keyword">if</span> query_states<span class="token punctuation">.</span>device<span class="token punctuation">.</span><span class="token builtin">type</span> <span class="token operator">==</span> <span class="token string">&quot;cuda&quot;</span> <span class="token keyword">and</span> causal_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            query_states <span class="token operator">=</span> query_states<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
            key_states <span class="token operator">=</span> key_states<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
            value_states <span class="token operator">=</span> value_states<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># We dispatch to SDPA&#39;s Flash Attention or Efficient kernels via this if statement instead of an</span>
        <span class="token comment"># inline conditional assignment to support both torch.compile&#39;s \`dynamic=True\` and \`fullgraph=True\`</span>
        is_causal <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token keyword">if</span> causal_mask <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">and</span> q_len <span class="token operator">&gt;</span> <span class="token number">1</span> <span class="token keyword">else</span> <span class="token boolean">False</span>

        attn_output <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>scaled_dot_product_attention<span class="token punctuation">(</span>
            query_states<span class="token punctuation">,</span>
            key_states<span class="token punctuation">,</span>
            value_states<span class="token punctuation">,</span>
            attn_mask<span class="token operator">=</span>causal_mask<span class="token punctuation">,</span>
            dropout_p<span class="token operator">=</span>self<span class="token punctuation">.</span>attention_dropout <span class="token keyword">if</span> self<span class="token punctuation">.</span>training <span class="token keyword">else</span> <span class="token number">0.0</span><span class="token punctuation">,</span>
            is_causal<span class="token operator">=</span>is_causal<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

        attn_output <span class="token operator">=</span> attn_output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
        attn_output <span class="token operator">=</span> attn_output<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>

        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">(</span>attn_output<span class="token punctuation">)</span>

        <span class="token keyword">return</span> attn_output<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> past_key_value
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-8-llamarotaryembedding" tabindex="-1"><a class="header-anchor" href="#_6-8-llamarotaryembedding" aria-hidden="true">#</a> 6.8. LlamaRotaryEmbedding</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LlamaRotaryEmbedding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> max_position_embeddings<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> base<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> scaling_factor<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>scaling_factor <span class="token operator">=</span> scaling_factor <span class="token comment"># used to adjust the magnitude of positional encoding</span>
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> dim
        self<span class="token punctuation">.</span>max_position_embeddings <span class="token operator">=</span> max_position_embeddings <span class="token comment"># Indicates the maximum sequence length, i.e., the maximum number of positional encodings that the model can handle</span>
        self<span class="token punctuation">.</span>base <span class="token operator">=</span> base <span class="token comment"># used for calculating frequencies</span>
        inv_freq <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>base <span class="token operator">**</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>dim<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># Inverse frequency; the shape of inv_freq is (dim/2), and each element of inv_freq represents how quickly the encoding for that dimension will cycle through the sinusoidal functions. Smaller inv_freq leads to lower frequency (slower cycling), whereas a larger inv_freq results in higher frequency (faster cycling). In this way, the feature vector for any position will be unique, allowing the model to understand and exploit the positional information of elements in the sequence</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">&quot;inv_freq&quot;</span><span class="token punctuation">,</span> inv_freq<span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token comment"># For BC we register cos and sin cached</span>
        self<span class="token punctuation">.</span>max_seq_len_cached <span class="token operator">=</span> max_position_embeddings

    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> position_ids<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># x: [bs, num_attention_heads, seq_len, head_size]</span>
        inv_freq_expanded <span class="token operator">=</span> self<span class="token punctuation">.</span>inv_freq<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>position_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># （64）-&gt;(batch.size, 64, 1)</span>
        position_ids_expanded <span class="token operator">=</span> position_ids<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># (1, seq.len)-&gt;(batch.size, 1, seq.len)</span>
        <span class="token comment"># Force float32 since bfloat16 loses precision on long contexts</span>
        <span class="token comment"># See https://github.com/huggingface/transformers/pull/29285</span>
        device_type <span class="token operator">=</span> x<span class="token punctuation">.</span>device<span class="token punctuation">.</span><span class="token builtin">type</span>
        device_type <span class="token operator">=</span> device_type <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>device_type<span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token keyword">and</span> device_type <span class="token operator">!=</span> <span class="token string">&quot;mps&quot;</span> <span class="token keyword">else</span> <span class="token string">&quot;cpu&quot;</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>autocast<span class="token punctuation">(</span>device_type<span class="token operator">=</span>device_type<span class="token punctuation">,</span> enabled<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            freqs <span class="token operator">=</span> <span class="token punctuation">(</span>inv_freq_expanded<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> @ position_ids_expanded<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment"># (batch.size, 64, 1) * (batch.size, 1, seq.len) -&gt; (batch.size, 64, seq.len) -&gt; transpose(1, 2) -&gt; (batch.size, seq.len, 64)</span>
            emb <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>freqs<span class="token punctuation">,</span> freqs<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># (batch.size, seq.len, 128)</span>
            cos <span class="token operator">=</span> emb<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># (batch.size, seq.len, 128)</span>
            sin <span class="token operator">=</span> emb<span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># (batch.size, seq.len, 128)</span>
        <span class="token keyword">return</span> cos<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> sin<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-9-llamamlp" tabindex="-1"><a class="header-anchor" href="#_6-9-llamamlp" aria-hidden="true">#</a> 6.9. LlamaMLP</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LlamaMLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size
        self<span class="token punctuation">.</span>intermediate_size <span class="token operator">=</span> config<span class="token punctuation">.</span>intermediate_size
        self<span class="token punctuation">.</span>gate_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>mlp_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>mlp_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>down_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>mlp_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act_fn <span class="token operator">=</span> ACT2FN<span class="token punctuation">[</span>config<span class="token punctuation">.</span>hidden_act<span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token builtin">slice</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>intermediate_size <span class="token operator">//</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp
            gate_proj_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
            up_proj_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
            down_proj_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

            gate_proj <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>
                <span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> gate_proj_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>
            <span class="token punctuation">)</span>
            up_proj <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> up_proj_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

            intermediate_states <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>act_fn<span class="token punctuation">(</span>gate_proj<span class="token punctuation">)</span> <span class="token operator">*</span> up_proj<span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
            down_proj <span class="token operator">=</span> <span class="token punctuation">[</span>
                F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>intermediate_states<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> down_proj_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span>
            <span class="token punctuation">]</span>
            down_proj <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>down_proj<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            down_proj <span class="token operator">=</span> self<span class="token punctuation">.</span>down_proj<span class="token punctuation">(</span>self<span class="token punctuation">.</span>act_fn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>gate_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>up_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> down_proj
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,37);function b(_,h){return u(),r("div",null,[v,k(" more "),m])}const w=l(d,[["render",b],["__file","005_llama.html.vue"]]);export{w as default};
