const t=JSON.parse('{"key":"v-a50f96aa","path":"/posts/llm/028_distribution_and_parallelism_4.html","title":"Distributed Training Part 5: Introduction to GPU","lang":"en-US","frontmatter":{"icon":"lightbulb","sidebar":false,"date":"2025-03-06T00:00:00.000Z","prev":false,"next":"./027_distribution_and_parallelism_3","category":["LLM"],"tag":["Distributed","Parallelism"],"description":"Distributed Training Part 5: Introduction to GPU","head":[["link",{"rel":"alternate","hreflang":"zh-cn","href":"https://liz-in-tech.github.io/blog/zh/posts/llm/028_distribution_and_parallelism_4.html"}],["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/blog/posts/llm/028_distribution_and_parallelism_4.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"Distributed Training Part 5: Introduction to GPU"}],["meta",{"property":"og:description","content":"Distributed Training Part 5: Introduction to GPU"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:locale:alternate","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-03-08T14:32:06.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:tag","content":"Distributed"}],["meta",{"property":"article:tag","content":"Parallelism"}],["meta",{"property":"article:published_time","content":"2025-03-06T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-03-08T14:32:06.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Distributed Training Part 5: Introduction to GPU\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-03-06T00:00:00.000Z\\",\\"dateModified\\":\\"2025-03-08T14:32:06.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. GPU Architecture","slug":"_1-gpu-architecture","link":"#_1-gpu-architecture","children":[]},{"level":2,"title":"2. How to Improve Performance with Kernels","slug":"_2-how-to-improve-performance-with-kernels","link":"#_2-how-to-improve-performance-with-kernels","children":[{"level":3,"title":"2.1. Tools for Writing Kernel Code","slug":"_2-1-tools-for-writing-kernel-code","link":"#_2-1-tools-for-writing-kernel-code","children":[]},{"level":3,"title":"2.2. torch.compile Decorator","slug":"_2-2-torch-compile-decorator","link":"#_2-2-torch-compile-decorator","children":[]},{"level":3,"title":"2.3. Implementing Triton Kernels","slug":"_2-3-implementing-triton-kernels","link":"#_2-3-implementing-triton-kernels","children":[]},{"level":3,"title":"2.4. Implementing CUDA Kernels","slug":"_2-4-implementing-cuda-kernels","link":"#_2-4-implementing-cuda-kernels","children":[]}]},{"level":2,"title":"3. Fused Kernels","slug":"_3-fused-kernels","link":"#_3-fused-kernels","children":[]},{"level":2,"title":"4. Flash Attention","slug":"_4-flash-attention","link":"#_4-flash-attention","children":[{"level":3,"title":"4.1. Before Optimization","slug":"_4-1-before-optimization","link":"#_4-1-before-optimization","children":[]},{"level":3,"title":"4.2. Flash Attention Optimization","slug":"_4-2-flash-attention-optimization","link":"#_4-2-flash-attention-optimization","children":[]}]}],"git":{"createdTime":1741444326000,"updatedTime":1741444326000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":5.8,"words":1739},"filePathRelative":"posts/llm/028_distribution_and_parallelism_4.md","localizedDate":"March 6, 2025","excerpt":"<h1> Distributed Training Part 5: Introduction to GPU</h1>\\n","autoDesc":true}');export{t as data};
