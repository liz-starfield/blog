const e=JSON.parse('{"key":"v-cfc8eeea","path":"/posts/llm/027_distribution_and_parallelism_3.html","title":"Distributed Training Part 4: Parallel Strategies","lang":"en-US","frontmatter":{"icon":"lightbulb","sidebar":false,"date":"2025-03-04T00:00:00.000Z","prev":"./028_distribution_and_parallelism_4","next":"./026_distribution_and_parallelism_2","category":["LLM"],"tag":["Distributed","Parallelism"],"description":"Distributed Training Part 4: Parallel Strategies","head":[["link",{"rel":"alternate","hreflang":"zh-cn","href":"https://liz-in-tech.github.io/blog/zh/posts/llm/027_distribution_and_parallelism_3.html"}],["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/blog/posts/llm/027_distribution_and_parallelism_3.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"Distributed Training Part 4: Parallel Strategies"}],["meta",{"property":"og:description","content":"Distributed Training Part 4: Parallel Strategies"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:locale:alternate","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-03-08T14:32:06.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:tag","content":"Distributed"}],["meta",{"property":"article:tag","content":"Parallelism"}],["meta",{"property":"article:published_time","content":"2025-03-04T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-03-08T14:32:06.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Distributed Training Part 4: Parallel Strategies\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-03-04T00:00:00.000Z\\",\\"dateModified\\":\\"2025-03-08T14:32:06.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. Five Dimensions of Parallelization Strategies","slug":"_1-five-dimensions-of-parallelization-strategies","link":"#_1-five-dimensions-of-parallelization-strategies","children":[{"level":3,"title":"1.1. Five Dimensions","slug":"_1-1-five-dimensions","link":"#_1-1-five-dimensions","children":[]},{"level":3,"title":"1.2. Combining Multiple Parallel Strategies","slug":"_1-2-combining-multiple-parallel-strategies","link":"#_1-2-combining-multiple-parallel-strategies","children":[]},{"level":3,"title":"1.3. Impact Scope","slug":"_1-3-impact-scope","link":"#_1-3-impact-scope","children":[]},{"level":3,"title":"1.4. PP vs ZeRO-3","slug":"_1-4-pp-vs-zero-3","link":"#_1-4-pp-vs-zero-3","children":[]},{"level":3,"title":"1.5. TP & SP vs CP vs EP","slug":"_1-5-tp-sp-vs-cp-vs-ep","link":"#_1-5-tp-sp-vs-cp-vs-ep","children":[]},{"level":3,"title":"1.6. Memory Savings Comparison for Each Parallel Strategy","slug":"_1-6-memory-savings-comparison-for-each-parallel-strategy","link":"#_1-6-memory-savings-comparison-for-each-parallel-strategy","children":[]}]},{"level":2,"title":"2. Optimal Training Configuration","slug":"_2-optimal-training-configuration","link":"#_2-optimal-training-configuration","children":[{"level":3,"title":"2.1. Step 1: Fitting a Training Step in Memory / Fit a full model instance on our GPUs","slug":"_2-1-step-1-fitting-a-training-step-in-memory-fit-a-full-model-instance-on-our-gpus","link":"#_2-1-step-1-fitting-a-training-step-in-memory-fit-a-full-model-instance-on-our-gpus","children":[]},{"level":3,"title":"2.2. Step 2: Achieving Target Global Batch Size","slug":"_2-2-step-2-achieving-target-global-batch-size","link":"#_2-2-step-2-achieving-target-global-batch-size","children":[]},{"level":3,"title":"2.3. Step 3: Optimizing Training Throughput / Ensure the training is running as fast as possible","slug":"_2-3-step-3-optimizing-training-throughput-ensure-the-training-is-running-as-fast-as-possible","link":"#_2-3-step-3-optimizing-training-throughput-ensure-the-training-is-running-as-fast-as-possible","children":[]},{"level":3,"title":"2.4. Top Configurations","slug":"_2-4-top-configurations","link":"#_2-4-top-configurations","children":[]}]},{"level":2,"title":"3. Tensor Parallelism (TP)","slug":"_3-tensor-parallelism-tp","link":"#_3-tensor-parallelism-tp","children":[{"level":3,"title":"3.1. TP Principle","slug":"_3-1-tp-principle","link":"#_3-1-tp-principle","children":[]},{"level":3,"title":"3.2. TP Application in Transformer Blocks","slug":"_3-2-tp-application-in-transformer-blocks","link":"#_3-2-tp-application-in-transformer-blocks","children":[]},{"level":3,"title":"3.3. Impact of Scaling TP Shard Size on Throughput and Memory","slug":"_3-3-impact-of-scaling-tp-shard-size-on-throughput-and-memory","link":"#_3-3-impact-of-scaling-tp-shard-size-on-throughput-and-memory","children":[]}]},{"level":2,"title":"4. Sequence Parallelism (SP)","slug":"_4-sequence-parallelism-sp","link":"#_4-sequence-parallelism-sp","children":[{"level":3,"title":"4.1. TP Only vs TP with SP","slug":"_4-1-tp-only-vs-tp-with-sp","link":"#_4-1-tp-only-vs-tp-with-sp","children":[]},{"level":3,"title":"4.2. Throughput and Memory Usage","slug":"_4-2-throughput-and-memory-usage","link":"#_4-2-throughput-and-memory-usage","children":[]}]},{"level":2,"title":"5. Context Parallelism (CP)","slug":"_5-context-parallelism-cp","link":"#_5-context-parallelism-cp","children":[{"level":3,"title":"5.1. Ring Attention","slug":"_5-1-ring-attention","link":"#_5-1-ring-attention","children":[]},{"level":3,"title":"5.2. Zig-Zag Ring Attention","slug":"_5-2-zig-zag-ring-attention","link":"#_5-2-zig-zag-ring-attention","children":[]}]},{"level":2,"title":"6. Pipeline Parallelism (PP)","slug":"_6-pipeline-parallelism-pp","link":"#_6-pipeline-parallelism-pp","children":[{"level":3,"title":"6.1. Memory Usage","slug":"_6-1-memory-usage","link":"#_6-1-memory-usage","children":[]},{"level":3,"title":"6.2. Main Challenge: Minimize GPU computation idle time, improve GPU utilization","slug":"_6-2-main-challenge-minimize-gpu-computation-idle-time-improve-gpu-utilization","link":"#_6-2-main-challenge-minimize-gpu-computation-idle-time-improve-gpu-utilization","children":[]},{"level":3,"title":"6.3. Naive PP","slug":"_6-3-naive-pp","link":"#_6-3-naive-pp","children":[]},{"level":3,"title":"6.4. All-forward-all-backward (AFAB) Scheme / Forward then Backward / F then B","slug":"_6-4-all-forward-all-backward-afab-scheme-forward-then-backward-f-then-b","link":"#_6-4-all-forward-all-backward-afab-scheme-forward-then-backward-f-then-b","children":[]},{"level":3,"title":"6.5. One-forward-one-backward (1F1B) and LLama 3.1 Schemes","slug":"_6-5-one-forward-one-backward-1f1b-and-llama-3-1-schemes","link":"#_6-5-one-forward-one-backward-1f1b-and-llama-3-1-schemes","children":[]}]},{"level":2,"title":"7. Expert Parallelism (EP)","slug":"_7-expert-parallelism-ep","link":"#_7-expert-parallelism-ep","children":[]}],"git":{"createdTime":1741444326000,"updatedTime":1741444326000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":8.1,"words":2429},"filePathRelative":"posts/llm/027_distribution_and_parallelism_3.md","localizedDate":"March 4, 2025","excerpt":"<h1> Distributed Training Part 4: Parallel Strategies</h1>\\n","autoDesc":true}');export{e as data};
