const e=JSON.parse('{"key":"v-5ce9663c","path":"/zh/posts/llm/027_distribution_and_parallelism_3.html","title":"分布式训练之四：并行策略","lang":"zh-CN","frontmatter":{"icon":"lightbulb","sidebar":false,"date":"2025-03-04T00:00:00.000Z","prev":"./028_distribution_and_parallelism_4","next":"./026_distribution_and_parallelism_2","category":["LLM"],"tag":["分布式","并行"],"description":"分布式训练之四：并行策略","head":[["link",{"rel":"alternate","hreflang":"en-us","href":"https://liz-in-tech.github.io/blog/posts/llm/027_distribution_and_parallelism_3.html"}],["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/blog/zh/posts/llm/027_distribution_and_parallelism_3.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"分布式训练之四：并行策略"}],["meta",{"property":"og:description","content":"分布式训练之四：并行策略"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:locale:alternate","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-03-08T14:32:06.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:tag","content":"分布式"}],["meta",{"property":"article:tag","content":"并行"}],["meta",{"property":"article:published_time","content":"2025-03-04T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-03-08T14:32:06.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"分布式训练之四：并行策略\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-03-04T00:00:00.000Z\\",\\"dateModified\\":\\"2025-03-08T14:32:06.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. 五个维度的并行策略 5D Parallelization Strategies","slug":"_1-五个维度的并行策略-5d-parallelization-strategies","link":"#_1-五个维度的并行策略-5d-parallelization-strategies","children":[{"level":3,"title":"1.1. 五个维度","slug":"_1-1-五个维度","link":"#_1-1-五个维度","children":[]},{"level":3,"title":"1.2. 多个并行策略的结合","slug":"_1-2-多个并行策略的结合","link":"#_1-2-多个并行策略的结合","children":[]},{"level":3,"title":"1.3. 影响范围","slug":"_1-3-影响范围","link":"#_1-3-影响范围","children":[]},{"level":3,"title":"1.4. PP vs ZeRO-3","slug":"_1-4-pp-vs-zero-3","link":"#_1-4-pp-vs-zero-3","children":[]},{"level":3,"title":"1.5. TP & SP vs CP vs EP","slug":"_1-5-tp-sp-vs-cp-vs-ep","link":"#_1-5-tp-sp-vs-cp-vs-ep","children":[]},{"level":3,"title":"1.6. 每种并行策略节约内存的对比","slug":"_1-6-每种并行策略节约内存的对比","link":"#_1-6-每种并行策略节约内存的对比","children":[]}]},{"level":2,"title":"2. 最佳训练配置","slug":"_2-最佳训练配置","link":"#_2-最佳训练配置","children":[{"level":3,"title":"2.1. Step1: Fitting a Training Step in Memory / Fit a full model instance on our GPUs","slug":"_2-1-step1-fitting-a-training-step-in-memory-fit-a-full-model-instance-on-our-gpus","link":"#_2-1-step1-fitting-a-training-step-in-memory-fit-a-full-model-instance-on-our-gpus","children":[]},{"level":3,"title":"2.2. Step2: Achieving Target Global Batch Size","slug":"_2-2-step2-achieving-target-global-batch-size","link":"#_2-2-step2-achieving-target-global-batch-size","children":[]},{"level":3,"title":"2.3. Step3: Optimizing Training Throughput / make sure the training is running as fast as possible","slug":"_2-3-step3-optimizing-training-throughput-make-sure-the-training-is-running-as-fast-as-possible","link":"#_2-3-step3-optimizing-training-throughput-make-sure-the-training-is-running-as-fast-as-possible","children":[]},{"level":3,"title":"2.4. Top Configurations","slug":"_2-4-top-configurations","link":"#_2-4-top-configurations","children":[]}]},{"level":2,"title":"3. 张量并行 Tensor Parallelism（TP）","slug":"_3-张量并行-tensor-parallelism-tp","link":"#_3-张量并行-tensor-parallelism-tp","children":[{"level":3,"title":"3.1. TP原理","slug":"_3-1-tp原理","link":"#_3-1-tp原理","children":[]},{"level":3,"title":"3.2. Transformer块的TP应用","slug":"_3-2-transformer块的tp应用","link":"#_3-2-transformer块的tp应用","children":[]},{"level":3,"title":"3.3. 缩放TP分片大小对吞吐量和内存的影响","slug":"_3-3-缩放tp分片大小对吞吐量和内存的影响","link":"#_3-3-缩放tp分片大小对吞吐量和内存的影响","children":[]}]},{"level":2,"title":"4. 序列并行 Sequence Parallelism (SP)","slug":"_4-序列并行-sequence-parallelism-sp","link":"#_4-序列并行-sequence-parallelism-sp","children":[{"level":3,"title":"4.1. TP Only 与 TP with SP","slug":"_4-1-tp-only-与-tp-with-sp","link":"#_4-1-tp-only-与-tp-with-sp","children":[]},{"level":3,"title":"4.2. 吞吐量和内存占用","slug":"_4-2-吞吐量和内存占用","link":"#_4-2-吞吐量和内存占用","children":[]}]},{"level":2,"title":"5. Context Parallelism (CP)","slug":"_5-context-parallelism-cp","link":"#_5-context-parallelism-cp","children":[{"level":3,"title":"5.1. Ring Attention 环形注意力","slug":"_5-1-ring-attention-环形注意力","link":"#_5-1-ring-attention-环形注意力","children":[]},{"level":3,"title":"5.2. Zig-Zag Ring Attention","slug":"_5-2-zig-zag-ring-attention","link":"#_5-2-zig-zag-ring-attention","children":[]}]},{"level":2,"title":"6. 流水线并行 Pipeline Parallelism (PP)","slug":"_6-流水线并行-pipeline-parallelism-pp","link":"#_6-流水线并行-pipeline-parallelism-pp","children":[{"level":3,"title":"6.1. 内存占用","slug":"_6-1-内存占用","link":"#_6-1-内存占用","children":[]},{"level":3,"title":"6.2. 主要挑战：尽量避免GPU计算的闲置，提高GPU利用率","slug":"_6-2-主要挑战-尽量避免gpu计算的闲置-提高gpu利用率","link":"#_6-2-主要挑战-尽量避免gpu计算的闲置-提高gpu利用率","children":[]},{"level":3,"title":"6.3. 朴素PP","slug":"_6-3-朴素pp","link":"#_6-3-朴素pp","children":[]},{"level":3,"title":"6.4. all-forward-all-backward (AFAB) 方案 / forward then backword / F then B","slug":"_6-4-all-forward-all-backward-afab-方案-forward-then-backword-f-then-b","link":"#_6-4-all-forward-all-backward-afab-方案-forward-then-backword-f-then-b","children":[]},{"level":3,"title":"6.5. One-forward-one-backward (1F1B)  and LLama 3.1 schemes 方案","slug":"_6-5-one-forward-one-backward-1f1b-and-llama-3-1-schemes-方案","link":"#_6-5-one-forward-one-backward-1f1b-and-llama-3-1-schemes-方案","children":[]}]},{"level":2,"title":"7. Expert Parallelism (EP)","slug":"_7-expert-parallelism-ep","link":"#_7-expert-parallelism-ep","children":[]}],"git":{"createdTime":1741444326000,"updatedTime":1741444326000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":12.49,"words":3748},"filePathRelative":"zh/posts/llm/027_distribution_and_parallelism_3.md","localizedDate":"2025年3月4日","excerpt":"<h1> 分布式训练之四：并行策略</h1>\\n","autoDesc":true}');export{e as data};
