import{_ as t,a as i,b as r,c as p,d as c,e as o,f as m,g as h,h as u}from"./014_mrr_example-sQvWJqCE.js";import{_ as d}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as g,o as v,c as y,f,a as s,b as a,d as n,e as l}from"./app-MbMw1XaM.js";const b={},_=s("h1",{id:"rag评估指标",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#rag评估指标","aria-hidden":"true"},"#"),a(" RAG评估指标")],-1),R=s("ul",null,[s("li",null,"如何评估RAG"),s("li",null,"Generation Evaluation"),s("li",null,"Retrieval Evaluation")],-1),k=l('<h2 id="_1-如何评估rag" tabindex="-1"><a class="header-anchor" href="#_1-如何评估rag" aria-hidden="true">#</a> 1. 如何评估RAG</h2><p>评估框架</p><ul><li>ragas <ul><li>https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/</li></ul></li><li>RAGChecker</li></ul><p>Ragas: RAG可以通过四个指标来评估。其中两个指标偏向于LLM，另外两个偏向于上下文。 <img src="'+t+'" alt="RAG评估指标" loading="lazy"></p><p>这张图表展示了RAG（检索增强生成）评估的指标和框架，分为三个主要部分：检索、生成/幻觉和端到端。</p><p>更多参考：https://www.cnblogs.com/ting1/p/18451762</p><p>检索部分的评估指标有：</p><ul><li>召回率（Recall）</li><li>平均倒数排名（MRR）</li><li>平均精度（MAP）</li></ul><p>生成/幻觉部分的评估类型有三种：</p><ul><li>基于n-gram的评估：如BLEU、ROUGE、METEOR等。</li><li>基于模型的评估：如BERTScore、BARTScore等。</li><li>基于LLM的评估：如G-Eval、UniEval、GPTScore、TRUE、SelfCheckGPT、ChatProtect、Chainpoll等。</li></ul><p>端到端部分包含多个框架和指标：</p><ul><li>Ragas框架：评估上下文召回、上下文精度、上下文相关性、答案语义相似性、答案正确性、忠实度、答案相关性。</li><li>promptfoo框架：评估上下文依从性、上下文召回、上下文相关性、真实性、答案相关性。</li><li>RAG Triad框架：评估上下文相关性、答案相关性、扎实性。</li><li>ARES框架：评估上下文相关性、答案忠实度、答案相关性。</li><li>EXAM框架：评估上下文相关性。</li></ul><figure><img src="'+i+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>生成评估 <ul><li>Faithfulness 忠实性(准确性)</li><li>Answer Relevancy 答案相关性</li><li>ROUGE <ul><li>latency of online pipeline 在线流水线的延迟</li><li>cost of offline pipeline 离线流水线的成本</li></ul></li><li>BLUE</li><li>BertScore</li><li>LLM as a Judge</li><li>Other <ul><li>Latency（延迟）</li><li>Diversity（多样性）</li><li>Noise Robustness（噪声鲁棒性）</li><li>Negative Rejection（负面拒绝）</li><li>Counterfactual Robustness（反事实鲁棒性）</li><li>Information Integration（信息集成）</li></ul></li></ul></li><li>检索评估 <ul><li>Context Precision 上下文准确率</li><li>Context Recall 上下文召回率</li><li>OpenAI <ul><li>Hit Rate 命中率 (和 Recall 一致)</li><li>MAP (Mean Average Precision) 均值平均精度</li><li>MRR (Mean Reciprocal Rank) 平均倒数排名</li><li>NDCG (Normalized Discounted Cumulative Gain) 归一化折扣累积增益</li></ul></li><li>Common <ul><li>Precision</li><li>Recall</li><li>F1</li><li>AUC (Area Under Curve)</li></ul></li></ul></li></ul><h2 id="_2-与llm相关的指标-generation-evaluation" tabindex="-1"><a class="header-anchor" href="#_2-与llm相关的指标-generation-evaluation" aria-hidden="true">#</a> 2 与LLM相关的指标 Generation Evaluation</h2><h3 id="_2-1-忠实性-faithfulness" tabindex="-1"><a class="header-anchor" href="#_2-1-忠实性-faithfulness" aria-hidden="true">#</a> 2.1. 忠实性 Faithfulness</h3><ul><li>评估生成的答案与给定上下文的事实一致性（Faithfulness metric measures the factual consistency of the generated answer against the given context.）</li></ul>',17),x=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mtext",null,"Faithfulness score"),s("mo",null,"="),s("mfrac",null,[s("mrow",null,[s("mi",{mathvariant:"normal"},"∣"),s("mtext",null,"Number of claims in the generated answer that can be inferred from given context"),s("mi",{mathvariant:"normal"},"∣")]),s("mrow",null,[s("mi",{mathvariant:"normal"},"∣"),s("mtext",null,"Total number of claims in the generated answer"),s("mi",{mathvariant:"normal"},"∣")])])]),s("annotation",{encoding:"application/x-tex"}," \\text{Faithfulness score} = {|\\text{Number of claims in the generated answer that can be inferred from given context}| \\over |\\text{Total number of claims in the generated answer}|} ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6944em"}}),s("span",{class:"mord text"},[s("span",{class:"mord"},"Faithfulness score")]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"2.363em","vertical-align":"-0.936em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.427em"}},[s("span",{style:{top:"-2.314em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"∣"),s("span",{class:"mord text"},[s("span",{class:"mord"},"Total number of claims in the generated answer")]),s("span",{class:"mord"},"∣")])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.677em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"∣"),s("span",{class:"mord text"},[s("span",{class:"mord"},"Number of claims in the generated answer that can be inferred from given context")]),s("span",{class:"mord"},"∣")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.936em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})])])])])])])],-1),M=l('<blockquote><p>将答案拆分(采用llm)，并尝试将其与事实对照(采用llm)，与事实相符所占的比例即为结果。 如果答案无法与事实匹配，则视为幻觉Hallucinate 生成。</p></blockquote><figure><img src="'+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_2-2-答案相关性-answer-relevancy" tabindex="-1"><a class="header-anchor" href="#_2-2-答案相关性-answer-relevancy" aria-hidden="true">#</a> 2.2. 答案相关性 Answer Relevancy</h3><ul><li>评估生成的答案与给定Prompt的相关程度 (Response Relevancy metric focuses on assessing how relevant the generated answer is to the given prompt.)</li><li>较低的分数分配给不完整或包含冗余信息的答案，较高的分数表示更好的相关性。</li><li>重要的是，我们对答案相关性的评估不考虑事实性，而是对答案缺乏完整性或包含冗余细节的情况进行惩罚。</li><li>假设模型提供了大量上下文，模型利用这些上下文来回答问题，但答案与用户的原始需求相差甚远，因此，此指标评估模型提供的答案的相关性。</li></ul><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><blockquote><p>从答案逆向生成对应的几个问题，然后计算生成的问题与实际问题之间的平均余弦相似度</p></blockquote><figure><img src="'+o+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+m+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_2-3-rouge" tabindex="-1"><a class="header-anchor" href="#_2-3-rouge" aria-hidden="true">#</a> 2.3. ROUGE</h3><p>ROUGE指标主要用于评估文本摘要的质量。它通过计算生成文本和参考文本之间的词汇重叠来衡量文本相似性，特别关注文本中单词的覆盖率。</p><p>ROUGE有几个变体，主要包括：</p><ul><li>ROUGE-N：计算生成文本和参考文本之间的N-gram（如ROUGE-1表示单词，ROUGE-2表示词对等）的重叠。适合评估精细的词语顺序和重复模式。</li><li>ROUGE-L：基于最长公共子序列（LCS）计算得分，考虑了句子中词语的顺序，适合处理句子较长的情况。</li><li>ROUGE-W：给较长的连续匹配子序列更高的权重，适用于段落级别的总结。</li><li>ROUGE-S：统计句子中相邻词组对的共同部分，允许词之间有间隔。</li></ul><p>ROUGE的得分包括Precision（精确率）、Recall（召回率） 和 F-Score（F值），其中ROUGE常侧重召回率，因为文本摘要中重要信息的覆盖更为重要。</p><p>应用场景： ROUGE在文本摘要、文档生成、问答系统等任务中表现优异，尤其适合评估信息覆盖性的重要性。</p><ul><li>ROUGE <ul><li>latency of online pipeline 在线流水线的延迟</li><li>cost of offline pipeline 离线流水线的成本</li></ul></li></ul><h3 id="_2-4-blue" tabindex="-1"><a class="header-anchor" href="#_2-4-blue" aria-hidden="true">#</a> 2.4. BLUE</h3><p>BLEU是机器翻译领域中应用最广泛的评估标准之一。与ROUGE不同，BLEU主要关注生成文本和参考文本之间的n-gram精确匹配。BLEU通过计算生成句子中n-gram和参考句子中n-gram匹配的比例，反映生成内容的准确性。</p><p>BLEU的得分主要基于以下内容：</p><ul><li>n-gram匹配：计算生成句子和参考句子的n-gram（常用n=1到4）的匹配情况，以衡量生成内容的词汇精确性。</li><li>Brevity Penalty（长度惩罚）：如果生成文本长度显著短于参考文本，会降低得分，防止生成过短内容以提升匹配率。 BLEU的计算通常包括对多个参考翻译的匹配，因此生成内容对某一参考文本的某些细微变化不会显著影响总分。</li></ul><p>应用场景： BLEU广泛应用于机器翻译、对话生成和图像描述等生成任务，适合对生成内容精确性的高要求评估。BLEU 也有其局限性，比如不能考虑生成文本的流畅性或语法性。</p><p>区别与应用总结 ROUGE侧重召回率，适合信息覆盖性评估，常用于文本摘要任务； BLEU侧重精确率，适合内容精确性的评估，常用于机器翻译任务。</p><h3 id="_2-5-bertscore" tabindex="-1"><a class="header-anchor" href="#_2-5-bertscore" aria-hidden="true">#</a> 2.5. BertScore</h3><p>BertScore利用来自预训练的(transformers)（如(BERT)）的上下文嵌入，来评估生成文本和参考文本之间的语义相似性。BertScore使用上下文嵌入计算词级别的相似性，并生成精确率(precision)、召回率(recall)和(F1)分数。与基于(n-gram)的指标不同，BertScore可以在上下文中捕捉词语的意义，因此对释义更具鲁棒性，并且对语义等价更为敏感。</p><h3 id="_2-6-llm-as-a-judge" tabindex="-1"><a class="header-anchor" href="#_2-6-llm-as-a-judge" aria-hidden="true">#</a> 2.6. LLM as a Judge</h3><p>使用“(LLM as a Judge)”来评估生成文本是一种较新的方法。在这种方法中，LLM用于根据连贯性(coherence)、相关性(relevance)和流畅性(fluency)等标准对生成文本进行评分。此LLM可以选择性地在人工判断上进行微调，以预测未见文本的质量，或在零样本(zero-shot)或少样本(few-shot)设置中生成评估。这种方法利用了LLM对语言和上下文的理解，从而提供更细致的文本质量评估。</p><p>这种方法涵盖了内容评估的关键方面，包括连贯性(coherence)、相关性(relevance)、流畅性(fluency)、覆盖性(coverage)、多样性(diversity)和细节(detail)。</p><h3 id="_2-7-其他" tabindex="-1"><a class="header-anchor" href="#_2-7-其他" aria-hidden="true">#</a> 2.7. 其他</h3><ul><li>Latency（延迟） <ul><li>测量RAG系统完成一次查询响应所需的时间。它是用户体验的关键因素，尤其是在交互式应用中，例如聊天机器人或搜索引擎。</li><li>Single Query Latency（单查询延迟）：处理单个查询的平均时间，包括检索和生成阶段。</li></ul></li><li>Diversity（多样性） <ul><li>评估RAG系统检索和生成信息的多样性和广度。它确保系统能够提供广泛的视角，并避免在响应中出现冗余。</li><li>Cosine Similarity / Cosine Distance（余弦相似度/余弦距离）：通过计算检索到的文档或生成的响应的嵌入来衡量相似性。较低的余弦相似度分数表示更高的多样性，表明系统能够检索或生成更广泛的信息。</li></ul></li><li>Noise Robustness（噪声鲁棒性） <ul><li>衡量RAG系统在处理无关或误导性信息时，不降低响应质量的能力</li><li>Misleading Rate（误导率） 和 Mistake Reappearance Rate（错误重现率）</li></ul></li><li>Negative Rejection（负面拒绝） <ul><li>评估系统在可用信息不足或过于模糊无法提供准确答案时，拒绝生成响应的能力</li><li>Rejection Rate（拒绝率）：系统拒绝生成响应的频率。</li></ul></li><li>Counterfactual Robustness（反事实鲁棒性） <ul><li>评估系统识别和忽略检索文档中错误或反事实信息的能力。</li><li>Error Detection Rate（错误检测率）：检索信息中检测到的反事实陈述的比例。</li></ul></li><li>Information Integration （信息集成） <ul><li>从多个文档中整合答案的能力</li></ul></li></ul><h2 id="_3-与上下文相关的指标-retrieval-evaluation" tabindex="-1"><a class="header-anchor" href="#_3-与上下文相关的指标-retrieval-evaluation" aria-hidden="true">#</a> 3 与上下文相关的指标 Retrieval Evaluation</h2><h3 id="_3-1-上下文准确率-context-precision-误报" tabindex="-1"><a class="header-anchor" href="#_3-1-上下文准确率-context-precision-误报" aria-hidden="true">#</a> 3.1. 上下文准确率 Context Precision（误报）</h3><ul><li>precision = 召回docs中与query相关的doc数 / 总召回doc数</li><li>从客户的角度来看是最有用的，因为在某些情况下模型的准确性可能很高，但上下文精度较低。Recall不容易计算数据库中总相关文档数</li><li>经典RAG场景可以是将更多上下文加入到上下文窗口中，但当模型获得更多上下文时，可能会出现更多幻觉（参见论文：《Lost in the Middle: How Language Models Use Long Contexts》）</li><li>因此，上下文精度评估检索内容的信噪比。它记录内容日志并与答案进行比较，以确定检索的内容是否符合“应有答案”。</li></ul><h3 id="_3-2-上下文召回率-context-recall-漏报-hit-rate-命中率" tabindex="-1"><a class="header-anchor" href="#_3-2-上下文召回率-context-recall-漏报-hit-rate-命中率" aria-hidden="true">#</a> 3.2. 上下文召回率 Context Recall（漏报）Hit Rate 命中率</h3><ul><li>recall = 召回docs中与query相关的doc数 / 数据库中总相关doc数</li><li>该指标常用于bootstrap阶段（初始启动阶段），后续提升几乎不可用</li><li>Context Relevance和Context Recall可组成F1 score: F1 = 2 * relevance * recall / (relevance + recall)</li><li>模型是否能够检索出回答问题所需的所有相关信息？ <ul><li>Recall不容易计算数据库中总相关文档数</li></ul></li><li>模型置顶的搜索结果是否满足问题的需求？</li><li>上下文召回率显示是否需要优化搜索，可能需要增加重排 (Reranking)、微调嵌入(Fine-tune Embeddings)，或者需要使用不同的嵌入 (Embeddings) 来提取更相关的内容。</li></ul><p>OpenAI Cookbook中提及的 https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb</p><p>命中率计算的是在前 k 个检索文档中找到正确答案的查询比例。</p><p>示例： 假设一个用户在测试集中对5个项目感兴趣，推荐系统推荐了10个项目，其中3个是用户实际感兴趣的，那么命中率为 3/5 = 0.6。</p><h3 id="_3-3-map-mean-average-precision-均值平均精度" tabindex="-1"><a class="header-anchor" href="#_3-3-map-mean-average-precision-均值平均精度" aria-hidden="true">#</a> 3.3. MAP (Mean Average Precision) 均值平均精度</h3><h4 id="_3-3-1-ap-平均精度" tabindex="-1"><a class="header-anchor" href="#_3-3-1-ap-平均精度" aria-hidden="true">#</a> 3.3.1. AP 平均精度</h4>',39),G=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",null,"M"),s("mi",null,"A"),s("mi",null,"P"),s("mi",{mathvariant:"normal"},"@"),s("mi",null,"K"),s("mo",null,"="),s("mfrac",null,[s("mn",null,"1"),s("mi",null,"K")]),s("munderover",null,[s("mo",null,"∑"),s("mrow",null,[s("mi",null,"k"),s("mo",null,"="),s("mn",null,"1")]),s("mi",null,"K")]),s("msub",null,[s("mtext",null,"precision"),s("mi",null,"k")])]),s("annotation",{encoding:"application/x-tex"}," MAP@K = \\frac{1}{K}\\sum_{k=1}^K \\text{precision}_k ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6944em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.10903em"}},"M"),s("span",{class:"mord mathnormal"},"A"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"mord"},"@"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"K"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"3.1304em","vertical-align":"-1.3021em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.3214em"}},[s("span",{style:{top:"-2.314em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"K")])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.677em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.686em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mop op-limits"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.8283em"}},[s("span",{style:{top:"-1.8479em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k"),s("span",{class:"mrel mtight"},"="),s("span",{class:"mord mtight"},"1")])])]),s("span",{style:{top:"-3.05em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",null,[s("span",{class:"mop op-symbol large-op"},"∑")])]),s("span",{style:{top:"-4.3em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.07153em"}},"K")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.3021em"}},[s("span")])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord text"},[s("span",{class:"mord"},"precision")]),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.242em"}},[s("span",{style:{top:"-2.4559em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2441em"}},[s("span")])])])])])])])])])],-1),E=l(`<p>实际场景用的最多</p><p>假设一个具体例子： 用户查询：&quot;公司的休假制度&quot; 相关文档总共3个: doc1, doc2, doc4 系统返回排序结果: [doc1, doc3, doc2, doc4, doc5]</p><p>在每个位置计算precision:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 在每个位置计算Precision:</span>
position1 <span class="token punctuation">(</span>doc1<span class="token punctuation">)</span>: <span class="token number">1</span>/1    <span class="token punctuation">(</span>找到1个相关/已返回1个<span class="token punctuation">)</span>
position2 <span class="token punctuation">(</span>doc3<span class="token punctuation">)</span>: <span class="token number">1</span>/2    <span class="token punctuation">(</span>找到1个相关/已返回2个<span class="token punctuation">)</span>
position3 <span class="token punctuation">(</span>doc2<span class="token punctuation">)</span>: <span class="token number">2</span>/3    <span class="token punctuation">(</span>找到2个相关/已返回3个<span class="token punctuation">)</span>
position4 <span class="token punctuation">(</span>doc4<span class="token punctuation">)</span>: <span class="token number">3</span>/4    <span class="token punctuation">(</span>找到3个相关/已返回4个<span class="token punctuation">)</span>
position5 <span class="token punctuation">(</span>doc5<span class="token punctuation">)</span>: <span class="token number">3</span>/5    <span class="token punctuation">(</span>找到3个相关/已返回5个<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>MAP@5就是上面分数的平均</p><h4 id="_3-3-2-map-均值平均精度-多个查询的平均值" tabindex="-1"><a class="header-anchor" href="#_3-3-2-map-均值平均精度-多个查询的平均值" aria-hidden="true">#</a> 3.3.2. MAP 均值平均精度（多个查询的平均值）</h4><figure><img src="`+h+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>其中，P(k) 是列表中截止 k 处的精确度，rel(k) 是一个指标函数，如果排名 k 的项目是相关文档，则该函数等于 1，否则等于 0，n 是检索文档的数量。</p><p>具体求解：</p><p>假设有两个查询，查询1有4个相关文档，查询2有5个相关文档。某系统对查询1检索出4个相关文档，其rank分别为1,2,4,7；对于查询2检索出3个相关文档，其rank分别为1,3,5。</p><p>对于查询1，AP平均正确率为:(1/1+2/2+3/4+4/7)/4=0.83</p><p>对于查询2，AP平均正确率为:(1/1+2/3+3/5)/5=0.45</p><p>则平均正确率均值为:(0.83+0.45)/2=0.64</p><h3 id="_3-4-mean-reciprocal-rank-mrr-平均倒数排名" tabindex="-1"><a class="header-anchor" href="#_3-4-mean-reciprocal-rank-mrr-平均倒数排名" aria-hidden="true">#</a> 3.4. Mean Reciprocal Rank (MRR) 平均倒数排名</h3><p>OpenAI Cookbook中提及的 https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb</p>',15),w=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",null,"M"),s("mi",null,"R"),s("mi",null,"R"),s("mo",null,"="),s("mfrac",null,[s("mn",null,"1"),s("mi",null,"Q")]),s("munderover",null,[s("mo",null,"∑"),s("mrow",null,[s("mi",null,"q"),s("mo",null,"="),s("mn",null,"1")]),s("mi",null,"Q")]),s("mfrac",null,[s("mn",null,"1"),s("mrow",null,[s("mi",null,"r"),s("mi",null,"a"),s("mi",null,"n"),s("msub",null,[s("mi",null,"k"),s("mi",null,"q")])])])]),s("annotation",{encoding:"application/x-tex"}," MRR = \\frac{1}{Q} \\sum_{q=1}^Q\\frac{1}{rank_q} ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.00773em"}},"MRR"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"3.2787em","vertical-align":"-1.4032em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.3214em"}},[s("span",{style:{top:"-2.314em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"Q")])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.677em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8804em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mop op-limits"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.8754em"}},[s("span",{style:{top:"-1.8829em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"q"),s("span",{class:"mrel mtight"},"="),s("span",{class:"mord mtight"},"1")])])]),s("span",{style:{top:"-3.05em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",null,[s("span",{class:"mop op-symbol large-op"},"∑")])]),s("span",{style:{top:"-4.3471em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"Q")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.4032em"}},[s("span")])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.3214em"}},[s("span",{style:{top:"-2.314em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"mord mathnormal"},"an"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03148em"}},"k"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0315em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"q")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])])])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.677em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.9721em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})])])])])])],-1),L=s("p",null,[a("其中，Q是查询的次数，"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"r"),s("mi",null,"a"),s("mi",null,"n"),s("msub",null,[s("mi",null,"k"),s("mi",null,"q")])]),s("annotation",{encoding:"application/x-tex"},"{rank_q}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.9805em","vertical-align":"-0.2861em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"mord mathnormal"},"an"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03148em"}},"k"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0315em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"q")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])])])])])]),a("是每次查询第一个相关文档的排名。")],-1),A=l('<p>对于每个查询，MRR 通过查看排名最高的相关文档的排名来评估系统的准确性。具体来说，它是所有查询中这些排名的倒数的平均值。因此，如果第一个相关文档是最高结果，则倒数排名为 1；如果是第二个，则倒数排名为 1/2，以此类推。</p><p>示例：<br> 假设查询了3次，每次查询第一个出现的相关文档分别排在第3，2，1的名次，那么MRR为： MRR=(1/3 + 1/2 + 1/1) / 3 = 11/18 = 0.611</p><figure><img src="'+u+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><blockquote><p>MRR评估的是“平均来说，系统多快能够在响应用户查询时检索到第一个相关文档？” 其对排序质量相当敏感</p></blockquote><p>与MAP比较:</p><ul><li>MAP考虑所有相关文档的位置，MRR只考虑第一个</li><li>MAP能更全面地评估系统性能，MRR更关注快速找到首个相关文档</li></ul><h3 id="_3-5-ndcg-归一化折扣累积增益" tabindex="-1"><a class="header-anchor" href="#_3-5-ndcg-归一化折扣累积增益" aria-hidden="true">#</a> 3.5. NDCG 归一化折扣累积增益</h3><p>Normalized Discounted Cumulative Gain: 归一化折扣累积增益</p><p>概念：NDCG是一种在信息检索领域中广泛使用的评价指标，用于衡量排名质量。它考虑了所有相关项目的排名，并根据排名对其赋予不同的权重（排名越靠前，权重越大）。</p><p>缺点:需要详细的相关性标注（多级相关性）</p><p>它的特点是考虑到文档的相关程度（可以是多级相关性）以及排序位置的重要性</p><p>计算组成部分:</p><ul><li>DCG (Discounted Cumulative Gain)：考虑位置折扣的累积增益（实际排名，考虑相关性分数和排名位置）</li><li>IDCG (Ideal DCG)：理想情况下的DCG（理想的排名，相关文档按相关性排序）</li><li>NDCG = DCG / IDCG：归一化，使得不同查询可比</li></ul><p>计算过程:</p><p>假设我们对文档相关性进行0-3的评分：</p><ul><li>3分：非常相关</li><li>2分：相关</li><li>1分：一般相关</li><li>0分：不相关</li></ul><p>例如查询&quot;公司休假制度&quot;：</p><p>系统返回序列: [doc1, doc3, doc2, doc4, doc5]</p><p>相关性评分: [3, 0, 2, 2, 0]</p><p>相比MAP和MRR：</p><ul><li>MAP：只考虑二元相关性（相关/不相关）</li><li>MRR：只关注第一个相关文档</li><li>NDCG：同时考虑多级相关性和位置权重</li></ul><h2 id="_4-参考内容" tabindex="-1"><a class="header-anchor" href="#_4-参考内容" aria-hidden="true">#</a> 4. 参考内容</h2>',22),C={href:"https://arxiv.org/abs/2405.07437",target:"_blank",rel:"noopener noreferrer"},z={href:"https://arxiv.org/pdf/2309.01431",target:"_blank",rel:"noopener noreferrer"};function U(P,B){const e=g("ExternalLinkIcon");return v(),y("div",null,[_,R,f(" more "),k,x,M,G,E,w,L,A,s("ul",null,[s("li",null,[s("a",C,[a("Evaluation of Retrieval-Augmented Generation:A Survey （By Tencent）"),n(e)])]),s("li",null,[s("a",z,[a("Benchmarking Large Language Models in Retrieval-Augmented Generation"),n(e)])])])])}const D=d(b,[["render",U],["__file","014_rag_evaluation.html.vue"]]);export{D as default};
