import{_ as l,a,b as e,c as t,d as r,e as n,f as s,g as d,h as o,i as u,j as g,k as h,l as p,m as c,n as f,o as P,p as m,q as _,r as b,s as x,t as T,u as y,v as z,w as G,x as w,y as S,z as O,A as U,B as k,C as R,D as A,E as M,F as v,G as F}from"./027_ep2-bnsUQED8.js";import{_ as Z}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as B,c as L,f as V,a as i,b as C,e as D}from"./app-cafaW3Tc.js";const E={},K=i("h1",{id:"分布式训练之四-并行策略",tabindex:"-1"},[i("a",{class:"header-anchor",href:"#分布式训练之四-并行策略","aria-hidden":"true"},"#"),C(" 分布式训练之四：并行策略")],-1),q=D('<h2 id="_1-五个维度的并行策略-5d-parallelization-strategies" tabindex="-1"><a class="header-anchor" href="#_1-五个维度的并行策略-5d-parallelization-strategies" aria-hidden="true">#</a> 1. 五个维度的并行策略 5D Parallelization Strategies</h2><h3 id="_1-1-五个维度" tabindex="-1"><a class="header-anchor" href="#_1-1-五个维度" aria-hidden="true">#</a> 1.1. 五个维度</h3><ul><li>Data Parallelism (DP) -&gt; batch维度 <ul><li>ZeRO (Zero Redundancy Optimizer) <ul><li>ZeRO-1: optimizer state 分片</li><li>ZeRO-2: optimizer state + gradient 分片</li><li>ZeRO-3 / FSDP (Fully-Sharded Data Parallelism): optimizer state + gradient + parameter 分片</li></ul></li></ul></li><li>Tensor Parallelism (TP) -&gt; hidden_state维度</li><li>Sequence Parallelism (SP) -&gt; sequence维度</li><li>Context Parallelism (CP) -&gt; sequence维度</li><li>Pipeline parallelism (PP) -&gt; model_layer维度</li><li>Expert Parallelism (PP) -&gt; model_expert维度</li></ul><h3 id="_1-2-多个并行策略的结合" tabindex="-1"><a class="header-anchor" href="#_1-2-多个并行策略的结合" aria-hidden="true">#</a> 1.2. 多个并行策略的结合</h3><ul><li>PP + ZeRO-1/ZeRO-2/ZeRO-3 <ul><li>eg. the training of DeepSeek-v3 used PP combined with ZeRO-1</li></ul></li><li>TP &amp; SP + PP</li><li>TP &amp; SP + ZeRO-3</li><li>CP + EP</li><li>TP &amp; SP + CP + EP + PP + FSDP</li></ul><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_1-3-影响范围" tabindex="-1"><a class="header-anchor" href="#_1-3-影响范围" aria-hidden="true">#</a> 1.3. 影响范围</h3><ul><li>TP &amp; SP: 通过分片权重和激活值，影响整个模型的计算</li><li>CP: 主要影响注意力层，因为那是需要跨序列通信的地方，而其他层则在分片的序列上独立运行</li><li>EP: 主要影响MoE层（这些层替换了标准的MLP块），而注意力和其他组件保持不变</li><li>PP: 并不特别针对任何子模块或组件</li><li>ZeRO: 并不特别针对任何子模块或组件</li></ul><h3 id="_1-4-pp-vs-zero-3" tabindex="-1"><a class="header-anchor" href="#_1-4-pp-vs-zero-3" aria-hidden="true">#</a> 1.4. PP vs ZeRO-3</h3><p>共同点：都是将模型权重分割到多个 GPU 上，并沿着模型深度轴进行通信和计算的方法，每个设备上都会计算完整的层操作</p><p>不同点：</p><figure><img src="'+a+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_1-5-tp-sp-vs-cp-vs-ep" tabindex="-1"><a class="header-anchor" href="#_1-5-tp-sp-vs-cp-vs-ep" aria-hidden="true">#</a> 1.5. TP &amp; SP vs CP vs EP</h3><figure><img src="'+e+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_1-6-每种并行策略节约内存的对比" tabindex="-1"><a class="header-anchor" href="#_1-6-每种并行策略节约内存的对比" aria-hidden="true">#</a> 1.6. 每种并行策略节约内存的对比</h3><figure><img src="'+t+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_2-最佳训练配置" tabindex="-1"><a class="header-anchor" href="#_2-最佳训练配置" aria-hidden="true">#</a> 2. 最佳训练配置</h2><p>考虑点</p><ul><li>考虑到计算集群的各种物理属性，网络带宽，每个节点的GPU数，每个GPU的显存大小</li><li>考虑模型大小</li><li>考虑批次大小</li></ul><h3 id="_2-1-step1-fitting-a-training-step-in-memory-fit-a-full-model-instance-on-our-gpus" tabindex="-1"><a class="header-anchor" href="#_2-1-step1-fitting-a-training-step-in-memory-fit-a-full-model-instance-on-our-gpus" aria-hidden="true">#</a> 2.1. Step1: Fitting a Training Step in Memory / Fit a full model instance on our GPUs</h3><ul><li>GPU资源多 <ul><li>低于10B参数的模型 <ul><li>在8个GPU上使用单一的并行策略 <ul><li>e.g. Tensor Parallelism or ZeRO-3/DP with Full Recompute across 8 GPUs</li></ul></li></ul></li><li>参数在10B-100B之间的模型 <ul><li>在8个GPU上使用混合并行策略 <ul><li>TP（TP=8）+ PP</li><li>TP（TP=8）+ ZeRO-3</li><li>only ZeRO-3</li></ul></li></ul></li><li>在512个以上的GPU规模下 <ul><li>由于通信成本，纯数据并行/ZeRO-3会开始变得低效，此时最好将数据并行与张量并行或流水线并行结合使用</li></ul></li><li>在1024个以上的GPU规模下 <ul><li>推荐的配置可以是张量并行（TP=8）结合数据并行（ZeRO-2）和流水线并行</li></ul></li><li>特别考虑 <ul><li>对于超长序列：CC</li><li>对于MoE架构：EP</li></ul></li></ul></li><li>GPU资源少 <ul><li>完全的激活值重新计算用时间换空间 (训练有些缓慢)</li><li>增加梯度累积来处理更大的批次</li></ul></li></ul><h3 id="_2-2-step2-achieving-target-global-batch-size" tabindex="-1"><a class="header-anchor" href="#_2-2-step2-achieving-target-global-batch-size" aria-hidden="true">#</a> 2.2. Step2: Achieving Target Global Batch Size</h3><ul><li>增加当前的全局批次大小 <ul><li>扩大 DP 或 梯度累积步骤</li><li>对于长序列，采用 CP</li></ul></li><li>减少当前的全局批次大小 <ul><li>减少 DP</li><li>对于长序列，减少 CP</li></ul></li></ul><h3 id="_2-3-step3-optimizing-training-throughput-make-sure-the-training-is-running-as-fast-as-possible" tabindex="-1"><a class="header-anchor" href="#_2-3-step3-optimizing-training-throughput-make-sure-the-training-is-running-as-fast-as-possible" aria-hidden="true">#</a> 2.3. Step3: Optimizing Training Throughput / make sure the training is running as fast as possible</h3><p>在内存和通信不是瓶颈的情形下，尝试以下操作：</p><ul><li>扩大TP，使用快速的节点内带宽，直到并行度接近节点大小，从而减少其他并行方式的使用</li><li>在保持目标批次大小的同时，增加使用ZeRO-3的数据并行</li><li>当数据并行的通信开始成为瓶颈时，过渡到使用流水线并行</li><li>尝试逐一扩展不同的并行方式</li><li>实验几种微批次大小（mbs），以在最大全局批次大小（GBS）、模型大小、计算和通信之间寻求最佳平衡</li></ul><h3 id="_2-4-top-configurations" tabindex="-1"><a class="header-anchor" href="#_2-4-top-configurations" aria-hidden="true">#</a> 2.4. Top Configurations</h3><p>固定的实验设置： sequence length: 4096 gbs(global batch size): 1M tokens</p><figure><img src="'+n+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>展示了不同的模型大小，计算节点数（每个节点有8个GPU）的最佳配置，颜色表示了MFU (Model FLOPs Utilization)，其中FLOPs 为 Floating point operations per second，越亮的颜色代表更高的效率</p><p>包含的配置细节</p><ul><li>DP</li><li>TP</li><li>PP</li><li>GAS (Gradient Accumulation Steps)</li><li>MBS (Micro Batch Size)</li><li>ZeRO</li></ul><p>可获得的重要信息</p><ul><li>随着节点数量的增加（更高的并行度），效率有所下降，对小模型更为明显（虽然可以通过增大批次大小来补偿小模型的尺寸，但我们受到全局批次大小限制100万的约束）</li><li>较大模型带来了不同的挑战。随着模型大小的增加，内存需求大幅增长。这在节点较少的情况下产生了两种情景：要么模型完全无法匹配内存大小，要么勉强匹配但由于接近GPU内存限制而运行效率低下（例如，80亿参数模型在4个节点上的训练）</li><li>性能在很大程度上取决于每种并行策略的具体实现的质量（当我们首次实现这两种并行策略时，张量并行（TP）优于流水线并行（PP）。在优化了我们的PP代码后，它成为了更快的选择。现在我们正在改进TP实现中的通信重叠，预计它将重新获得性能领先优势。）</li></ul><h2 id="_3-张量并行-tensor-parallelism-tp" tabindex="-1"><a class="header-anchor" href="#_3-张量并行-tensor-parallelism-tp" aria-hidden="true">#</a> 3. 张量并行 Tensor Parallelism（TP）</h2><h3 id="_3-1-tp原理" tabindex="-1"><a class="header-anchor" href="#_3-1-tp原理" aria-hidden="true">#</a> 3.1. TP原理</h3><ul><li>ZeRO 对模型的参数、梯度和优化器状态进行了分片，但一旦激活值内存超出了我们的内存预算，我们就遇到了限制。</li><li>这时，我们引入张量并行（Tensor Parallelism, TP），这是一种不仅分片权重、梯度和优化器状态，还分片激活值的方法，而且在计算之前无需将它们全部聚集（gather）。</li></ul><p>TP的原理是利用了矩阵乘法的数学性质：</p><figure><img src="'+s+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>案例：如何对以下的计算进行TP</p><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>方案一：列分片 / column-wise sharding / column-linear</p><ul><li>将 X 进行广播：broadcast</li><li>将 W 进行列分片</li><li>聚集得到 Y：all-gather</li></ul><figure><img src="'+o+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>方案二：行分片 / row-wise sharding / row-linear</p><ul><li>将 X 进行列分片：scatter</li><li>将 W 进行行分片</li><li>求和得到 Y：all-reduce</li></ul><figure><img src="'+u+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_3-2-transformer块的tp应用" tabindex="-1"><a class="header-anchor" href="#_3-2-transformer块的tp应用" aria-hidden="true">#</a> 3.2. Transformer块的TP应用</h3><p>Transformer两个主要的块：</p><ul><li>MLP / Feedforward layers</li><li>MHA / Multi-Head Attention</li></ul><h4 id="_3-2-1-mlp块-列分片-行分片" tabindex="-1"><a class="header-anchor" href="#_3-2-1-mlp块-列分片-行分片" aria-hidden="true">#</a> 3.2.1. MLP块：列分片 -&gt; 行分片</h4><figure><img src="'+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+h+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>图中的all-reduce操作是必要的，且不能和GPU计算重叠</li><li>TP确实有助于减少矩阵乘法的激活值内存，但我们仍需要聚集完整的激活值用于LayerNorm的运算</li></ul><h4 id="_3-2-2-mha块" tabindex="-1"><a class="header-anchor" href="#_3-2-2-mha块" aria-hidden="true">#</a> 3.2.2. MHA块：</h4><ul><li>Q、K、V矩阵：用列分片 <ul><li>多头：多头注意力的每个头本身是并行的，TP正好利用这一特性</li><li>MQA (Multi-Query Attention)：所有 Q 共享一组 K 和 V</li><li>GQA (Grouped-Query Attention)：多个 Q 共享一组 K 和 V（分组共享）</li><li>张量并行的列分片方式对 MQA 和 GQA 同样适用，因为 K 和 V 的共享特性不会影响头的独立计算</li><li>限制条件 <ul><li>TP分片个数不要超过Q/K/V头的个数（否则不能独立的计算，需要额外的通信操作）</li><li>GQA分片个数不要超过K/V的头数（否则需要复制K/V的头来保持同步），例如Llama-3 8B有8个K/V头，所以TP分片不要超过8</li></ul></li></ul></li><li>O矩阵：用行分片</li></ul><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_3-3-缩放tp分片大小对吞吐量和内存的影响" tabindex="-1"><a class="header-anchor" href="#_3-3-缩放tp分片大小对吞吐量和内存的影响" aria-hidden="true">#</a> 3.3. 缩放TP分片大小对吞吐量和内存的影响</h3><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>权衡</p><ul><li>计算效率：增加的TP分片大小使得吞吐量降低（TP=8到TP=16,有显著的下降，TP=16到TP=32,有更陡的下降，下降程度随着TP的升高越发严重）</li><li>可用内存：增加的TP分片大小使得可处理更大的批大小</li></ul><figure><img src="'+f+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h2 id="_4-序列并行-sequence-parallelism-sp" tabindex="-1"><a class="header-anchor" href="#_4-序列并行-sequence-parallelism-sp" aria-hidden="true">#</a> 4. 序列并行 Sequence Parallelism (SP)</h2><ul><li>序列并行（SP）涉及对模型中未被张量并行（Tensor Parallelism, TP）处理的部分（例如 Dropout 和 LayerNorm）的激活值和计算进行分片，但分片是沿着输入序列维度（sequence dimension）进行的，而不是沿着隐藏维度（hidden dimension）。</li><li>此处的序列并行与张量并行紧密耦合，主要应用于 Dropout 和 LayerNorm 操作（For example, LayerNorm needs the full hidden dimension to compute mean and variance）</li><li>然而，当我们处理更长的序列时，注意力计算会成为瓶颈，这时需要引入一些技术，例如 Ring-Attention，这些技术有时也被称为序列并行，但我们将它们称为上下文并行（Context Parallelism），以区分这两种方法。因此，每次看到“序列并行”时，请记住它通常与张量并行一起使用（而上下文并行可以独立使用）。</li></ul><h3 id="_4-1-tp-only-与-tp-with-sp" tabindex="-1"><a class="header-anchor" href="#_4-1-tp-only-与-tp-with-sp" aria-hidden="true">#</a> 4.1. TP Only 与 TP with SP</h3><figure><img src="'+P+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+m+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>术语</p><ul><li>b : batch_size 批大小（张量的第一维度）</li><li>s : sequence_length 序列长度（张量的第二维度）</li><li>h : hidden_state 隐状态（张量的第三维度）</li></ul><p>对比 TP Only 和 TP with SP：</p><ul><li>序列并行最关键的优势是减少的需要存储的最大激活值大小 <ul><li>TP Only：在多个点都需要存形状为(b, s, h)的激活值，激活值大小为 b * s * h</li><li>TP with SP：激活值形状转为(b, s, h/k) 或 (b, s/k, h)，最大激活值大小被减小为 b * s * h / k，其中k为并行数</li></ul></li><li>两者在前向传播和后向传播中的通信开销都一样 <ul><li>TP Only：每个Transformer块有2个all-reduce操作</li><li>TP with SP：每个Transformer块有2个all-gather操作和2个reduce-scatter操作，但由于all-reduce = reduce-scatter + all-gather，所以相当于有2个all-reduce操作，和TP Only一致</li></ul></li></ul><h4 id="_4-1-1-左图-tp-only" tabindex="-1"><a class="header-anchor" href="#_4-1-1-左图-tp-only" aria-hidden="true">#</a> 4.1.1. 左图：TP Only</h4><ul><li>f 与 f* 操作 <ul><li>f 操作 <ul><li>前向传播中是空操作</li><li>反向传播中是all-reduce操作</li></ul></li><li>f* 操作 <ul><li>前向传播中是all-reduce操作</li><li>反向传播中是空操作</li></ul></li></ul></li><li>整体的张量变化 <ul><li>(b, s, h) -&gt; Full</li><li>f</li><li>(b, s, h/k) -&gt; TP (列分片 -&gt; 行分片)</li><li>f*</li><li>(b, s, h) -&gt; Full</li><li>f</li><li>(b, s, h/k) -&gt; TP (列分片 -&gt; 行分片)</li><li>f*</li><li>(b, s, h) -&gt; Full</li></ul></li></ul><h4 id="_4-1-2-右图-tp-with-sp" tabindex="-1"><a class="header-anchor" href="#_4-1-2-右图-tp-with-sp" aria-hidden="true">#</a> 4.1.2. 右图：TP with SP</h4><ul><li>g 与 g* 操作 <ul><li>g 操作 <ul><li>前向传播中是all-gather操作</li><li>反向传播中是reduce-scatter操作</li></ul></li><li>g* 操作 <ul><li>前向传播中是reduce-scatter操作</li><li>反向传播中是all-gather操作</li></ul></li></ul></li><li>整体的张量变化 <ul><li>(b, s/k, h) -&gt; SP</li><li>g</li><li>(b, s, h/k) -&gt; TP (列分片 -&gt; 行分片)</li><li>g*</li><li>(b, s/k, h) -&gt; SP</li><li>g</li><li>(b, s, h/k) -&gt; TP (列分片 -&gt; 行分片)</li><li>g*</li><li>(b, s/k, h) -&gt; SP</li></ul></li></ul><h3 id="_4-2-吞吐量和内存占用" tabindex="-1"><a class="header-anchor" href="#_4-2-吞吐量和内存占用" aria-hidden="true">#</a> 4.2. 吞吐量和内存占用</h3><p>TP with SP 的 MLP 部分情况：</p><ul><li>与TP Only一样，GPU通信不能与GPU计算重叠，这使得吞吐量严重依赖于通信带宽</li></ul><figure><img src="'+_+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>70B模型的内存占用：</p><figure><img src="'+b+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>3B model with 4096 seqlen 的 TP with SP 缩放对吞吐量和内存利用的影响：</p><figure><img src="'+x+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>同样需要权衡吞吐量和内存占用</p><h2 id="_5-context-parallelism-cp" tabindex="-1"><a class="header-anchor" href="#_5-context-parallelism-cp" aria-hidden="true">#</a> 5. Context Parallelism (CP)</h2><figure><img src="'+T+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_5-1-ring-attention-环形注意力" tabindex="-1"><a class="header-anchor" href="#_5-1-ring-attention-环形注意力" aria-hidden="true">#</a> 5.1. Ring Attention 环形注意力</h3><figure><img src="'+y+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>在每个时间步，每个GPU接连着执行这三个操作：</p><ul><li>1.发送当前的K、V到下一个GPU</li><li>2.本地计算注意力得分</li><li>3.等待接收上一个GPU传来的K、V</li></ul><p>朴素实现存在的问题：因为有掩码，所以数据呈下三角，而Softmax是按行计算的，各个GPU上的计算是不平衡的</p><figure><img src="'+z+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_5-2-zig-zag-ring-attention" tabindex="-1"><a class="header-anchor" href="#_5-2-zig-zag-ring-attention" aria-hidden="true">#</a> 5.2. Zig-Zag Ring Attention</h3><p>平衡计算的实现：不纯粹按顺序分配，二把前面的后面的token进行混合到一个GPU上</p><figure><img src="'+G+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>两种重叠计算和通信的方式</p><ul><li>all-gather实现：重组所有的KV在每个GPU上（以ZeRO-3的方式） <ul><li>需要临时存储所有的KV对</li><li>通信只发生在第一步</li></ul></li><li>all-to-all(Ring)实现：环形依次收集每个GPU上的KV <ul><li>只需要临时存储额外的一块</li><li>通信从头到尾，和计算重叠，有一些延迟开销</li></ul></li></ul><figure><img src="'+w+'" alt="all-gather实现" tabindex="0" loading="lazy"><figcaption>all-gather实现</figcaption></figure><figure><img src="'+S+'" alt="all-to-all(Ring)实现" tabindex="0" loading="lazy"><figcaption>all-to-all(Ring)实现</figcaption></figure><h2 id="_6-流水线并行-pipeline-parallelism-pp" tabindex="-1"><a class="header-anchor" href="#_6-流水线并行-pipeline-parallelism-pp" aria-hidden="true">#</a> 6. 流水线并行 Pipeline Parallelism (PP)</h2><p>流水线并行：把模型的层分到多个GPU上，又被称为“层间并行”</p><h3 id="_6-1-内存占用" tabindex="-1"><a class="header-anchor" href="#_6-1-内存占用" aria-hidden="true">#</a> 6.1. 内存占用</h3><figure><img src="'+O+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>每个GPU仍需处理这批次的全部数据，只是在不同的层上，所以激活值占内存大小是完整的，激活值在一个GPU的层上处理完后被发送给下一个GPU，从而继续前向传播</p><h3 id="_6-2-主要挑战-尽量避免gpu计算的闲置-提高gpu利用率" tabindex="-1"><a class="header-anchor" href="#_6-2-主要挑战-尽量避免gpu计算的闲置-提高gpu利用率" aria-hidden="true">#</a> 6.2. 主要挑战：尽量避免GPU计算的闲置，提高GPU利用率</h3><p>案例：16层的模型分布在4个GPU上</p><ul><li>t<sub>f</sub> : 前向传播的耗时</li><li>t<sub>b</sub> : 反向传播的耗时</li><li>一个简单的假设：t<sub>b</sub> = 2 * t<sub>f</sub></li></ul><h3 id="_6-3-朴素pp" tabindex="-1"><a class="header-anchor" href="#_6-3-朴素pp" aria-hidden="true">#</a> 6.3. 朴素PP</h3><ul><li>理想总耗时：t<sub>ideal</sub> = t<sub>f</sub> + t<sub>b</sub></li><li>闲置时间：t<sub>pipeline_bubble</sub> = (p - 1) * (t<sub>f</sub> + t<sub>b</sub>)，其中p为并行数</li><li>闲置时间与理想时间的比例：r<sub>bubble</sub> = (p - 1) * (t<sub>f</sub> + t<sub>b</sub>) / (t<sub>f</sub> + t<sub>b</sub>) = p - 1</li></ul><figure><img src="'+U+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_6-4-all-forward-all-backward-afab-方案-forward-then-backword-f-then-b" tabindex="-1"><a class="header-anchor" href="#_6-4-all-forward-all-backward-afab-方案-forward-then-backword-f-then-b" aria-hidden="true">#</a> 6.4. all-forward-all-backward (AFAB) 方案 / forward then backword / F then B</h3><ul><li>把批次再分为微批次，图中方块中的编号就是微批次</li><li>每批次被划分为8个微批次，9-16编号是下一个批次的微批次</li><li>假设模型有4层，每个GPU上放一层</li><li>AFAB是指等每个批次的所有微批次的前向传播都完成后，开启这个批次所有微批次的反向传播</li></ul><figure><img src="'+k+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>存在的问题：要存所有激活值（只有等前向传播都完成了，且该微批次的反向传播完成后才能释放该微批次的激活值）</p><h3 id="_6-5-one-forward-one-backward-1f1b-and-llama-3-1-schemes-方案" tabindex="-1"><a class="header-anchor" href="#_6-5-one-forward-one-backward-1f1b-and-llama-3-1-schemes-方案" aria-hidden="true">#</a> 6.5. One-forward-one-backward (1F1B) and LLama 3.1 schemes 方案</h3><h4 id="_6-5-1-non-interleaved-schedule-非交错调度-默认是此" tabindex="-1"><a class="header-anchor" href="#_6-5-1-non-interleaved-schedule-非交错调度-默认是此" aria-hidden="true">#</a> 6.5.1. non-interleaved schedule 非交错调度 （默认是此）</h4><ul><li>和AFAB相比，只要有一个微批次的前向传播完成了，就开启这个微批次的反向传播</li><li>每个微批次和其他微批次不进行同步</li><li>非交错式调度可分为三个阶段。第一阶段是热身阶段，处理器进行不同数量的前向计算。在接下来的阶段，处理器进行一次前向计算，然后是一次后向计算。最后一个阶段处理器完成后向计算。</li></ul><figure><img src="'+R+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>只存部分激活值（只要该微批次自己的前向传播完成，就可开启该微批次的反向传播来释放激活值，此时其他微批次的前向传播还在进行）</li><li>1F1B改善了内存占用但没有改善闲置问题</li></ul><h4 id="_6-5-2-interleaving-stages-interleaved-schedule-交错式调度" tabindex="-1"><a class="header-anchor" href="#_6-5-2-interleaving-stages-interleaved-schedule-交错式调度" aria-hidden="true">#</a> 6.5.2. Interleaving stages / interleaved schedule 交错式调度</h4><p>Transformer两个主要的块：</p><ul><li>MLP / Feedforward layers</li><li>MHA / Multi-Head Attention</li></ul><p>这里的每个块代表一个计算块，绿色块代表注意力块MHA的前向传播，青色块代表前馈神经网络MLP的前向传播，粉色块代表MHA的反向传播，紫色块代表MLP的反向传播，块上的数字表示微批次ID</p><figure><img src="'+A+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_7-expert-parallelism-ep" tabindex="-1"><a class="header-anchor" href="#_7-expert-parallelism-ep" aria-hidden="true">#</a> 7. Expert Parallelism (EP)</h2><p>MoE(Mixture-of-Experts)基础: https://huggingface.co/blog/moe</p><figure><img src="'+M+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Expert Parallelism (EP)：在专家维度上并行</p><ul><li>每个专家的FFN Layer是完全独立的</li><li>比起TP更轻量级，因为不需要切分矩阵乘法，只需要路由hidden states到正确的专家</li><li>一般毁于其他并行策略一起采用，如DP</li></ul><figure><img src="'+v+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+F+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>',132);function Q(N,H){return B(),L("div",null,[K,V(" more "),q])}const Y=Z(E,[["render",Q],["__file","027_distribution_and_parallelism_3.html.vue"]]);export{Y as default};
