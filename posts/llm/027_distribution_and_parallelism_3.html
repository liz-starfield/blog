<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.0" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.13" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="alternate" hreflang="zh-cn" href="https://liz-in-tech.github.io/blog/zh/posts/llm/027_distribution_and_parallelism_3.html"><meta property="og:url" content="https://liz-in-tech.github.io/blog/posts/llm/027_distribution_and_parallelism_3.html"><meta property="og:site_name" content="Liz"><meta property="og:title" content="Distributed Training Part 4: Parallel Strategies"><meta property="og:description" content="Distributed Training Part 4: Parallel Strategies"><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:locale:alternate" content="zh-CN"><meta property="og:updated_time" content="2025-03-08T14:32:06.000Z"><meta property="article:author" content="Liz"><meta property="article:tag" content="Distributed"><meta property="article:tag" content="Parallelism"><meta property="article:published_time" content="2025-03-04T00:00:00.000Z"><meta property="article:modified_time" content="2025-03-08T14:32:06.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Distributed Training Part 4: Parallel Strategies","image":[""],"datePublished":"2025-03-04T00:00:00.000Z","dateModified":"2025-03-08T14:32:06.000Z","author":[{"@type":"Person","name":"Liz","url":"https://github.com/liz-in-tech"}]}</script><link rel="icon" herf="/blogger.png"><link rel="icon" href="/blog/blogger.png"><title>Distributed Training Part 4: Parallel Strategies | Liz</title><meta name="description" content="Distributed Training Part 4: Parallel Strategies">
    <link rel="preload" href="/blog/assets/style-m_obra2h.css" as="style"><link rel="stylesheet" href="/blog/assets/style-m_obra2h.css">
    <link rel="modulepreload" href="/blog/assets/app-74fYY7Rf.js"><link rel="modulepreload" href="/blog/assets/027_distribution_and_parallelism_3.html-IYByZhbZ.js"><link rel="modulepreload" href="/blog/assets/027_distribution_and_parallelism_3.html-Djdzmvel.js"><link rel="modulepreload" href="/blog/assets/027_ep2-bnsUQED8.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-x3n3nnut.js">
    <link rel="prefetch" href="/blog/assets/index.html-YbPtte5_.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-CGfhr1vY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FUMOuem4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--TTjrkIy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g4Nfr7z1.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-bitGHKd2.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-nrisQopy.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-9XtwFAwc.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-oji9upQP.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-xrin91s2.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-q7LEGqjL.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-c628DmZb.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-4RcS3hxb.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-_8stdYVV.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-eluz3bTT.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-Ft0RQWf3.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-CImz0KSx.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-1CqW55t5.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-6CxOIR84.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-l_kCYUI1.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html-JrbwEhT3.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-XtD4OMNh.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html-dm178NRn.js" as="script"><link rel="prefetch" href="/blog/assets/018_huggingface.html-7ujiqdYY.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-kLkdC4dl.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-61m3-6IM.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-0d0frUhq.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-FazlUPBT.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-5MCDM_Sd.js" as="script"><link rel="prefetch" href="/blog/assets/024_distribution_and_parallelism.html-uCefT9T7.js" as="script"><link rel="prefetch" href="/blog/assets/025_distribution_and_parallelism_1.html-32IzAErP.js" as="script"><link rel="prefetch" href="/blog/assets/026_distribution_and_parallelism_2.html-MEZ1JY2b.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-Kub1JEXd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZDCSnlc1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZYw6WxxA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OavE9BET.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-hE_T0u_5.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-iH0mq6XB.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-WyFhRqF6.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-YaT0PR6o.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-x2qtCXhJ.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-WHPR-17-.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-hz6Q-DAA.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-3KMCwhrh.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-7gxKMp_3.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-KndRvZAj.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-Akmj-Ub_.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-bXz0YlpJ.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-W-CdR_ck.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-aq7YkQ-T.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html-V8hUrR0F.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-DCPUUxWe.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html-RlA7g4kG.js" as="script"><link rel="prefetch" href="/blog/assets/018_huggingface.html-YkW8PRbE.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-V5tcXCC4.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-qFyQHNwt.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-kvlLpO3f.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-AiRwjg9s.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-VZsN2kO8.js" as="script"><link rel="prefetch" href="/blog/assets/024_distribution_and_parallelism.html-MO4kg9bC.js" as="script"><link rel="prefetch" href="/blog/assets/025_distribution_and_parallelism_1.html-ZR9kuXUl.js" as="script"><link rel="prefetch" href="/blog/assets/026_distribution_and_parallelism_2.html-GkjDPoeT.js" as="script"><link rel="prefetch" href="/blog/assets/027_distribution_and_parallelism_3.html-5kFJGreK.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-38Gbieji.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wh_dBtOR.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-cxLWDy2T.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kf4JCRaf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-RkA-insV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4kI_oqSd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-WhuidxNt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OqGkeUA_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5GeN-sdD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-c4Rf4yh1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2r0jUs7o.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BSKRXRQc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-KjTsJ0Hg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TteIwMx3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g00XXzrL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-YxbJgo4L.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-3T79Cy0i.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-A5tlQHan.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-04ff5e0O.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-vgZ6rfFh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OmipPplE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-E1KrJL6a.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oPH9QkTj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cHRqZSs8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-74SU9ZTn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--iJiA8oX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oZPWb_Fc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FUIWSLsp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5ERWyusD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Zjn0JNqd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jcvPTrgB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9t2TsyuQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CcLVFNIv.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DVoYOOaL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-V52ipvRm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9If_KW0o.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-df9Mrf2R.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-f9bWoKcO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GYH0QUoo.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-JVTfeijx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_C1QVNqX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-L_IXFmna.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nZWHmXY7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-x4gPgqE4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bWnVyuyA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-LHukpLS7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-howjHe2f.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DeN_iOWx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cnlzR0a7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0P_c_pcU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-dy_CcFmq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FzFytZ_p.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2KSwV7xp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-F1coElwg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ffflqCb9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-F8ZuLYgH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-HeYWaFeL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xKYeJEc5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xB-iS7Ql.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OEUPqfTV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-EE4iQI9m.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g1PUF_BG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nUBcs85a.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9Y3la5Sf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wufIFDPM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bXZQIxRE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wd6ZkEHi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Xlk0AXmC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Uv7c7pYa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FfZqC9tZ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mSPhZxqB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-51HoKD5A.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Q5_K6Vux.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jkWPo860.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kTEqch5G.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-y4iBqBqc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-iETYQT4k.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-ule1Jz9M.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B26Ce-6E.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-JMfyTDfh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GNSVMZPS.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-XiFX24l6.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-aTUuNcf0.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-F-G_LZNv.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-1wjML3Lp.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html--z2TABMK.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-qJgD5cA8.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-MyTGH7ZD.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-R276mg6X.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-FggHTkut.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-vQmkuCAd.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-uw8sfEbw.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-njw3HA-K.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-YZl8_3DJ.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-0KGQiBPr.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-ZE5Abz_N.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html-N7MDUyd-.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-tY4Lrou5.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html-LLZQoHT_.js" as="script"><link rel="prefetch" href="/blog/assets/018_huggingface.html-vlUjdk6y.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-0E1a8ymD.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-VDOBSpBs.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-NhowT-zb.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-m1QqxyaQ.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-s6aaQyPq.js" as="script"><link rel="prefetch" href="/blog/assets/024_distribution_and_parallelism.html-mcWDVxU9.js" as="script"><link rel="prefetch" href="/blog/assets/025_distribution_and_parallelism_1.html-3Q1VNe84.js" as="script"><link rel="prefetch" href="/blog/assets/026_distribution_and_parallelism_2.html-plxA0Vv1.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-ZyvwGllI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-MMIbGt8W.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Meezp5An.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-LHTTg6UW.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html--LyYB64w.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-iQVqBKNY.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-53ADn2wT.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-97ovvest.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-YceYqKyv.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-6bgRRWw9.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-vouBuA9W.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-km78igxp.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-SQpsJNVu.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-gx3_TnZU.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-ZDe65xaT.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-KiDWLV83.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-qZETJJKw.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-vvS2fOQT.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html--pvIAqdI.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-1Ysa9iba.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html-oMffmvvX.js" as="script"><link rel="prefetch" href="/blog/assets/018_huggingface.html-iKSVOJFV.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-yVAuISKR.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-vk88Pa0d.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-L5FlOwpQ.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-xRgh74ep.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-747Xrf6v.js" as="script"><link rel="prefetch" href="/blog/assets/024_distribution_and_parallelism.html-IbLXEclX.js" as="script"><link rel="prefetch" href="/blog/assets/025_distribution_and_parallelism_1.html-dAavkjtv.js" as="script"><link rel="prefetch" href="/blog/assets/026_distribution_and_parallelism_2.html-Oarj2nhk.js" as="script"><link rel="prefetch" href="/blog/assets/027_distribution_and_parallelism_3.html-77Cvv0oS.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-PBsY_rap.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Zcp-eU4_.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-rkj_7lo2.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-QLwL-7Ep.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Oxf_ebnS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5D2IjCOg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-A31xeYwf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-msPfzzJg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DZdGhB_L.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-boE_F-iQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-h_NjiMfU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-p2Tsu2p1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_xON_iJC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B2Pkj6YQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-W452oCoL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TmPJxG-7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-V70cqVcH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B_T6Dw-T.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-dPdCaTCj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-A2W7lKnn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-j29Sywkd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-1H13OqYN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0J9j5_9y.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-6XaTVS8A.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0XfA9SW_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-tAjPVsUY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-IHCZ1ifx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_hyrX4u7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cyiudMuz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9hZ2W15j.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5UYirEbR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-YEuUpom_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-QgDkvTJo.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-27VGLSN_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-m7x4n_H9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-vZ2L26cY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TFKHFmGv.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bOiN38kD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-JVCXdH3X.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-11ZIgyrs.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-3m_mRo18.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-tzLGsnDk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-IxMKj7wS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mYApoqog.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OmzkmQ32.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nWtmao90.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xZKfuCLl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-WqiXb0WV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-w71sLuGm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GtICBxns.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BnLb2pp-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-l_HMqEsW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4ecOk4nY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-r9SC3-tn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Z6k6LLnw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Knbb-9ji.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5x90Xd-z.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2bCBwEM1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kq0jp3Og.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-XbG0eQrg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DDni3m4T.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Kfam6HkQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-tLjhI3cT.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-dgJiYNze.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mLruQIuy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Gk8Wyjk0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-8GIfW0mp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-zzlJAm82.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-gQrm5__9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BflITUVW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Sps6mSh5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZrPf0SHj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Das9heZJ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mcwfHmXR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jyaziJXl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UmpcJspg.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-08_zHRDQ.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><!--[--><div class="theme-container no-sidebar has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/blog/"><img class="vp-nav-logo" src="/blog/blogger.png" alt><!----><span class="vp-site-name hide-in-pad">Liz</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="Home" class="vp-link nav-link nav-link" href="/blog/"><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span>Home<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Project" class="vp-link nav-link nav-link" href="/blog/demo/"><span class="font-icon icon fa-fw fa-sm fas fa-star" style=""></span>Project<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><div class="nav-item"><div class="dropdown-wrapper i18n-dropdown"><button type="button" class="dropdown-title" aria-label="Select language"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="i18n icon" style="width:1rem;height:1rem;vertical-align:middle;"><path d="M379.392 460.8 494.08 575.488l-42.496 102.4L307.2 532.48 138.24 701.44l-71.68-72.704L234.496 460.8l-45.056-45.056c-27.136-27.136-51.2-66.56-66.56-108.544h112.64c7.68 14.336 16.896 27.136 26.112 35.84l45.568 46.08 45.056-45.056C382.976 312.32 409.6 247.808 409.6 204.8H0V102.4h256V0h102.4v102.4h256v102.4H512c0 70.144-37.888 161.28-87.04 210.944L378.88 460.8zM576 870.4 512 1024H409.6l256-614.4H768l256 614.4H921.6l-64-153.6H576zM618.496 768h196.608L716.8 532.48 618.496 768z"></path></svg><!--]--><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="English" class="vp-link nav-link active nav-link active" href="/blog/posts/llm/027_distribution_and_parallelism_3.html"><!---->English<!----></a></li><li class="dropdown-item"><a aria-label="ç®€ä½“ä¸­æ–‡" class="vp-link nav-link nav-link" href="/blog/zh/posts/llm/027_distribution_and_parallelism_3.html"><!---->ç®€ä½“ä¸­æ–‡<!----></a></li></ul></button></div></div><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/liz-in-tech" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Distributed Training Part 4: Parallel Strategies</h1><div class="page-info"><span class="page-author-info" aria-label="AuthorðŸ–Š" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/liz-in-tech" target="_blank" rel="noopener noreferrer">Liz</a></span><span property="author" content="Liz"></span></span><!----><span class="page-date-info" aria-label="Writing DateðŸ“…" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2025-03-04T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="Reading TimeâŒ›" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 8 min</span><meta property="timeRequired" content="PT8M"></span><span class="page-category-info" aria-label="CategoryðŸŒˆ" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category6 clickable" role="navigation">LLM</span><!--]--><meta property="articleSection" content="LLM"></span><span class="page-tag-info" aria-label="TagðŸ·" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag3 clickable" role="navigation">Distributed</span><span class="page-tag-item tag0 clickable" role="navigation">Parallelism</span><!--]--><meta property="keywords" content="Distributed,Parallelism"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_1-five-dimensions-of-parallelization-strategies">1. Five Dimensions of Parallelization Strategies</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_1-1-five-dimensions">1.1. Five Dimensions</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_1-2-combining-multiple-parallel-strategies">1.2. Combining Multiple Parallel Strategies</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_1-3-impact-scope">1.3. Impact Scope</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_1-4-pp-vs-zero-3">1.4. PP vs ZeRO-3</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_1-5-tp-sp-vs-cp-vs-ep">1.5. TP &amp; SP vs CP vs EP</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_1-6-memory-savings-comparison-for-each-parallel-strategy">1.6. Memory Savings Comparison for Each Parallel Strategy</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_2-optimal-training-configuration">2. Optimal Training Configuration</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_2-1-step-1-fitting-a-training-step-in-memory-fit-a-full-model-instance-on-our-gpus">2.1. Step 1: Fitting a Training Step in Memory / Fit a full model instance on our GPUs</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_2-2-step-2-achieving-target-global-batch-size">2.2. Step 2: Achieving Target Global Batch Size</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_2-3-step-3-optimizing-training-throughput-ensure-the-training-is-running-as-fast-as-possible">2.3. Step 3: Optimizing Training Throughput / Ensure the training is running as fast as possible</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_2-4-top-configurations">2.4. Top Configurations</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_3-tensor-parallelism-tp">3. Tensor Parallelism (TP)</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-1-tp-principle">3.1. TP Principle</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-2-tp-application-in-transformer-blocks">3.2. TP Application in Transformer Blocks</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-3-impact-of-scaling-tp-shard-size-on-throughput-and-memory">3.3. Impact of Scaling TP Shard Size on Throughput and Memory</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_4-sequence-parallelism-sp">4. Sequence Parallelism (SP)</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_4-1-tp-only-vs-tp-with-sp">4.1. TP Only vs TP with SP</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_4-2-throughput-and-memory-usage">4.2. Throughput and Memory Usage</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_5-context-parallelism-cp">5. Context Parallelism (CP)</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-1-ring-attention">5.1. Ring Attention</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-2-zig-zag-ring-attention">5.2. Zig-Zag Ring Attention</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_6-pipeline-parallelism-pp">6. Pipeline Parallelism (PP)</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-1-memory-usage">6.1. Memory Usage</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-2-main-challenge-minimize-gpu-computation-idle-time-improve-gpu-utilization">6.2. Main Challenge: Minimize GPU computation idle time, improve GPU utilization</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-3-naive-pp">6.3. Naive PP</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-4-all-forward-all-backward-afab-scheme-forward-then-backward-f-then-b">6.4. All-forward-all-backward (AFAB) Scheme / Forward then Backward / F then B</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-5-one-forward-one-backward-1f1b-and-llama-3-1-schemes">6.5. One-forward-one-backward (1F1B) and LLama 3.1 Schemes</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_7-expert-parallelism-ep">7. Expert Parallelism (EP)</a></li><!----><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h1 id="distributed-training-part-4-parallel-strategies" tabindex="-1"><a class="header-anchor" href="#distributed-training-part-4-parallel-strategies" aria-hidden="true">#</a> Distributed Training Part 4: Parallel Strategies</h1><!-- more --><h2 id="_1-five-dimensions-of-parallelization-strategies" tabindex="-1"><a class="header-anchor" href="#_1-five-dimensions-of-parallelization-strategies" aria-hidden="true">#</a> 1. Five Dimensions of Parallelization Strategies</h2><h3 id="_1-1-five-dimensions" tabindex="-1"><a class="header-anchor" href="#_1-1-five-dimensions" aria-hidden="true">#</a> 1.1. Five Dimensions</h3><ul><li>Data Parallelism (DP) -&gt; batch dimension <ul><li>ZeRO (Zero Redundancy Optimizer) <ul><li>ZeRO-1: optimizer state sharding</li><li>ZeRO-2: optimizer state + gradient sharding</li><li>ZeRO-3 / FSDP (Fully-Sharded Data Parallelism): optimizer state + gradient + parameter sharding</li></ul></li></ul></li><li>Tensor Parallelism (TP) -&gt; hidden_state dimension</li><li>Sequence Parallelism (SP) -&gt; sequence dimension</li><li>Context Parallelism (CP) -&gt; sequence dimension</li><li>Pipeline Parallelism (PP) -&gt; model_layer dimension</li><li>Expert Parallelism (EP) -&gt; model_expert dimension</li></ul><h3 id="_1-2-combining-multiple-parallel-strategies" tabindex="-1"><a class="header-anchor" href="#_1-2-combining-multiple-parallel-strategies" aria-hidden="true">#</a> 1.2. Combining Multiple Parallel Strategies</h3><ul><li>PP + ZeRO-1/ZeRO-2/ZeRO-3 <ul><li>e.g., the training of DeepSeek-v3 used PP combined with ZeRO-1</li></ul></li><li>TP &amp; SP + PP</li><li>TP &amp; SP + ZeRO-3</li><li>CP + EP</li><li>TP &amp; SP + CP + EP + PP + FSDP</li></ul><figure><img src="/blog/assets/027_combine_strategies-v1G8nTkV.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_1-3-impact-scope" tabindex="-1"><a class="header-anchor" href="#_1-3-impact-scope" aria-hidden="true">#</a> 1.3. Impact Scope</h3><ul><li>TP &amp; SP: Affect the entire model&#39;s computation by sharding weights and activations</li><li>CP: Mainly affects the attention layer, as that&#39;s where cross-sequence communication is needed, while other layers run independently on sharded sequences</li><li>EP: Mainly affects MoE layers (these layers replace standard MLP blocks), while attention and other components remain unchanged</li><li>PP: Does not specifically target any submodule or component</li><li>ZeRO: Does not specifically target any submodule or component</li></ul><h3 id="_1-4-pp-vs-zero-3" tabindex="-1"><a class="header-anchor" href="#_1-4-pp-vs-zero-3" aria-hidden="true">#</a> 1.4. PP vs ZeRO-3</h3><p>Commonality: Both are methods of splitting model weights across multiple GPUs and communicating and computing along the model depth axis, with each device computing full layer operations</p><p>Differences:</p><figure><img src="/blog/assets/027_pp_vs_zero3-Y5fGKK90.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_1-5-tp-sp-vs-cp-vs-ep" tabindex="-1"><a class="header-anchor" href="#_1-5-tp-sp-vs-cp-vs-ep" aria-hidden="true">#</a> 1.5. TP &amp; SP vs CP vs EP</h3><figure><img src="/blog/assets/027_tpsp_cp_ep-ML1IozvN.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_1-6-memory-savings-comparison-for-each-parallel-strategy" tabindex="-1"><a class="header-anchor" href="#_1-6-memory-savings-comparison-for-each-parallel-strategy" aria-hidden="true">#</a> 1.6. Memory Savings Comparison for Each Parallel Strategy</h3><figure><img src="/blog/assets/027_memory_usage_comparation-PiBc-0JK.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/blog/assets/027_memory_savings-tjqnAD0r.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_2-optimal-training-configuration" tabindex="-1"><a class="header-anchor" href="#_2-optimal-training-configuration" aria-hidden="true">#</a> 2. Optimal Training Configuration</h2><p>Considerations</p><ul><li>Consider various physical attributes of the compute cluster, network bandwidth, number of GPUs per node, and memory size of each GPU</li><li>Consider model size</li><li>Consider batch size</li></ul><h3 id="_2-1-step-1-fitting-a-training-step-in-memory-fit-a-full-model-instance-on-our-gpus" tabindex="-1"><a class="header-anchor" href="#_2-1-step-1-fitting-a-training-step-in-memory-fit-a-full-model-instance-on-our-gpus" aria-hidden="true">#</a> 2.1. Step 1: Fitting a Training Step in Memory / Fit a full model instance on our GPUs</h3><ul><li>Abundant GPU resources <ul><li>Models with less than 10B parameters <ul><li>Use a single parallel strategy across 8 GPUs <ul><li>e.g., Tensor Parallelism or ZeRO-3/DP with Full Recompute across 8 GPUs</li></ul></li></ul></li><li>Models with parameters between 10B-100B <ul><li>Use hybrid parallel strategies across 8 GPUs <ul><li>TP (TP=8) + PP</li><li>TP (TP=8) + ZeRO-3</li><li>only ZeRO-3</li></ul></li></ul></li><li>At scales of more than 512 GPUs <ul><li>Due to communication costs, pure data parallelism/ZeRO-3 becomes inefficient, and it&#39;s best to combine data parallelism with tensor parallelism or pipeline parallelism</li></ul></li><li>At scales of more than 1024 GPUs <ul><li>Recommended configuration can be tensor parallelism (TP=8) combined with data parallelism (ZeRO-2) and pipeline parallelism</li></ul></li><li>Special considerations <ul><li>For ultra-long sequences: CC</li><li>For MoE architectures: EP</li></ul></li></ul></li><li>Limited GPU resources <ul><li>Full activation recomputation trades time for space (training is somewhat slow)</li><li>Increase gradient accumulation to handle larger batches</li></ul></li></ul><h3 id="_2-2-step-2-achieving-target-global-batch-size" tabindex="-1"><a class="header-anchor" href="#_2-2-step-2-achieving-target-global-batch-size" aria-hidden="true">#</a> 2.2. Step 2: Achieving Target Global Batch Size</h3><ul><li>Increase current global batch size <ul><li>Expand DP or gradient accumulation steps</li><li>For long sequences, adopt CP</li></ul></li><li>Decrease current global batch size <ul><li>Reduce DP</li><li>For long sequences, reduce CP</li></ul></li></ul><h3 id="_2-3-step-3-optimizing-training-throughput-ensure-the-training-is-running-as-fast-as-possible" tabindex="-1"><a class="header-anchor" href="#_2-3-step-3-optimizing-training-throughput-ensure-the-training-is-running-as-fast-as-possible" aria-hidden="true">#</a> 2.3. Step 3: Optimizing Training Throughput / Ensure the training is running as fast as possible</h3><p>When memory and communication are not bottlenecks, try the following:</p><ul><li>Expand TP, using fast intra-node bandwidth until parallelism approaches node size, reducing the use of other parallel methods</li><li>Increase the use of ZeRO-3 data parallelism while maintaining the target batch size</li><li>Transition to using pipeline parallelism when data parallel communication becomes a bottleneck</li><li>Try expanding different parallel methods one by one</li><li>Experiment with various micro-batch sizes (mbs) to find the best balance between maximum global batch size (GBS), model size, computation, and communication</li></ul><h3 id="_2-4-top-configurations" tabindex="-1"><a class="header-anchor" href="#_2-4-top-configurations" aria-hidden="true">#</a> 2.4. Top Configurations</h3><p>Fixed experimental settings: sequence length: 4096 gbs (global batch size): 1M tokens</p><figure><img src="/blog/assets/027_best_configurations-o11yGyWp.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Shows the best configurations for different model sizes and the number of compute nodes (8 GPUs per node), with color indicating MFU (Model FLOPs Utilization), where FLOPs stands for Floating point operations per second, and brighter colors represent higher efficiency</p><p>Included configuration details</p><ul><li>DP</li><li>TP</li><li>PP</li><li>GAS (Gradient Accumulation Steps)</li><li>MBS (Micro Batch Size)</li><li>ZeRO</li></ul><p>Key insights</p><ul><li>Efficiency decreases with increased node count (higher parallelism), more noticeable for smaller models (although batch size can be increased to compensate for small model size, we are constrained by a global batch size limit of 1 million)</li><li>Larger models present different challenges. As model size increases, memory requirements grow significantly. This creates two scenarios with fewer nodes: either the model cannot fit in memory at all, or it barely fits but runs inefficiently due to being close to GPU memory limits (e.g., training an 8 billion parameter model on 4 nodes)</li><li>Performance largely depends on the quality of each parallel strategy&#39;s specific implementation (when we first implemented these two parallel strategies, tensor parallelism (TP) outperformed pipeline parallelism (PP). After optimizing our PP code, it became the faster choice. We are now improving communication overlap in the TP implementation, expecting it to regain performance leadership.)</li></ul><h2 id="_3-tensor-parallelism-tp" tabindex="-1"><a class="header-anchor" href="#_3-tensor-parallelism-tp" aria-hidden="true">#</a> 3. Tensor Parallelism (TP)</h2><h3 id="_3-1-tp-principle" tabindex="-1"><a class="header-anchor" href="#_3-1-tp-principle" aria-hidden="true">#</a> 3.1. TP Principle</h3><ul><li>ZeRO shards model parameters, gradients, and optimizer states, but once activation memory exceeds our memory budget, we hit a limitation.</li><li>At this point, we introduce Tensor Parallelism (TP), a method that shards not only weights, gradients, and optimizer states but also activations, without needing to gather them all before computation.</li></ul><p>TP&#39;s principle leverages the mathematical properties of matrix multiplication:</p><figure><img src="/blog/assets/027_matrix_math-tR8yvVYh.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Example: How to apply TP to the following computation</p><figure><img src="/blog/assets/027_tp_example-yxpHhuav.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Option 1: Column-wise sharding / column-linear</p><ul><li>Broadcast X</li><li>Column-shard W</li><li>All-gather to obtain Y</li></ul><figure><img src="/blog/assets/027_tp_column_linear-dA3LCRhR.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Option 2: Row-wise sharding / row-linear</p><ul><li>Scatter X column-wise</li><li>Row-shard W</li><li>All-reduce to obtain Y</li></ul><figure><img src="/blog/assets/027_tp_row_linear-DJPB9Iiy.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_3-2-tp-application-in-transformer-blocks" tabindex="-1"><a class="header-anchor" href="#_3-2-tp-application-in-transformer-blocks" aria-hidden="true">#</a> 3.2. TP Application in Transformer Blocks</h3><p>Two main blocks of Transformer:</p><ul><li>MLP / Feedforward layers</li><li>MHA / Multi-Head Attention</li></ul><h4 id="_3-2-1-mlp-block-column-wise-sharding-row-wise-sharding" tabindex="-1"><a class="header-anchor" href="#_3-2-1-mlp-block-column-wise-sharding-row-wise-sharding" aria-hidden="true">#</a> 3.2.1. MLP Block: Column-wise Sharding -&gt; Row-wise Sharding</h4><figure><img src="/blog/assets/027_tp_mlp-Jryv6bBS.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/blog/assets/027_tp_region_mlp-63_r0hn1.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>The all-reduce operation in the figure is necessary and cannot overlap with GPU computation</li><li>TP helps reduce activation memory for matrix multiplication, but we still need to gather full activations for LayerNorm computation</li></ul><h4 id="_3-2-2-mha-block" tabindex="-1"><a class="header-anchor" href="#_3-2-2-mha-block" aria-hidden="true">#</a> 3.2.2. MHA Block:</h4><ul><li>Q, K, V matrices: Use column-wise sharding <ul><li>Multi-head: Each head of multi-head attention is inherently parallel, and TP takes advantage of this</li><li>MQA (Multi-Query Attention): All Qs share a set of K and V</li><li>GQA (Grouped-Query Attention): Multiple Qs share a set of K and V (group sharing)</li><li>Column-wise sharding in TP is also applicable to MQA and GQA, as the shared nature of K and V does not affect the independent computation of heads</li><li>Constraints <ul><li>TP shard count should not exceed the number of Q/K/V heads (otherwise, independent computation is not possible, requiring additional communication)</li><li>GQA shard count should not exceed the number of K/V heads (otherwise, K/V heads need to be duplicated to maintain synchronization), e.g., Llama-3 8B has 8 K/V heads, so TP shard count should not exceed 8</li></ul></li></ul></li><li>O matrix: Use row-wise sharding</li></ul><figure><img src="/blog/assets/027_tp_mha-dRkkvl3k.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_3-3-impact-of-scaling-tp-shard-size-on-throughput-and-memory" tabindex="-1"><a class="header-anchor" href="#_3-3-impact-of-scaling-tp-shard-size-on-throughput-and-memory" aria-hidden="true">#</a> 3.3. Impact of Scaling TP Shard Size on Throughput and Memory</h3><figure><img src="/blog/assets/027_tp_scale-wyNMkcli.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Trade-offs</p><ul><li>Computational efficiency: Increasing TP shard size reduces throughput (significant drop from TP=8 to TP=16, steeper drop from TP=16 to TP=32, with the drop becoming more severe as TP increases)</li><li>Available memory: Increasing TP shard size allows for handling larger batch sizes</li></ul><figure><img src="/blog/assets/027_tp_memory_usage-GKHC2Xky.png" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h2 id="_4-sequence-parallelism-sp" tabindex="-1"><a class="header-anchor" href="#_4-sequence-parallelism-sp" aria-hidden="true">#</a> 4. Sequence Parallelism (SP)</h2><ul><li>Sequence Parallelism (SP) involves sharding activations and computations of parts of the model not handled by Tensor Parallelism (TP), such as Dropout and LayerNorm, but sharding is done along the input sequence dimension rather than the hidden dimension.</li><li>Sequence parallelism here is tightly coupled with tensor parallelism, mainly applied to Dropout and LayerNorm operations (For example, LayerNorm needs the full hidden dimension to compute mean and variance)</li><li>However, when dealing with longer sequences, attention computation becomes a bottleneck, requiring techniques like Ring-Attention, sometimes also referred to as sequence parallelism, but we call them context parallelism to distinguish between the two methods. Therefore, whenever you see &quot;sequence parallelism,&quot; remember it is usually used with tensor parallelism (while context parallelism can be used independently).</li></ul><h3 id="_4-1-tp-only-vs-tp-with-sp" tabindex="-1"><a class="header-anchor" href="#_4-1-tp-only-vs-tp-with-sp" aria-hidden="true">#</a> 4.1. TP Only vs TP with SP</h3><figure><img src="/blog/assets/027_tp_vs_tpsp-h5t0OcUf.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/blog/assets/027_tpsp-sPHKFuAf.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Terminology</p><ul><li>b : batch_size (first dimension of the tensor)</li><li>s : sequence_length (second dimension of the tensor)</li><li>h : hidden_state (third dimension of the tensor)</li></ul><p>Comparison of TP Only and TP with SP:</p><ul><li>The key advantage of sequence parallelism is the reduced maximum activation size that needs to be stored <ul><li>TP Only: Activations of shape (b, s, h) are needed at multiple points, with activation size b * s * h</li><li>TP with SP: Activation shape changes to (b, s, h/k) or (b, s/k, h), reducing maximum activation size to b * s * h / k, where k is the parallelism degree</li></ul></li><li>Both have the same communication overhead in forward and backward passes <ul><li>TP Only: Each Transformer block has 2 all-reduce operations</li><li>TP with SP: Each Transformer block has 2 all-gather operations and 2 reduce-scatter operations, but since all-reduce = reduce-scatter + all-gather, it is equivalent to 2 all-reduce operations, consistent with TP Only</li></ul></li></ul><h4 id="_4-1-1-left-figure-tp-only" tabindex="-1"><a class="header-anchor" href="#_4-1-1-left-figure-tp-only" aria-hidden="true">#</a> 4.1.1. Left Figure: TP Only</h4><ul><li>f and f* operations <ul><li>f operation <ul><li>No operation in forward pass</li><li>All-reduce operation in backward pass</li></ul></li><li>f* operation <ul><li>All-reduce operation in forward pass</li><li>No operation in backward pass</li></ul></li></ul></li><li>Overall tensor changes <ul><li>(b, s, h) -&gt; Full</li><li>f</li><li>(b, s, h/k) -&gt; TP (column-wise sharding -&gt; row-wise sharding)</li><li>f*</li><li>(b, s, h) -&gt; Full</li><li>f</li><li>(b, s, h/k) -&gt; TP (column-wise sharding -&gt; row-wise sharding)</li><li>f*</li><li>(b, s, h) -&gt; Full</li></ul></li></ul><h4 id="_4-1-2-right-figure-tp-with-sp" tabindex="-1"><a class="header-anchor" href="#_4-1-2-right-figure-tp-with-sp" aria-hidden="true">#</a> 4.1.2. Right Figure: TP with SP</h4><ul><li>g and g* operations <ul><li>g operation <ul><li>All-gather operation in forward pass</li><li>Reduce-scatter operation in backward pass</li></ul></li><li>g* operation <ul><li>Reduce-scatter operation in forward pass</li><li>All-gather operation in backward pass</li></ul></li></ul></li><li>Overall tensor changes <ul><li>(b, s/k, h) -&gt; SP</li><li>g</li><li>(b, s, h/k) -&gt; TP (column-wise sharding -&gt; row-wise sharding)</li><li>g*</li><li>(b, s/k, h) -&gt; SP</li><li>g</li><li>(b, s, h/k) -&gt; TP (column-wise sharding -&gt; row-wise sharding)</li><li>g*</li><li>(b, s/k, h) -&gt; SP</li></ul></li></ul><h3 id="_4-2-throughput-and-memory-usage" tabindex="-1"><a class="header-anchor" href="#_4-2-throughput-and-memory-usage" aria-hidden="true">#</a> 4.2. Throughput and Memory Usage</h3><p>TP with SP in the MLP part:</p><ul><li>Like TP Only, GPU communication cannot overlap with GPU computation, making throughput heavily dependent on communication bandwidth</li></ul><figure><img src="/blog/assets/027_tpsp1--MM33BKQ.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Memory usage of a 70B model:</p><figure><img src="/blog/assets/027_tp_memory_usage1-nZZfQ95H.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Impact of scaling TP with SP on throughput and memory utilization for a 3B model with 4096 sequence length:</p><figure><img src="/blog/assets/027_tp_scale1-SYIrodxv.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Similarly, a trade-off between throughput and memory usage is needed</p><h2 id="_5-context-parallelism-cp" tabindex="-1"><a class="header-anchor" href="#_5-context-parallelism-cp" aria-hidden="true">#</a> 5. Context Parallelism (CP)</h2><figure><img src="/blog/assets/027_cp-NFE-Vz8g.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_5-1-ring-attention" tabindex="-1"><a class="header-anchor" href="#_5-1-ring-attention" aria-hidden="true">#</a> 5.1. Ring Attention</h3><figure><img src="/blog/assets/027_ring_attention-otkVRj8M.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>At each time step, each GPU sequentially performs these three operations:</p><ul><li><ol><li>Send current K, V to the next GPU</li></ol></li><li><ol start="2"><li>Compute attention scores locally</li></ol></li><li><ol start="3"><li>Wait to receive K, V from the previous GPU</li></ol></li></ul><p>Problem with naive implementation: Due to masking, data is lower triangular, and Softmax is computed row-wise, leading to imbalanced computation across GPUs</p><figure><img src="/blog/assets/027_causal_attention_mask-KR45d9ve.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_5-2-zig-zag-ring-attention" tabindex="-1"><a class="header-anchor" href="#_5-2-zig-zag-ring-attention" aria-hidden="true">#</a> 5.2. Zig-Zag Ring Attention</h3><p>Balanced computation implementation: Not purely sequential allocation, but mixing front and back tokens onto one GPU</p><figure><img src="/blog/assets/027_zig_zag-G0Bvq0Ek.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Two ways to overlap computation and communication</p><ul><li>All-gather implementation: Reassemble all KVs on each GPU (in the manner of ZeRO-3) <ul><li>Requires temporary storage of all KV pairs</li><li>Communication occurs only in the first step</li></ul></li><li>All-to-all (Ring) implementation: Sequentially gather KVs from each GPU in a ring <ul><li>Requires only temporary storage of an additional block</li><li>Communication overlaps with computation from start to finish, with some latency overhead</li></ul></li></ul><figure><img src="/blog/assets/027_cp_allgather-Tz_Ypeyk.png" alt="All-gather implementation" tabindex="0" loading="lazy"><figcaption>All-gather implementation</figcaption></figure><figure><img src="/blog/assets/027_cp_alltoall-ioER_nRf.png" alt="All-to-all (Ring) implementation" tabindex="0" loading="lazy"><figcaption>All-to-all (Ring) implementation</figcaption></figure><h2 id="_6-pipeline-parallelism-pp" tabindex="-1"><a class="header-anchor" href="#_6-pipeline-parallelism-pp" aria-hidden="true">#</a> 6. Pipeline Parallelism (PP)</h2><p>Pipeline parallelism: Distribute model layers across multiple GPUs, also known as &quot;inter-layer parallelism&quot;</p><h3 id="_6-1-memory-usage" tabindex="-1"><a class="header-anchor" href="#_6-1-memory-usage" aria-hidden="true">#</a> 6.1. Memory Usage</h3><figure><img src="/blog/assets/027_pp_memory_usage-WZGB_6dC.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Each GPU still processes the entire batch of data, just on different layers, so activation memory size is complete. Activations are processed on one GPU&#39;s layers and then sent to the next GPU to continue forward propagation</p><h3 id="_6-2-main-challenge-minimize-gpu-computation-idle-time-improve-gpu-utilization" tabindex="-1"><a class="header-anchor" href="#_6-2-main-challenge-minimize-gpu-computation-idle-time-improve-gpu-utilization" aria-hidden="true">#</a> 6.2. Main Challenge: Minimize GPU computation idle time, improve GPU utilization</h3><p>Example: 16-layer model distributed across 4 GPUs</p><ul><li>t<sub>f</sub>: Time for forward propagation</li><li>t<sub>b</sub>: Time for backward propagation</li><li>A simple assumption: t<sub>b</sub> = 2 * t<sub>f</sub></li></ul><h3 id="_6-3-naive-pp" tabindex="-1"><a class="header-anchor" href="#_6-3-naive-pp" aria-hidden="true">#</a> 6.3. Naive PP</h3><ul><li>Ideal total time: t<sub>ideal</sub> = t<sub>f</sub> + t<sub>b</sub></li><li>Idle time: t<sub>pipeline_bubble</sub> = (p - 1) * (t<sub>f</sub> + t<sub>b</sub>), where p is the parallelism degree</li><li>Ratio of idle time to ideal time: r<sub>bubble</sub> = (p - 1) * (t<sub>f</sub> + t<sub>b</sub>) / (t<sub>f</sub> + t<sub>b</sub>) = p - 1</li></ul><figure><img src="/blog/assets/027_naive_pp-TioArJ31.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_6-4-all-forward-all-backward-afab-scheme-forward-then-backward-f-then-b" tabindex="-1"><a class="header-anchor" href="#_6-4-all-forward-all-backward-afab-scheme-forward-then-backward-f-then-b" aria-hidden="true">#</a> 6.4. All-forward-all-backward (AFAB) Scheme / Forward then Backward / F then B</h3><ul><li>Divide the batch into micro-batches, with numbers in the blocks representing micro-batches</li><li>Each batch is divided into 8 micro-batches, numbers 9-16 are micro-batches of the next batch</li><li>Assume the model has 4 layers, with one layer on each GPU</li><li>AFAB means starting the backward pass for all micro-batches of a batch only after all forward passes are completed</li></ul><figure><img src="/blog/assets/027_pp_afab-fYB9xx0E.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Problem: Need to store all activations (only after all forward passes are completed and the backward pass of a micro-batch is completed can the activations of that micro-batch be released)</p><h3 id="_6-5-one-forward-one-backward-1f1b-and-llama-3-1-schemes" tabindex="-1"><a class="header-anchor" href="#_6-5-one-forward-one-backward-1f1b-and-llama-3-1-schemes" aria-hidden="true">#</a> 6.5. One-forward-one-backward (1F1B) and LLama 3.1 Schemes</h3><h4 id="_6-5-1-non-interleaved-schedule-default" tabindex="-1"><a class="header-anchor" href="#_6-5-1-non-interleaved-schedule-default" aria-hidden="true">#</a> 6.5.1. Non-interleaved Schedule (default)</h4><ul><li>Compared to AFAB, start the backward pass of a micro-batch as soon as its forward pass is completed</li><li>Each micro-batch does not synchronize with other micro-batches</li><li>Non-interleaved scheduling can be divided into three phases. The first phase is the warm-up phase, where processors perform different amounts of forward computation. In the next phase, processors perform one forward computation followed by one backward computation. The final phase completes backward computation.</li></ul><figure><img src="/blog/assets/027_pp_1f1b-g3lBcSw9.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Store only partial activations (as soon as a micro-batch&#39;s forward pass is completed, start its backward pass to release activations, while other micro-batches&#39; forward passes are still ongoing)</li><li>1F1B improves memory usage but does not improve idle time</li></ul><h4 id="_6-5-2-interleaving-stages-interleaved-schedule" tabindex="-1"><a class="header-anchor" href="#_6-5-2-interleaving-stages-interleaved-schedule" aria-hidden="true">#</a> 6.5.2. Interleaving Stages / Interleaved Schedule</h4><p>Two main blocks of Transformer:</p><ul><li>MLP / Feedforward layers</li><li>MHA / Multi-Head Attention</li></ul><p>Here, each block represents a computation block, with green blocks representing forward propagation of attention blocks (MHA), cyan blocks representing forward propagation of feedforward networks (MLP), pink blocks representing backward propagation of MHA, and purple blocks representing backward propagation of MLP, with numbers on the blocks indicating micro-batch IDs</p><figure><img src="/blog/assets/027_pp_interleaved_schedule-1UoE05Xj.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_7-expert-parallelism-ep" tabindex="-1"><a class="header-anchor" href="#_7-expert-parallelism-ep" aria-hidden="true">#</a> 7. Expert Parallelism (EP)</h2><p>MoE (Mixture-of-Experts) basics: https://huggingface.co/blog/moe</p><figure><img src="/blog/assets/027_ep-WUyOgdDw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Expert Parallelism (EP): Parallelism along the expert dimension</p><ul><li>Each expert&#39;s FFN Layer is completely independent</li><li>Lighter than TP, as it does not require splitting matrix multiplication, only routing hidden states to the correct expert</li><li>Typically used with other parallel strategies, such as DP</li></ul><figure><img src="/blog/assets/027_ep1-cGtqi5ny.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/blog/assets/027_ep2-YcQYADC8.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure></div><!--[--><!----><!--]--><footer class="page-meta"><!----><div class="meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a aria-label="Distributed Training Part 5: Introduction to GPU" class="vp-link nav-link prev nav-link prev" href="/blog/posts/llm/028_distribution_and_parallelism_4.html"><div class="hint"><span class="arrow start"></span>Prev</div><div class="link"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Distributed Training Part 5: Introduction to GPU</div></a><a aria-label="Distributed Training Part 3: Data Parallelism" class="vp-link nav-link next nav-link next" href="/blog/posts/llm/026_distribution_and_parallelism_2.html"><div class="hint">Next<span class="arrow end"></span></div><div class="link">Distributed Training Part 3: Data Parallelism<span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span></div></a></nav><!----><!--[--><!----><!--]--><!--]--></main><!--]--><!----></div><!--]--><!--]--><!----><!--]--></div>
    <script type="module" src="/blog/assets/app-74fYY7Rf.js" defer></script>
  </body>
</html>
