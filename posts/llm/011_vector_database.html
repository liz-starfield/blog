<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.0" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.13" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="alternate" hreflang="zh-cn" href="https://liz-in-tech.github.io/blog/zh/posts/llm/011_vector_database.html"><meta property="og:url" content="https://liz-in-tech.github.io/blog/posts/llm/011_vector_database.html"><meta property="og:site_name" content="Liz"><meta property="og:title" content="Vector Databases and Similarity Search"><meta property="og:description" content="Vector Databases and Similarity Search Vector Embeddings Similarity (Distance) Calculation Between Vectors Traditional Recommendation System Modules Vector Databases Indexing Algorithms: KNN, ANN, NSW, HNSW, PQ Sparse Search, Dense Search, Hybrid Search"><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:locale:alternate" content="zh-CN"><meta property="og:updated_time" content="2024-11-09T12:17:18.000Z"><meta property="article:author" content="Liz"><meta property="article:tag" content="Vector"><meta property="article:tag" content="Vector Database"><meta property="article:tag" content="Search"><meta property="article:tag" content="Index"><meta property="article:published_time" content="2024-11-01T00:00:00.000Z"><meta property="article:modified_time" content="2024-11-09T12:17:18.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Vector Databases and Similarity Search","image":[""],"datePublished":"2024-11-01T00:00:00.000Z","dateModified":"2024-11-09T12:17:18.000Z","author":[{"@type":"Person","name":"Liz","url":"https://github.com/liz-in-tech"}]}</script><link rel="icon" herf="/blogger.png"><link rel="icon" href="/blog/blogger.png"><title>Vector Databases and Similarity Search | Liz</title><meta name="description" content="Vector Databases and Similarity Search Vector Embeddings Similarity (Distance) Calculation Between Vectors Traditional Recommendation System Modules Vector Databases Indexing Algorithms: KNN, ANN, NSW, HNSW, PQ Sparse Search, Dense Search, Hybrid Search">
    <link rel="preload" href="/blog/assets/style-lMqD2uhz.css" as="style"><link rel="stylesheet" href="/blog/assets/style-lMqD2uhz.css">
    <link rel="modulepreload" href="/blog/assets/app-x5fIoSzf.js"><link rel="modulepreload" href="/blog/assets/011_vector_database.html-0CaD_vgm.js"><link rel="modulepreload" href="/blog/assets/011_pq_distance-AD1rQWST.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-x3n3nnut.js"><link rel="modulepreload" href="/blog/assets/011_vector_database.html-CImz0KSx.js">
    <link rel="prefetch" href="/blog/assets/index.html-YbPtte5_.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-CGfhr1vY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FUMOuem4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--TTjrkIy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g4Nfr7z1.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-bitGHKd2.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-nrisQopy.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-9XtwFAwc.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-oji9upQP.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-GS0KjDPy.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-uTrDxa9X.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-SkR0eAcU.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-4RcS3hxb.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-_8stdYVV.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-eluz3bTT.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-kyWTqF2Z.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-1CqW55t5.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-OQBAAMSx.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-Z-6jMDVd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZDCSnlc1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZYw6WxxA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OavE9BET.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-hE_T0u_5.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-iH0mq6XB.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-WyFhRqF6.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-0uw54xim.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-gL7Qg_J4.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-8B4QlsI8.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-hz6Q-DAA.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-3KMCwhrh.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-7gxKMp_3.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-hkr_5TZF.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-Akmj-Ub_.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-bXz0YlpJ.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-_lpeafZW.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-fuedF1il.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wh_dBtOR.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-cxLWDy2T.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kf4JCRaf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-RkA-insV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4kI_oqSd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-WhuidxNt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OqGkeUA_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5GeN-sdD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-c4Rf4yh1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2r0jUs7o.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BSKRXRQc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-KjTsJ0Hg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TteIwMx3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g00XXzrL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-YxbJgo4L.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-3T79Cy0i.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-A5tlQHan.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-04ff5e0O.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-vgZ6rfFh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OmipPplE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-E1KrJL6a.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oPH9QkTj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cHRqZSs8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-74SU9ZTn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--iJiA8oX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oZPWb_Fc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FUIWSLsp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5ERWyusD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Zjn0JNqd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jcvPTrgB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9t2TsyuQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CcLVFNIv.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-LHukpLS7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-howjHe2f.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DeN_iOWx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cnlzR0a7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0P_c_pcU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-dy_CcFmq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FzFytZ_p.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2KSwV7xp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-F1coElwg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ffflqCb9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-F8ZuLYgH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-HeYWaFeL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xKYeJEc5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xB-iS7Ql.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OEUPqfTV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-EE4iQI9m.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g1PUF_BG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nUBcs85a.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9Y3la5Sf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wufIFDPM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-X7WKa8oH.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-dOWnOzNI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-qhsmgah_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kxKjnuC3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-yaNV6FSD.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-xp-mvxY3.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-M_4oCxuy.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-1H3JbWvF.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-sm92v0S4.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-3_tLfNve.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-OO5EBAeO.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-cQ2oJLOK.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-RM21nc6t.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-i6RDLLrH.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-gbw5-WLj.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-BCA8NNW2.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-fEr0Ypul.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-4OqZ5kUZ.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-WR75rRD2.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Ja-tR156.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7nhlyCVq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-sfcwxP0I.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-uD3U8C5T.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-ev3pjJMQ.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-Cwl0gUh6.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-DdsNNA2T.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-0VBqc0Zw.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-ueAXmPCM.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-bIVtDsEt.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-gWUn35wJ.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-mvHKTmfE.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-O6Qjr3Bz.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-mB3pe3n_.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-ga89ubq6.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-2_01knhZ.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-mFj3h9BE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OlxyLKE3.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-KeaqV3k5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZhOD0ksx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-lXvJB-OD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xv2JNvDn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-IbRNhxKo.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-YrKcAkff.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-f9kIC7BL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-X8db9wIM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ILb5D7Kx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-6X7krbra.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-moW2eDt0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-iXseG8WK.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SZmc7tn7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-p_q6tE7u.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-t5Aop33N.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-k7MYHPzr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nA4KpoLw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-euy8hy4E.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Go-dK6nY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-A2ygF1yM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nWk6rUFN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--7Sg94tT.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0tZL4EpD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-P7PIcv2U.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wASMVm-Q.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bn7NN_Xf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-XEo2fLEx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-1-o8YcoT.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rlfIQTwg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OtDqbEzl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-WKILMeGX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-zPDYW2O8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-IHhp0UGY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Cp5ip8Sa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-L59epuJc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9Wu_DyT6.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5VY-3Bmk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-gFSPmdNb.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GC92g2S5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OsTluYDu.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-t-oQXzB5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-We5SjoZH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0vhilZiW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Z0jrX4CJ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-sFlfnyt7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-E4E1CfmI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-MUmrIPWT.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-gU9hYvhO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-1kQJPfCy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bs0NQPT_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-x0Igm1_-.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-08_zHRDQ.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><!--[--><div class="theme-container no-sidebar has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/blog/"><img class="vp-nav-logo" src="/blog/blogger.png" alt><!----><span class="vp-site-name hide-in-pad">Liz</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="Home" class="vp-link nav-link nav-link" href="/blog/"><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span>Home<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Project" class="vp-link nav-link nav-link" href="/blog/demo/"><span class="font-icon icon fa-fw fa-sm fas fa-star" style=""></span>Project<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><div class="nav-item"><div class="dropdown-wrapper i18n-dropdown"><button type="button" class="dropdown-title" aria-label="Select language"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="i18n icon" style="width:1rem;height:1rem;vertical-align:middle;"><path d="M379.392 460.8 494.08 575.488l-42.496 102.4L307.2 532.48 138.24 701.44l-71.68-72.704L234.496 460.8l-45.056-45.056c-27.136-27.136-51.2-66.56-66.56-108.544h112.64c7.68 14.336 16.896 27.136 26.112 35.84l45.568 46.08 45.056-45.056C382.976 312.32 409.6 247.808 409.6 204.8H0V102.4h256V0h102.4v102.4h256v102.4H512c0 70.144-37.888 161.28-87.04 210.944L378.88 460.8zM576 870.4 512 1024H409.6l256-614.4H768l256 614.4H921.6l-64-153.6H576zM618.496 768h196.608L716.8 532.48 618.496 768z"></path></svg><!--]--><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="English" class="vp-link nav-link active nav-link active" href="/blog/posts/llm/011_vector_database.html"><!---->English<!----></a></li><li class="dropdown-item"><a aria-label="ç®€ä½“ä¸­æ–‡" class="vp-link nav-link nav-link" href="/blog/zh/posts/llm/011_vector_database.html"><!---->ç®€ä½“ä¸­æ–‡<!----></a></li></ul></button></div></div><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/liz-in-tech" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Vector Databases and Similarity Search</h1><div class="page-info"><span class="page-author-info" aria-label="AuthorðŸ–Š" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/liz-in-tech" target="_blank" rel="noopener noreferrer">Liz</a></span><span property="author" content="Liz"></span></span><!----><span class="page-date-info" aria-label="Writing DateðŸ“…" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2024-11-01T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="Reading TimeâŒ›" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 10 min</span><meta property="timeRequired" content="PT10M"></span><span class="page-category-info" aria-label="CategoryðŸŒˆ" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category6 clickable" role="navigation">LLM</span><!--]--><meta property="articleSection" content="LLM"></span><span class="page-tag-info" aria-label="TagðŸ·" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag6 clickable" role="navigation">Vector</span><span class="page-tag-item tag4 clickable" role="navigation">Vector Database</span><span class="page-tag-item tag8 clickable" role="navigation">Search</span><span class="page-tag-item tag2 clickable" role="navigation">Index</span><!--]--><meta property="keywords" content="Vector,Vector Database,Search,Index"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_1-vector-embeddings">1. Vector Embeddings</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_2-similarity-distance-calculation-between-vectors">2. Similarity (Distance) Calculation Between Vectors</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_3-traditional-recommendation-system-modules">3. Traditional Recommendation System Modules</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-1-classification">3.1. Classification</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-2-input">3.2. Input</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-3-goal">3.3. Goal</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-4-approach">3.4. Approach</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-5-traditional-recommendation-system-process">3.5. Traditional Recommendation System Process</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-6-recommendation-systems-combined-with-llms">3.6. Recommendation Systems Combined with LLMs</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_4-vector-databases">4. Vector Databases</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_4-1-vector-databases-solving-efficiency-issues">4.1. Vector Databases: Solving Efficiency Issues</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_4-2-vector-database-products">4.2. Vector Database Products</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_4-3-vector-database-comparison">4.3. Vector Database Comparison</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_4-4-indexing-algorithms">4.4. Indexing Algorithms</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_4-5-index-evaluation-criteria">4.5. Index Evaluation Criteria</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_5-search">5. Search</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-1-sparse-search-keyword-search-vs-dense-search-vector-search-semantic-search">5.1. Sparse Search (Keyword Search) vs Dense Search (Vector Search/Semantic Search)</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-2-sparse-search">5.2. Sparse Search</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-3-hybrid-search">5.3. Hybrid Search</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-4-multilingual-search">5.4. Multilingual Search</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_6-keyword-matching-algorithm-bm25-best-matching-25">6. Keyword Matching Algorithm: BM25 (Best Matching 25)</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_7-inverted-index-inverted-file-index-ivf-fast-text-search">7. Inverted Index / Inverted File Index (IVF) - Fast Text Search</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_8-knn-k-nearest-neighbors-algorithm-knn-search-brute-force-search">8. KNN (K-Nearest Neighbors Algorithm, KNN Search, Brute Force Search)</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_9-approximate-knn-dividing-space-into-several-modules">9. Approximate KNN - Dividing Space into Several Modules</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_9-1-boundary-problem">9.1. Boundary Problem</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_10-ann-approximate-nearest-neighbors-algorithm">10. ANN (Approximate Nearest Neighbors Algorithm)</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_11-nsw-navigable-small-world">11. NSW (Navigable Small World)</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_11-1-graph-construction-one-by-one">11.1. Graph Construction (one by one)</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_11-2-search">11.2. Search</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_12-hnsw-hierarchical-navigable-small-world">12. HNSW (Hierarchical Navigable Small World)</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_12-1-building-hierarchical-structure">12.1. Building - Hierarchical Structure</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_12-2-querying">12.2. Querying</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_12-3-further-optimization-hnsw-approximate-knn">12.3. Further Optimization: HNSW + Approximate KNN</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_13-product-quantization-pq">13. Product Quantization (PQ)</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_13-1-construction">13.1. Construction</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_13-2-querying">13.2. Querying</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_13-3-further-optimization">13.3. Further Optimization</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_14-references">14. References</a></li><!----><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h1 id="vector-databases-and-similarity-search" tabindex="-1"><a class="header-anchor" href="#vector-databases-and-similarity-search" aria-hidden="true">#</a> Vector Databases and Similarity Search</h1><ul><li><ol><li>Vector Embeddings</li></ol></li><li><ol start="2"><li>Similarity (Distance) Calculation Between Vectors</li></ol></li><li><ol start="3"><li>Traditional Recommendation System Modules</li></ol></li><li><ol start="4"><li>Vector Databases</li></ol></li><li><ol start="5"><li>Indexing Algorithms: KNN, ANN, NSW, HNSW, PQ</li></ol></li><li><ol start="6"><li>Sparse Search, Dense Search, Hybrid Search</li></ol></li></ul><!-- more --><h2 id="_1-vector-embeddings" tabindex="-1"><a class="header-anchor" href="#_1-vector-embeddings" aria-hidden="true">#</a> 1. Vector Embeddings</h2><ul><li>Vector embeddings capture the underlying meaning of the data and can be viewed as a machine-readable format of the data.</li><li>Embedding Models <ul><li>Open-source library: sentence-transformers <ul><li>Available via the Hugging Face model hub or directly from the source code repository.</li><li>Provides embedding models with dimensions in the range of 384, 512, and 768.</li></ul></li><li>Paid API Services <ul><li>OpenAI Embedding API <ul><li>Higher dimensionality with better quality, reaching thousands of dimensions.</li></ul></li><li>Cohere Embedding API <ul><li>Known for high-quality multilingual embedding models that outperform open-source variants.</li></ul></li></ul></li></ul></li><li>Embedding Evaluation Leaderboardï¼š MTEB LeaderBoard: https://huggingface.co/spaces/mteb/leaderboard</li></ul><h2 id="_2-similarity-distance-calculation-between-vectors" tabindex="-1"><a class="header-anchor" href="#_2-similarity-distance-calculation-between-vectors" aria-hidden="true">#</a> 2. Similarity (Distance) Calculation Between Vectors</h2><p>The greater the similarity, the smaller the distance; the two metrics are inversely related.</p><p>4 Distance Metrics</p><p>Dot Product and Cosine Distance are commonly used in the field of NLP, to evaluate how similar two sentence embeddings are.</p><ul><li>Euclidean Distance (L2) <ul><li>Calculates the shortest path.</li><li>Smaller values indicate better matches.</li></ul></li></ul><figure><img src="/blog/assets/011_euclidean_distance-MWqly83b.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Manhattan Distance (L1) <ul><li>Moves one direction axis at a time.</li><li>Smaller values indicate better matches.</li></ul></li></ul><figure><img src="/blog/assets/011_manhattan_distance-U28P_uAi.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Dot Product <ul><li>The projection of one vector onto another.</li><li>Produces a non-normalized value with arbitrary size.</li><li>Larger values indicate better matches, while negative values usually indicate greater distance.</li></ul></li></ul><figure><img src="/blog/assets/011_dot_product-qAW9G8qQ.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Cosine Distance <ul><li>Calculates the angle between vectors.</li><li>Produces a normalized value (between -1 and 1).</li><li>Smaller values indicate better matches.</li></ul></li></ul><figure><img src="/blog/assets/011_cosine_distance-UmtgmK-M.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_3-traditional-recommendation-system-modules" tabindex="-1"><a class="header-anchor" href="#_3-traditional-recommendation-system-modules" aria-hidden="true">#</a> 3. Traditional Recommendation System Modules</h2><h3 id="_3-1-classification" tabindex="-1"><a class="header-anchor" href="#_3-1-classification" aria-hidden="true">#</a> 3.1. Classification</h3><ul><li>Content-Based Recommendation <ul><li>Based on previously liked content.</li><li>Pros: Recommendations are explicitly aligned with user interests.</li><li>Cons: Does not recommend types of content not previously encountered.</li></ul></li><li>Collaborative Filtering Recommendation <ul><li>Recommends content liked by friends.</li><li>Pros: Can recommend types of content previously unseen.</li><li>Cons: Might recommend content the user is not interested in.</li></ul></li></ul><h3 id="_3-2-input" tabindex="-1"><a class="header-anchor" href="#_3-2-input" aria-hidden="true">#</a> 3.2. Input</h3><ul><li><ol><li>Behavior (e.g., articles viewed, articles clicked)</li></ol></li><li><ol start="2"><li>User&#39;s basic information</li></ol></li><li><ol start="3"><li>Article list</li></ol></li></ul><h3 id="_3-3-goal" tabindex="-1"><a class="header-anchor" href="#_3-3-goal" aria-hidden="true">#</a> 3.3. Goal</h3><ul><li>Recommend new content.</li></ul><h3 id="_3-4-approach" tabindex="-1"><a class="header-anchor" href="#_3-4-approach" aria-hidden="true">#</a> 3.4. Approach</h3><ul><li>Build user profiles based on behavior and basic information. <ul><li>User vectors (similar to tags, but these are all vectors).</li><li>Tags (liked topics, disliked topics, preferences, etc.) <ul><li>A collection of tags from articles the user has viewed (both provided and learned).</li></ul></li></ul></li><li>Recall candidates from the article database based on tags. <ul><li>Multiple recall iterations can be designed.</li></ul></li><li>Coarse ranking + fine ranking. <ul><li>Multiple ranking iterations can be designed.</li><li>Considerations: <ul><li>Changes in user interests.</li><li>Source and timeliness of articles.</li><li>More detailed exploration of user interests.</li></ul></li></ul></li><li>Top 10 articles.</li></ul><h3 id="_3-5-traditional-recommendation-system-process" tabindex="-1"><a class="header-anchor" href="#_3-5-traditional-recommendation-system-process" aria-hidden="true">#</a> 3.5. Traditional Recommendation System Process</h3><p>Article -&gt; User Profile -&gt; Recall -&gt; Candidates -&gt; Rank -&gt; Top 10</p><ul><li><ol><li>Article -&gt; User Profile</li></ol></li><li><ol start="2"><li>User Profile -&gt; Recall</li></ol></li><li><ol start="3"><li>Recall -&gt; Candidates</li></ol></li><li><ol start="4"><li>Candidates -&gt; Rank</li></ol></li><li><ol start="5"><li>Rank -&gt; Top 10</li></ol></li></ul><h3 id="_3-6-recommendation-systems-combined-with-llms" tabindex="-1"><a class="header-anchor" href="#_3-6-recommendation-systems-combined-with-llms" aria-hidden="true">#</a> 3.6. Recommendation Systems Combined with LLMs</h3><p>Consider which processes can incorporate LLMs:</p><ul><li>Step 1 <ul><li>Use LLM to extract tags.</li></ul></li><li>Step 4 <ul><li>Use LLM for ranking.</li></ul></li></ul><h2 id="_4-vector-databases" tabindex="-1"><a class="header-anchor" href="#_4-vector-databases" aria-hidden="true">#</a> 4. Vector Databases</h2><h3 id="_4-1-vector-databases-solving-efficiency-issues" tabindex="-1"><a class="header-anchor" href="#_4-1-vector-databases-solving-efficiency-issues" aria-hidden="true">#</a> 4.1. Vector Databases: Solving Efficiency Issues</h3><p>Vector databases have existed prior to the explosion of generative AI and have long been part of semantic search applications, which search based on the meaning similarity of words or phrases rather than exact keyword matching.</p><p>The main goal of vector databases is to provide a fast and efficient way to store and perform semantic query data.</p><ul><li>Previously: Similarity calculations with each vector.</li><li>Now: Approximate search.</li></ul><h3 id="_4-2-vector-database-products" tabindex="-1"><a class="header-anchor" href="#_4-2-vector-database-products" aria-hidden="true">#</a> 4.2. Vector Database Products</h3><ul><li>Open Source &amp; Closed Source <ul><li>Among all listed options, only one is completely closed-source: Pinecone. Zilliz is also a fully closed commercial solution but is entirely built on Milvus and can be considered the parent company of Milvus.</li></ul></li></ul><figure><img src="/blog/assets/011_vector_database_products-DYfds2pt.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Development History <ul><li>Vespa was one of the first major vendors to incorporate vector similarity search alongside the mainstream BM25 keyword search algorithm.</li><li>Established vendors like Elasticsearch, Redis, and PostgreSQL only began offering vector search in 2022 and beyond, much later than originally anticipated.</li></ul></li></ul><figure><img src="/blog/assets/011_vector_database_development-oWrZ6VI3.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Hosting Options <ul><li>Self-hosted (On-Premises) <ul><li>Follows a client-server architecture.</li></ul></li><li>Managed (Cloud-Native) <ul><li>Follows a client-server architecture.</li></ul></li><li>Recent Option: Embedded Mode <ul><li>The database itself is tightly coupled with application code, running in a serverless manner.</li><li>Currently, only Chroma and LanceDB are available as embedded databases.</li></ul></li></ul></li></ul><figure><img src="/blog/assets/011_vector_database_manage-x7tsKVq6.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_4-3-vector-database-comparison" tabindex="-1"><a class="header-anchor" href="#_4-3-vector-database-comparison" aria-hidden="true">#</a> 4.3. Vector Database Comparison</h3><ul><li>Trade-offs <ul><li>Hybrid search or keyword search? A hybrid of keyword and vector search can yield the best results; each vector database vendor recognizes this and provides their own customized hybrid search solution.</li><li>On-premises or cloud-native deployment? Many vendors tout &quot;cloud-native&quot; as a selling point, as if infrastructure is the biggest pain point globally, but on-premises deployment might be more cost-effective in the long run.</li><li>Open source or fully managed? Most vendors build on accessible or open-source code to demonstrate their foundational approaches and then offer deployment and infrastructure through fully managed SaaS. While self-hosting many solutions is still possible, it requires additional manpower and internal skill requirements.</li></ul></li></ul><figure><img src="/blog/assets/011_vector_database_compare-XNX2b1Wi.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/blog/assets/011_vector_database_compare1-0Rwj4vnv.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_4-4-indexing-algorithms" tabindex="-1"><a class="header-anchor" href="#_4-4-indexing-algorithms" aria-hidden="true">#</a> 4.4. Indexing Algorithms</h3><p>Data is stored in vector databases through indexing, which refers to creating data structures called indexes for efficient vector lookup by rapidly narrowing the search space.</p><p>As with most cases, choosing a vector index involves a trade-off between precision (accuracy/recall) and speed/throughput.</p><ul><li>BM25</li><li>IVF (Inverted File)</li><li>HNSW (Hierarchical Navigable Small World)</li><li>PQ (Product Quantization)</li><li>Flat</li><li>RHNSW</li><li>Vamana (used in the DiskANN implementation)</li></ul><figure><img src="/blog/assets/011_vector_database_index-IrhnNrO8.png" alt="Indexing algorithms used by various vector databases" tabindex="0" loading="lazy"><figcaption>Indexing algorithms used by various vector databases</figcaption></figure><figure><img src="/blog/assets/011_vector_database_index1-k4HlBe--.png" alt="Classification of vector indexes based on underlying data structures" tabindex="0" loading="lazy"><figcaption>Classification of vector indexes based on underlying data structures</figcaption></figure><h3 id="_4-5-index-evaluation-criteria" tabindex="-1"><a class="header-anchor" href="#_4-5-index-evaluation-criteria" aria-hidden="true">#</a> 4.5. Index Evaluation Criteria</h3><p>The evaluation of an index&#39;s quality always depends on the specific data model, generally including the following points:</p><ul><li>Query Time. The speed of queries is crucial, especially in large models.</li><li>Query Quality. ANN queries may not always return the most accurate results, but the quality should not deviate too much. Query quality has many metrics, with recall being one of the most commonly used.</li><li>Memory Consumption. The memory consumed by query indexing; looking up in memory is significantly faster than looking up on disk.</li><li>Training Time. Some query methods require training to achieve better performance.</li><li>Write Time. The impact on the index when writing vectors, including all maintenance.</li></ul><h2 id="_5-search" tabindex="-1"><a class="header-anchor" href="#_5-search" aria-hidden="true">#</a> 5. Search</h2><h3 id="_5-1-sparse-search-keyword-search-vs-dense-search-vector-search-semantic-search" tabindex="-1"><a class="header-anchor" href="#_5-1-sparse-search-keyword-search-vs-dense-search-vector-search-semantic-search" aria-hidden="true">#</a> 5.1. Sparse Search (Keyword Search) vs Dense Search (Vector Search/Semantic Search)</h3><ul><li>Sparse Search (Keyword Search) <ul><li>Keyword search, text matching.</li></ul></li><li>Dense Search (Vector Search/Semantic Search) <ul><li>Vector similarity search.</li><li>Searching for the closest objects in vector space, known as semantic search or vector search.</li><li>For example, &quot;Baby dogs&quot; &lt;=&gt; &quot;Here is content on puppies!&quot;</li><li>Limitations: <ul><li>Influenced by the embedding model used; vector similarity is only effective on the training dataset. If the search content is significantly different from the dataset used to train the embedding model, the search results will be poor due to the mismatch in distribution.</li><li>Do vectors truly represent semantics? They can be seen as hashes rather than true understanding, which can lead to misleading results.</li><li>With a large volume of vector data, search accuracy can significantly decline (refer to https://mp.weixin.qq.com/s/DH4-QCK1U8BYGlAblQLTLw).</li><li>Search is not suitable for data types lacking semantic information, such as sequential numbers or tabular data.</li></ul></li></ul></li><li>Hybrid Search <ul><li>Combines the ranking results of both sparse and dense searches.</li></ul></li></ul><h3 id="_5-2-sparse-search" tabindex="-1"><a class="header-anchor" href="#_5-2-sparse-search" aria-hidden="true">#</a> 5.2. Sparse Search</h3><figure><img src="/blog/assets/011_bag_of_words-p5ZybKb8.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Bag of Words</p><p>The simplest way to perform keyword matching is through Bag of Wordsâ€”counting how many times a word occurs in the query and the data vector, then returning objects with the highest matching word frequencies.</p><p>In practice, this method may capture only a small percentage of the available words, resulting in many zeros in the data.</p><h3 id="_5-3-hybrid-search" tabindex="-1"><a class="header-anchor" href="#_5-3-hybrid-search" aria-hidden="true">#</a> 5.3. Hybrid Search</h3><p>Combines the results of both search types by setting appropriate weight ratios.</p><figure><img src="/blog/assets/011_hybird_search-UJL-ec01.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_5-4-multilingual-search" tabindex="-1"><a class="header-anchor" href="#_5-4-multilingual-search" aria-hidden="true">#</a> 5.4. Multilingual Search</h3><p>In multilingual search scenarios, texts with the same meaning but in different languages will generate very similar (if not identical) embeddings.</p><figure><img src="/blog/assets/011_multilingual_search-FTUfOq1T.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_6-keyword-matching-algorithm-bm25-best-matching-25" tabindex="-1"><a class="header-anchor" href="#_6-keyword-matching-algorithm-bm25-best-matching-25" aria-hidden="true">#</a> 6. Keyword Matching Algorithm: BM25 (Best Matching 25)</h2><p>Belongs to sparse search (keyword search).</p><figure><img src="/blog/assets/011_bm25-IbDhRuhE.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>BM25 performs exceptionally well when searching across multiple keywords.</p><p>Concept: It counts the occurrences of words in the incoming phrases, where more frequently appearing words are weighted as less important during matching, while rarer words score significantly higher.</p><h2 id="_7-inverted-index-inverted-file-index-ivf-fast-text-search" tabindex="-1"><a class="header-anchor" href="#_7-inverted-index-inverted-file-index-ivf-fast-text-search" aria-hidden="true">#</a> 7. Inverted Index / Inverted File Index (IVF) - Fast Text Search</h2><p>Belongs to sparse search (keyword search).</p><p>Used by search engines like Google and Baidu, it is particularly suitable for text retrieval.</p><p>Forward Index: DocId -&gt; Value<br> Inverted Index: Value -&gt; DocId</p><figure><img src="/blog/assets/011_ivf-3_0ytYra.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol><li>Extract words from documents that are not part of the stop words list (function words that have little actual meaning and are used in almost every document).</li><li>Create a mapping between each word and the multiple document IDs it appears in.</li><li>When querying keywords, recall (Recall) all documents containing those keywords.</li><li>When querying a sentence, which has multiple keywords, recall the corresponding documents for each keyword, then take the intersection, rank them, and return the top k documents as results.</li></ol><h2 id="_8-knn-k-nearest-neighbors-algorithm-knn-search-brute-force-search" tabindex="-1"><a class="header-anchor" href="#_8-knn-k-nearest-neighbors-algorithm-knn-search-brute-force-search" aria-hidden="true">#</a> 8. KNN (K-Nearest Neighbors Algorithm, KNN Search, Brute Force Search)</h2><p>nickname</p><ul><li>a flat index</li><li>a brute force</li></ul><p>Brute force search: Calculates similarity with every vector.</p><p>In classical machine learning, this is known as the K-Nearest Neighbors (KNN) algorithm.</p><p>It compares the query vector with each vector in the database.</p><ol><li>Given a Query, find the distance between all vectors and the Query vector.</li><li>Sort all distances.</li><li>Return the top K matching objects that are closest.</li></ol><figure><img src="/blog/assets/011_knn-bsidCzmj.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>The issue with brute force search is the enormous computational cost; the total query time increases with the number of stored objects.</p><p>The more vectors you have, the longer the query takes, and in practical applications, you might be handling tens of millions or even hundreds of millions of objects.</p><figure><img src="/blog/assets/011_knn_runtime_complexity-mu68fq4A.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Advantage: Precise</li><li>Disadvantage: High computational cost, long processing time, time complexity O(n).</li></ul><h2 id="_9-approximate-knn-dividing-space-into-several-modules" tabindex="-1"><a class="header-anchor" href="#_9-approximate-knn-dividing-space-into-several-modules" aria-hidden="true">#</a> 9. Approximate KNN - Dividing Space into Several Modules</h2><figure><img src="/blog/assets/011_split_space-PpUzmT4I.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/blog/assets/011_split_space_search-ODFHYOWa.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol><li>Use a clustering algorithm (like K-means) to initialize k cluster centers (also called centroids).</li><li>Calculate the distance from each sample to the centroids and assign samples to the nearest centroid, forming clusters.</li><li>Divide the space using these centroids and the samples belonging to each cluster, resulting in a Voronoi diagram. <ul><li>The main property of a Voronoi diagram is that the distance from a centroid to any point in its region is less than the distance from that point to any other centroid.</li></ul></li><li>Given a Query, calculate the distance from the Query to each centroid and find the corresponding space for the Query. <ul><li>When given a new object, calculate the distances to all centroids of the Voronoi partitions. Then select the closest centroid and consider the vectors within that partition as candidates.</li></ul></li><li>The Query then computes distances with all nodes in the space of the centroid. <ul><li>By calculating distances to candidates and selecting the top k closest, it returns the final answer.</li></ul></li></ol><p>If there are k centroids, then the average number of nodes per space is n/k, resulting in a time complexity of O(k + n/k).</p><h3 id="_9-1-boundary-problem" tabindex="-1"><a class="header-anchor" href="#_9-1-boundary-problem" aria-hidden="true">#</a> 9.1. Boundary Problem</h3><p>In the image below, we can see a situation where the actual nearest neighbor is located in the red area, but we only select candidates from the green area. This is referred to as the boundary problem.</p><figure><img src="/blog/assets/011_border_problem-xd6mTnQj.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>The boundary problem essentially arises because:</p><p>The Voronoi diagram only guarantees that the distance from a node to its region&#39;s centroid is less than to other centroids, but does not guarantee that the distance from a node to other nodes within the region is less than to nodes in other regions.</p><p><strong>Solution:</strong> Expand the search range.</p><p>This situation often occurs when the query object is near the boundary of another region. To reduce errors in such cases, we can increase the search range and select several regions to search candidates based on the nearest m centroids.</p><figure><img src="/blog/assets/011_enlarge_regions-kOf4xFOs.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>The time complexity for expanding the search range is O(k + 3 * n/k).</p><p>The more regions searched, the more accurate the results, but the time required to compute them increases. A trade-off must be made between accuracy and processing time.</p><h2 id="_10-ann-approximate-nearest-neighbors-algorithm" tabindex="-1"><a class="header-anchor" href="#_10-ann-approximate-nearest-neighbors-algorithm" aria-hidden="true">#</a> 10. ANN (Approximate Nearest Neighbors Algorithm)</h2><p>It doesn&#39;t always find the best match, but it finds approximate nearest neighbor matches, which is still a fairly good result, even if it&#39;s not perfect.</p><p>By sacrificing some accuracy (not always returning the true nearest neighbors), substantial performance improvements can be achieved using ANN algorithms.</p><ul><li>NSW</li><li>HNSW</li></ul><h2 id="_11-nsw-navigable-small-world" tabindex="-1"><a class="header-anchor" href="#_11-nsw-navigable-small-world" aria-hidden="true">#</a> 11. NSW (Navigable Small World)</h2><h3 id="_11-1-graph-construction-one-by-one" tabindex="-1"><a class="header-anchor" href="#_11-1-graph-construction-one-by-one" aria-hidden="true">#</a> 11.1. Graph Construction (one by one)</h3><figure><img src="/blog/assets/011_construct_nsw-nn5OimY4.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li><p>m=2: The number of neighbors, meaning how many nodes are connected.</p></li><li><p>Highway Concept:</p><ul><li>Initially, connections may be long because they are made one by one, without better or closer options. As connections progress, they become denser and shorter.</li><li>This approach has the advantage of resembling highways: the initially distant connections allow for traveling long distances without going through many small paths, thus improving search efficiency.</li></ul></li><li><p>Construction Complexity: O(nÂ²)</p><ul><li>The i-th node is compared with the previous i-1 nodes.</li></ul></li></ul><h3 id="_11-2-search" tabindex="-1"><a class="header-anchor" href="#_11-2-search" aria-hidden="true">#</a> 11.2. Search</h3><ol><li>Start from a random entry node and move towards the Query along the nearest neighbors.</li><li>Keep track of the top k points in a priority queue.</li></ol><p><img src="/blog/assets/011_nsw_search-V_1xx2wN.png" alt="" loading="lazy"><img src="/blog/assets/011_nsw_search1-UeauSrOK.png" alt="" loading="lazy"><img src="/blog/assets/011_nsw_search2-S8sO0hvE.png" alt="" loading="lazy"></p><ul><li>Query Complexity: O(n log n)</li><li>Problem: Can&#39;t find the next step? Early Stopping <ul><li>Early stopping occurs when the two neighbors of the current node are farther from the query. The algorithm may return the current node as the response, even though closer nodes exist.</li><li>Improvement: Using multiple entry points can enhance search accuracy.</li></ul></li></ul><figure><img src="/blog/assets/011_nsw_problem-pd_Kyspn.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_12-hnsw-hierarchical-navigable-small-world" tabindex="-1"><a class="header-anchor" href="#_12-hnsw-hierarchical-navigable-small-world" aria-hidden="true">#</a> 12. HNSW (Hierarchical Navigable Small World)</h2><ul><li>HNSW Algorithm: An optimization of NSW based on the Skip List concept. <ul><li>HNSW explicitly distinguishes between highways in the highway concept, imagining layers for different modes of transport: airplane, train, car, bicycle, and walking.</li></ul></li><li>It has driven the development of some of the most powerful vector databases, which utilize this structure.</li></ul><figure><img src="/blog/assets/011_skip_list-QMW-0yas.png" alt="Skip List" tabindex="0" loading="lazy"><figcaption>Skip List</figcaption></figure><h3 id="_12-1-building-hierarchical-structure" tabindex="-1"><a class="header-anchor" href="#_12-1-building-hierarchical-structure" aria-hidden="true">#</a> 12.1. Building - Hierarchical Structure</h3><ul><li>The structure is divided into layers, with the number of nodes increasing from top to bottom, with the bottom layer containing all nodes.</li><li>How to allocate nodes into layers? <ul><li>Each node is assigned a random number as its maximum layer (Max Layer). A node with a Max Layer of zero exists only at the bottom layer. If the random number is 2, the node exists at layers 0, 1, and 2, and so forth.</li></ul></li><li>Construction Complexity: O(n log n)</li></ul><p><img src="/blog/assets/011_hnsw_construction-am8y4kJZ.png" alt="" loading="lazy"><img src="/blog/assets/011_hnsw_runtime_complexity-VN12A_il.png" alt="" loading="lazy"></p><h3 id="_12-2-querying" tabindex="-1"><a class="header-anchor" href="#_12-2-querying" aria-hidden="true">#</a> 12.2. Querying</h3><ul><li>Query Process: <ol><li>Start from a random entry node at the top layer, moving towards the nearest neighbors.</li><li>Move to the nearest node in that layer.</li><li>Enter the next layer and continue to the nearest neighbor until reaching the bottom layer.</li></ol></li></ul><p><img src="/blog/assets/011_hnsw_search-leeOC4_n.png" alt="" loading="lazy"><img src="/blog/assets/011_hnsw_search1-HfKrWyHV.png" alt="" loading="lazy"></p><ul><li>Query Complexity: O(log n) <ul><li>Query time grows logarithmically, meaning that it doesn&#39;t significantly impact speed as data volume increases.</li></ul></li></ul><figure><img src="/blog/assets/011_hnsw_runtime_complexity-VN12A_il.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_12-3-further-optimization-hnsw-approximate-knn" tabindex="-1"><a class="header-anchor" href="#_12-3-further-optimization-hnsw-approximate-knn" aria-hidden="true">#</a> 12.3. Further Optimization: HNSW + Approximate KNN</h3><figure><img src="/blog/assets/011_hnsw_and_approximate_knn-ZvlrX75r.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_13-product-quantization-pq" tabindex="-1"><a class="header-anchor" href="#_13-product-quantization-pq" aria-hidden="true">#</a> 13. Product Quantization (PQ)</h2><p>Enhances search efficiency while compressing data and reducing memory usage.</p><p>Core:</p><ul><li>Fast <ul><li>Time Complexity: O(number of vectors * (subspace/dimension))</li></ul></li><li>Compression <ul><li>Data Compression: For instance, if each vector in the database is 1024-dimensional and split into four 256-dimensional subvectors, the storage requirement is greatly reduced by converting from 1024 floats to 4 ints.</li></ul></li></ul><p>Product Quantization aims to reduce memory usage and improve query speed (due to reduced computational load). PQ is a lossy compression method, which can lead to decreased retrieval accuracy, but this is acceptable in the context of ANN requirements.</p><h3 id="_13-1-construction" tabindex="-1"><a class="header-anchor" href="#_13-1-construction" aria-hidden="true">#</a> 13.1. Construction</h3><p><img src="/blog/assets/011_pq_construction-6bHXm9o2.png" alt="" loading="lazy"><img src="/blog/assets/011_pq_construction1-Y8_IAPse.png" alt="" loading="lazy"></p><p>Steps:</p><ol><li><strong>Subvectors</strong>: Split the original high-dimensional vector into n low-dimensional subvectors. <ul><li>For example, if each vector in the database is 1024-dimensional, split it into four 256-dimensional subvectors.</li></ul></li><li><strong>Codebook</strong>: Use k-means to compute Voronoi diagrams for each of the n subvectors, resulting in n distinct Voronoi diagrams (assuming each diagram has k centroids).</li><li><strong>Clustering</strong>: Place the n subvectors into their respective completed Voronoi diagrams to find the nearest centroid.</li><li><strong>Quantized Vectors</strong>: Treat the nearest centroids as new vectors, leading to quantized vectors.</li><li><strong>Reproduction Values</strong>: Use the indices of the nearest centroids in each of the n subvectors as new values, collectively referred to as the PQ code. <ul><li>Each subvector gets the index of its nearest centroid, resulting in four new values per vector in the database.</li></ul></li></ol><figure><img src="/blog/assets/011_pq_code-wA_FK7qQ.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Based on the n subvectors and the k centroids in each subspace, we obtain an n*k centroid matrix. The index of each subvector&#39;s nearest centroid is the PQ code.</p><h3 id="_13-2-querying" tabindex="-1"><a class="header-anchor" href="#_13-2-querying" aria-hidden="true">#</a> 13.2. Querying</h3><p>To find the centroids using the PQ code, look for KNN in the subspace of the centroid, approximating the distance between the query vector and an existing vector ( y ) using the centroid.</p><p>Query Steps:</p><ol><li>Split the query vector into multiple subvectors.</li><li>Calculate the distance between the query subvectors and the centroid matrix (finding the distance from each query subvector to each centroid), resulting in a distance matrix.</li><li>For each vector in the database, use its PQ code to find the corresponding centroid, retrieve distances from the distance matrix, and calculate the approximate distance between the query vector and any vector in the database by summing the squared distances of each subvector and taking the square root.</li></ol><p><img src="/blog/assets/011_pq_distance_metrix-m-TEwSkQ.png" alt="" loading="lazy"><img src="/blog/assets/011_pq_distance-yKEMPV1C.png" alt="" loading="lazy"></p><h3 id="_13-3-further-optimization" tabindex="-1"><a class="header-anchor" href="#_13-3-further-optimization" aria-hidden="true">#</a> 13.3. Further Optimization</h3><p>Combining PQ with Approximate KNN allows for not needing to compute distances with every vector in the database.</p><h2 id="_14-references" tabindex="-1"><a class="header-anchor" href="#_14-references" aria-hidden="true">#</a> 14. References</h2><p>[Middleware for Large Models: Principles and Selection of Vector Databases] (https://hub.baai.ac.cn/view/29516)</p><p>https://www.modb.pro/db/1817186648364507136</p><p>https://www.xiaozhuai.com/similarity-search-part-4-hierarchical-navigable-small-world-hnsw.html</p></div><!--[--><!----><!--]--><footer class="page-meta"><!----><div class="meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a aria-label="Prompt Engineering" class="vp-link nav-link prev nav-link prev" href="/blog/posts/llm/012_prompt_engineering.html"><div class="hint"><span class="arrow start"></span>Prev</div><div class="link"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Prompt Engineering</div></a><a aria-label="RAG Workflow" class="vp-link nav-link next nav-link next" href="/blog/posts/llm/010_rag_workflow.html"><div class="hint">Next<span class="arrow end"></span></div><div class="link">RAG Workflow<span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span></div></a></nav><!----><!--[--><!----><!--]--><!--]--></main><!--]--><!----></div><!--]--><!--]--><!----><!--]--></div>
    <script type="module" src="/blog/assets/app-x5fIoSzf.js" defer></script>
  </body>
</html>
