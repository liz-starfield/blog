<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.0" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.13" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="alternate" hreflang="zh-cn" href="https://liz-starfield.github.io/blog/zh/posts/LLM/llama.html"><meta property="og:url" content="https://liz-starfield.github.io/blog/posts/LLM/llama.html"><meta property="og:site_name" content="Liz"><meta property="og:title" content="Llama Source Code Exploration"><meta property="og:description" content="Llama Source Code Exploration About Llama Overall Architecture Hyperparameters Tensor Dimensionality Transformation Number of Trainable Parameters Source Code"><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:locale:alternate" content="zh-CN"><meta property="og:updated_time" content="2024-06-03T01:33:06.000Z"><meta property="article:author" content="Liz"><meta property="article:tag" content="LLM"><meta property="article:published_time" content="2024-06-01T00:00:00.000Z"><meta property="article:modified_time" content="2024-06-03T01:33:06.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Llama Source Code Exploration","image":[""],"datePublished":"2024-06-01T00:00:00.000Z","dateModified":"2024-06-03T01:33:06.000Z","author":[{"@type":"Person","name":"Liz","url":"https://github.com/liz-starfield"}]}</script><link rel="icon" herf="/blogger.png"><link rel="icon" href="/blog/blogger.png"><title>Llama Source Code Exploration | Liz</title><meta name="description" content="Llama Source Code Exploration About Llama Overall Architecture Hyperparameters Tensor Dimensionality Transformation Number of Trainable Parameters Source Code">
    <link rel="preload" href="/blog/assets/style-7GABgOoK.css" as="style"><link rel="stylesheet" href="/blog/assets/style-7GABgOoK.css">
    <link rel="modulepreload" href="/blog/assets/app-7SIUZpS0.js"><link rel="modulepreload" href="/blog/assets/llama.html-grkfOiBk.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-x3n3nnut.js"><link rel="modulepreload" href="/blog/assets/llama.html-k3uDI0u6.js">
    <link rel="prefetch" href="/blog/assets/index.html-LFD91XO_.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-E6FIGQFr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-8pZ41A_w.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PxUfOlrM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-s_datAw3.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-3kAZxSDz.js" as="script"><link rel="prefetch" href="/blog/assets/careers.html-2YAb7m8X.js" as="script"><link rel="prefetch" href="/blog/assets/communication.html-T68lRBW_.js" as="script"><link rel="prefetch" href="/blog/assets/computers.html-ve4rOIVZ.js" as="script"><link rel="prefetch" href="/blog/assets/describing_something.html-8VjhUkmx.js" as="script"><link rel="prefetch" href="/blog/assets/dreams.html-lfx3_joE.js" as="script"><link rel="prefetch" href="/blog/assets/feelings.html-JZzQmHxo.js" as="script"><link rel="prefetch" href="/blog/assets/graduating.html-mdxyICER.js" as="script"><link rel="prefetch" href="/blog/assets/grammar.html-8Q9YJc8K.js" as="script"><link rel="prefetch" href="/blog/assets/greetings.html-FPO73DtD.js" as="script"><link rel="prefetch" href="/blog/assets/hobbies.html-IwAofZph.js" as="script"><link rel="prefetch" href="/blog/assets/immigration.html-ph07V6IZ.js" as="script"><link rel="prefetch" href="/blog/assets/introducing_someone.html--zYgm4q2.js" as="script"><link rel="prefetch" href="/blog/assets/phone.html-PBoULsJ3.js" as="script"><link rel="prefetch" href="/blog/assets/pronunciation.html-g3TyM_sS.js" as="script"><link rel="prefetch" href="/blog/assets/time.html-6vml1xp0.js" as="script"><link rel="prefetch" href="/blog/assets/traits.html-C2L_-Hiv.js" as="script"><link rel="prefetch" href="/blog/assets/langchain.html-zKUtrlMr.js" as="script"><link rel="prefetch" href="/blog/assets/langchain_source_code.html-hLzF0c4R.js" as="script"><link rel="prefetch" href="/blog/assets/llm_summary.html-cvktnuJX.js" as="script"><link rel="prefetch" href="/blog/assets/streamlit.html-EoCnwOlO.js" as="script"><link rel="prefetch" href="/blog/assets/transformer.html-tE9tDwxy.js" as="script"><link rel="prefetch" href="/blog/assets/grammar.html-YIfhia-3.js" as="script"><link rel="prefetch" href="/blog/assets/pronunciation.html-PXzChWPT.js" as="script"><link rel="prefetch" href="/blog/assets/sentence_pattern_and_expression.html-4kBe9GtQ.js" as="script"><link rel="prefetch" href="/blog/assets/01_python_environment.html-5fJs1XZK.js" as="script"><link rel="prefetch" href="/blog/assets/02_python_data_type.html-HK52Z0IM.js" as="script"><link rel="prefetch" href="/blog/assets/03_python_operator.html-9_6xioiS.js" as="script"><link rel="prefetch" href="/blog/assets/04_python_method.html-IrOHK25K.js" as="script"><link rel="prefetch" href="/blog/assets/05_python_builtin_module.html-XlhXg8tm.js" as="script"><link rel="prefetch" href="/blog/assets/06_python_popular_package.html-6--ZbeB9.js" as="script"><link rel="prefetch" href="/blog/assets/01_ai_concept.html-1dsmjxhQ.js" as="script"><link rel="prefetch" href="/blog/assets/02_neural_net_train.html-MX0ksQLN.js" as="script"><link rel="prefetch" href="/blog/assets/03_pytorch_operation.html-eCRdWA85.js" as="script"><link rel="prefetch" href="/blog/assets/04_pytorch_practice_nn.html-k-oRm6yz.js" as="script"><link rel="prefetch" href="/blog/assets/05_linear_nn.html-m_m1sUSe.js" as="script"><link rel="prefetch" href="/blog/assets/06_heterogeneous_graph.html-QSneYC4l.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-f_nlM_7J.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZYhhb8JE.js" as="script"><link rel="prefetch" href="/blog/assets/communication.html-zvcTQC5g.js" as="script"><link rel="prefetch" href="/blog/assets/routine.html-6eX-V_SR.js" as="script"><link rel="prefetch" href="/blog/assets/careers.html-bdlOwrsx.js" as="script"><link rel="prefetch" href="/blog/assets/common.html-OW6TsiGL.js" as="script"><link rel="prefetch" href="/blog/assets/communication.html-ngNfyKIh.js" as="script"><link rel="prefetch" href="/blog/assets/computers.html-7bmxLQJx.js" as="script"><link rel="prefetch" href="/blog/assets/describing_something.html-errot6uM.js" as="script"><link rel="prefetch" href="/blog/assets/dreams.html-lXPy5R8V.js" as="script"><link rel="prefetch" href="/blog/assets/feelings.html-1GCf4uqR.js" as="script"><link rel="prefetch" href="/blog/assets/graduating.html-alxCt55O.js" as="script"><link rel="prefetch" href="/blog/assets/greetings.html-VykKbfRC.js" as="script"><link rel="prefetch" href="/blog/assets/hobbies.html-EdUSvvU4.js" as="script"><link rel="prefetch" href="/blog/assets/immigration.html-41Xqau-e.js" as="script"><link rel="prefetch" href="/blog/assets/introducing_someone.html-1Fvj-iNy.js" as="script"><link rel="prefetch" href="/blog/assets/phone.html-v2zRlMYN.js" as="script"><link rel="prefetch" href="/blog/assets/routine.html-3G78W-YZ.js" as="script"><link rel="prefetch" href="/blog/assets/time_and_weather.html-SSViHS-g.js" as="script"><link rel="prefetch" href="/blog/assets/traits.html-NuGv_PjT.js" as="script"><link rel="prefetch" href="/blog/assets/langchain.html-SCmjtC1i.js" as="script"><link rel="prefetch" href="/blog/assets/langchain_source_code.html-z5i49H-V.js" as="script"><link rel="prefetch" href="/blog/assets/llama.html-46p_1T-q.js" as="script"><link rel="prefetch" href="/blog/assets/llm_summary.html-3pu0Iumg.js" as="script"><link rel="prefetch" href="/blog/assets/streamlit.html-iUfqazN1.js" as="script"><link rel="prefetch" href="/blog/assets/transformer.html-YfxXjngT.js" as="script"><link rel="prefetch" href="/blog/assets/01_python_environment.html-6W1uTEtk.js" as="script"><link rel="prefetch" href="/blog/assets/02_python_data_type.html-m1bRXT1E.js" as="script"><link rel="prefetch" href="/blog/assets/03_python_operator.html-ZpTatogN.js" as="script"><link rel="prefetch" href="/blog/assets/04_python_method.html-RoP1I2kq.js" as="script"><link rel="prefetch" href="/blog/assets/05_python_builtin_module.html-H8OsYbTz.js" as="script"><link rel="prefetch" href="/blog/assets/06_python_popular_package.html-D_hwoRQE.js" as="script"><link rel="prefetch" href="/blog/assets/01_ai_concept.html-sQ3f7eMA.js" as="script"><link rel="prefetch" href="/blog/assets/02_neural_net_train.html-ANQdT1B0.js" as="script"><link rel="prefetch" href="/blog/assets/03_pytorch_operation.html-YMujwhKY.js" as="script"><link rel="prefetch" href="/blog/assets/04_pytorch_practice_nn.html-QQ109Ack.js" as="script"><link rel="prefetch" href="/blog/assets/05_linear_nn.html-vHRH7ymp.js" as="script"><link rel="prefetch" href="/blog/assets/06_heterogeneous_graph.html-50Tvq9Km.js" as="script"><link rel="prefetch" href="/blog/assets/three.html-TDsxrnSO.js" as="script"><link rel="prefetch" href="/blog/assets/four.html-Ek6QyJ3q.js" as="script"><link rel="prefetch" href="/blog/assets/five.html-0W-riO3q.js" as="script"><link rel="prefetch" href="/blog/assets/six.html-7wgL3H0A.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-_gRo7Dcd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cKTGnjrW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UxATL_0T.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--Gb44XzH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Mv3ej2IF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hZU6eHY7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-gzyrJME8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7evZAwcB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kJ320kxl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PCaJfeIo.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-EqHBUaBn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-KdZMLo8O.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0JwxQglC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C3QK3cD9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jDGG44gj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-VOr9nBXB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-lMZf9nw6.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-8E67PwBh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-qostYW_M.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-T-uPJpYq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-fwy4Lfih.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-a5YkJeQn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bys25b8k.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-NSZidhQD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-sh1juioO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-VpAiyB26.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Qi9078Dh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZDxccGry.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-T-9GN81B.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Kzol0OIK.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9t8BxIqc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4rWeg-rw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-c5Din3Gd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SVlXYkho.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-u6ZwE8BE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-gikpHa9b.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CsjM6MnY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-yX1j37_q.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-gUmwP0GA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-fdx6_zlO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7pNQOk23.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-8dBZSIkT.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Ohri_95T.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-uJ4nU7QG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-MRcbvmhe.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-p-WoYfCM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hTrRFZkT.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-67j3W8G7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-m34kEvDY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-m0dv9t6y.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-zYmQGvIp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DVVGE8Pg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-dxLqYysi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--IEIJJE7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-YVQ3Gxv8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-qnPGVL8b.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4mmliAL3.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-7xzG-CpY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-M_Xeeld-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-K9b8abIt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0jBNnZ0_.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-bL1AjBMd.js" as="script"><link rel="prefetch" href="/blog/assets/careers.html-P9oZbaTu.js" as="script"><link rel="prefetch" href="/blog/assets/communication.html-H2XBfv8m.js" as="script"><link rel="prefetch" href="/blog/assets/computers.html-AhzTV_PI.js" as="script"><link rel="prefetch" href="/blog/assets/describing_something.html-pLi64r3e.js" as="script"><link rel="prefetch" href="/blog/assets/dreams.html-wlfjypzn.js" as="script"><link rel="prefetch" href="/blog/assets/feelings.html-dWd22BGC.js" as="script"><link rel="prefetch" href="/blog/assets/graduating.html-Ljh7fjgj.js" as="script"><link rel="prefetch" href="/blog/assets/grammar.html-pTPisxtA.js" as="script"><link rel="prefetch" href="/blog/assets/greetings.html-jvZAiKVs.js" as="script"><link rel="prefetch" href="/blog/assets/hobbies.html-LXokepyp.js" as="script"><link rel="prefetch" href="/blog/assets/immigration.html-90kgazmh.js" as="script"><link rel="prefetch" href="/blog/assets/introducing_someone.html-vhYd4Ff7.js" as="script"><link rel="prefetch" href="/blog/assets/phone.html-UsJMSloY.js" as="script"><link rel="prefetch" href="/blog/assets/pronunciation.html-NEfYyU5o.js" as="script"><link rel="prefetch" href="/blog/assets/time.html-OZQ00sIG.js" as="script"><link rel="prefetch" href="/blog/assets/traits.html-YfjyiKU4.js" as="script"><link rel="prefetch" href="/blog/assets/langchain.html-sDkU3h8q.js" as="script"><link rel="prefetch" href="/blog/assets/langchain_source_code.html-EdjSTR2W.js" as="script"><link rel="prefetch" href="/blog/assets/llm_summary.html-_3bXmOYm.js" as="script"><link rel="prefetch" href="/blog/assets/streamlit.html-WA2jnfBb.js" as="script"><link rel="prefetch" href="/blog/assets/transformer.html-VmJZaliH.js" as="script"><link rel="prefetch" href="/blog/assets/grammar.html-wl4WGzv9.js" as="script"><link rel="prefetch" href="/blog/assets/pronunciation.html-ibbTxFwh.js" as="script"><link rel="prefetch" href="/blog/assets/sentence_pattern_and_expression.html-DR32y_4l.js" as="script"><link rel="prefetch" href="/blog/assets/01_python_environment.html-M0rq-vRr.js" as="script"><link rel="prefetch" href="/blog/assets/02_python_data_type.html-KbRU8t7P.js" as="script"><link rel="prefetch" href="/blog/assets/03_python_operator.html-THLajKzG.js" as="script"><link rel="prefetch" href="/blog/assets/04_python_method.html-CWFd8m8h.js" as="script"><link rel="prefetch" href="/blog/assets/05_python_builtin_module.html-Hx_pjs6O.js" as="script"><link rel="prefetch" href="/blog/assets/06_python_popular_package.html-JCK2JgbO.js" as="script"><link rel="prefetch" href="/blog/assets/01_ai_concept.html-M7ree2ek.js" as="script"><link rel="prefetch" href="/blog/assets/02_neural_net_train.html-T4t1v2W7.js" as="script"><link rel="prefetch" href="/blog/assets/03_pytorch_operation.html-8teIfJAo.js" as="script"><link rel="prefetch" href="/blog/assets/04_pytorch_practice_nn.html-D88BQXxB.js" as="script"><link rel="prefetch" href="/blog/assets/05_linear_nn.html-L3cMEONv.js" as="script"><link rel="prefetch" href="/blog/assets/06_heterogeneous_graph.html-kUjXUiNx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SgtdLhQk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GDrnO2wl.js" as="script"><link rel="prefetch" href="/blog/assets/communication.html-rDjtrycp.js" as="script"><link rel="prefetch" href="/blog/assets/routine.html-Pg1XOlMv.js" as="script"><link rel="prefetch" href="/blog/assets/careers.html-TcVAVqMU.js" as="script"><link rel="prefetch" href="/blog/assets/common.html-QOHB4sob.js" as="script"><link rel="prefetch" href="/blog/assets/communication.html-UiF1B0Da.js" as="script"><link rel="prefetch" href="/blog/assets/computers.html-KUwjUNbw.js" as="script"><link rel="prefetch" href="/blog/assets/describing_something.html-N4DK5dzL.js" as="script"><link rel="prefetch" href="/blog/assets/dreams.html-hFjPspNt.js" as="script"><link rel="prefetch" href="/blog/assets/feelings.html-rLZdE5Sp.js" as="script"><link rel="prefetch" href="/blog/assets/graduating.html-fhuAy7Lj.js" as="script"><link rel="prefetch" href="/blog/assets/greetings.html-h1YDkhIM.js" as="script"><link rel="prefetch" href="/blog/assets/hobbies.html-1ja2bf_s.js" as="script"><link rel="prefetch" href="/blog/assets/immigration.html-dmyuNGH3.js" as="script"><link rel="prefetch" href="/blog/assets/introducing_someone.html-bGHqfRTN.js" as="script"><link rel="prefetch" href="/blog/assets/phone.html-nekCaYf2.js" as="script"><link rel="prefetch" href="/blog/assets/routine.html-q3Mtz9fr.js" as="script"><link rel="prefetch" href="/blog/assets/time_and_weather.html-xPd1lZzw.js" as="script"><link rel="prefetch" href="/blog/assets/traits.html-Hjcetc64.js" as="script"><link rel="prefetch" href="/blog/assets/langchain.html-wBasf2lm.js" as="script"><link rel="prefetch" href="/blog/assets/langchain_source_code.html-jOe6JA_X.js" as="script"><link rel="prefetch" href="/blog/assets/llama.html-Ypt14d0V.js" as="script"><link rel="prefetch" href="/blog/assets/llm_summary.html-RXJ3-JZB.js" as="script"><link rel="prefetch" href="/blog/assets/streamlit.html-BOBFwd46.js" as="script"><link rel="prefetch" href="/blog/assets/transformer.html-7jTWbYuO.js" as="script"><link rel="prefetch" href="/blog/assets/01_python_environment.html-oZx35jtd.js" as="script"><link rel="prefetch" href="/blog/assets/02_python_data_type.html-Vhcdk4wW.js" as="script"><link rel="prefetch" href="/blog/assets/03_python_operator.html-GA6Wwc-u.js" as="script"><link rel="prefetch" href="/blog/assets/04_python_method.html-t65o14Rl.js" as="script"><link rel="prefetch" href="/blog/assets/05_python_builtin_module.html-K15d9OEV.js" as="script"><link rel="prefetch" href="/blog/assets/06_python_popular_package.html-Df-l0X73.js" as="script"><link rel="prefetch" href="/blog/assets/01_ai_concept.html-hMplGpcz.js" as="script"><link rel="prefetch" href="/blog/assets/02_neural_net_train.html-BFTZk6bG.js" as="script"><link rel="prefetch" href="/blog/assets/03_pytorch_operation.html-heuV-mwO.js" as="script"><link rel="prefetch" href="/blog/assets/04_pytorch_practice_nn.html-IFAng4y8.js" as="script"><link rel="prefetch" href="/blog/assets/05_linear_nn.html-sY_i5fgu.js" as="script"><link rel="prefetch" href="/blog/assets/06_heterogeneous_graph.html-m801Pj27.js" as="script"><link rel="prefetch" href="/blog/assets/three.html-i6faadNH.js" as="script"><link rel="prefetch" href="/blog/assets/four.html-fw6nb1iH.js" as="script"><link rel="prefetch" href="/blog/assets/five.html-U7O7QXNh.js" as="script"><link rel="prefetch" href="/blog/assets/six.html-rs9ozh4t.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-EQ73-bxC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Umycq-w3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-c9LVFjWm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-tXIScy0e.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-asuDrRcy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wb_4-AVG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-pKlS7ENS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-c4nM6RtS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-sAHAk4Zy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cI9rPIw0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Z3_uUIxU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-IcVaWS4O.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-QJHmgfTd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-RrI_gKRY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-03qbxWSp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kDyJos-_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kvJn1ZYR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hyMh30pN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-eA0qnYFc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-RwXNpkSG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g3QJ8Ysg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PgXsHn0A.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SS2WPXZ8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-1ssjMB4t.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-18St-iE6.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-E5aKlhaX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-fMsr82FQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mi0Xo5Y3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_Jf5Jq8p.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-E4CQeDIV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bQMEj32h.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-gSxNr6gW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UwtAB__Q.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-KxgcLiXr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-imnrHg1H.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-btanmzdG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-urmsuGnJ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-LP42k3Iv.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_ZYYE7Rs.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-uDJh43W-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-LxgSOHxz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-aw8d2TSw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-YXggt5AF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-WYPDDXl4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-78GIzKRF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-MN_TVFUY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-lQfp_AuO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-LiWh7b_B.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-dZyV5_Rs.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-1xlGB1fy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-lU4Si_dy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hXTZqjT7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-EtlUwZD-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DR5qtqvd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-8SLKn3LK.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Sp2kZfC9.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-08_zHRDQ.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><!--[--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/blog/"><img class="vp-nav-logo" src="/blog/blogger.png" alt><!----><span class="vp-site-name hide-in-pad">Liz</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="Home" class="vp-link nav-link nav-link" href="/blog/"><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span>Home<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Blog" class="vp-link nav-link active nav-link active" href="/blog/posts/"><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span>Blog<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Project" class="vp-link nav-link nav-link" href="/blog/demo/"><span class="font-icon icon fa-fw fa-sm fas fa-star" style=""></span>Project<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><div class="nav-item"><div class="dropdown-wrapper i18n-dropdown"><button type="button" class="dropdown-title" aria-label="Select language"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="i18n icon" style="width:1rem;height:1rem;vertical-align:middle;"><path d="M379.392 460.8 494.08 575.488l-42.496 102.4L307.2 532.48 138.24 701.44l-71.68-72.704L234.496 460.8l-45.056-45.056c-27.136-27.136-51.2-66.56-66.56-108.544h112.64c7.68 14.336 16.896 27.136 26.112 35.84l45.568 46.08 45.056-45.056C382.976 312.32 409.6 247.808 409.6 204.8H0V102.4h256V0h102.4v102.4h256v102.4H512c0 70.144-37.888 161.28-87.04 210.944L378.88 460.8zM576 870.4 512 1024H409.6l256-614.4H768l256 614.4H921.6l-64-153.6H576zM618.496 768h196.608L716.8 532.48 618.496 768z"></path></svg><!--]--><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="English" class="vp-link nav-link active nav-link active" href="/blog/posts/LLM/llama.html"><!---->English<!----></a></li><li class="dropdown-item"><a aria-label="ç®€ä½“ä¸­æ–‡" class="vp-link nav-link nav-link" href="/blog/zh/posts/LLM/llama.html"><!---->ç®€ä½“ä¸­æ–‡<!----></a></li></ul></button></div></div><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/liz-starfield" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><!--[--><a aria-label="Home" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/blog/"><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span>Home<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="About Me" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/blog/intro.html"><span class="font-icon icon fa-fw fa-sm fas fa-user" style=""></span>About Me<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading active"><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><span class="vp-sidebar-title">Blog</span><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">Conversation</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">Language</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable active" type="button"><!----><span class="vp-sidebar-title">L L M</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><!--[--><a aria-label="Understanding LangChain in One Article: Building Powerful Applications with Large Language Models" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/blog/posts/LLM/langchain.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Understanding LangChain in One Article: Building Powerful Applications with Large Language Models<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="From the Source Code Perspective, Peering into the Operation Logic of LangChain" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/blog/posts/LLM/langchain_source_code.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>From the Source Code Perspective, Peering into the Operation Logic of LangChain<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Building Conversational Applications with Streamlit" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/blog/posts/LLM/streamlit.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Building Conversational Applications with Streamlit<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Transformer Source Code Exploration" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/blog/posts/LLM/transformer.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Transformer Source Code Exploration<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Llama Source Code Exploration" class="vp-link nav-link active vp-sidebar-link vp-sidebar-page active nav-link active vp-sidebar-link vp-sidebar-page active" href="/blog/posts/LLM/llama.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Llama Source Code Exploration<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="1. About" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/posts/LLM/llama.html#_1-about"><!---->1. About<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="2. Llama Overall Architecture" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/posts/LLM/llama.html#_2-llama-overall-architecture"><!---->2. Llama Overall Architecture<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="3. Hyperparameters" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/posts/LLM/llama.html#_3-hyperparameters"><!---->3. Hyperparameters<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="4. Tensor Dimensionality Transformation" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/posts/LLM/llama.html#_4-tensor-dimensionality-transformation"><!---->4. Tensor Dimensionality Transformation<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="5. Number of Trainable Parameters" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/posts/LLM/llama.html#_5-number-of-trainable-parameters"><!---->5. Number of Trainable Parameters<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6. Source Code" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/posts/LLM/llama.html#_6-source-code"><!---->6. Source Code<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="6.1. Entrance" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/posts/LLM/llama.html#_6-1-entrance"><!---->6.1. Entrance<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.2. GenerationMixin" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/posts/LLM/llama.html#_6-2-generationmixin"><!---->6.2. GenerationMixin<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.3. LlamaForCausalLM" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/posts/LLM/llama.html#_6-3-llamaforcausallm"><!---->6.3. LlamaForCausalLM<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.4. LlamaModel" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/posts/LLM/llama.html#_6-4-llamamodel"><!---->6.4. LlamaModel<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.5. LlamaDecoderLayer" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/posts/LLM/llama.html#_6-5-llamadecoderlayer"><!---->6.5. LlamaDecoderLayer<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.6. LlamaRMSNorm" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/posts/LLM/llama.html#_6-6-llamarmsnorm"><!---->6.6. LlamaRMSNorm<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.7. LlamaSdpaAttention" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/posts/LLM/llama.html#_6-7-llamasdpaattention"><!---->6.7. LlamaSdpaAttention<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.8. LlamaRotaryEmbedding" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/posts/LLM/llama.html#_6-8-llamarotaryembedding"><!---->6.8. LlamaRotaryEmbedding<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.9. LlamaMLP" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/posts/LLM/llama.html#_6-9-llamamlp"><!---->6.9. LlamaMLP<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li></ul><!--]--></li><li><!--[--><a aria-label="LLM Summary" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/blog/posts/LLM/llm_summary.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>LLM Summary<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">Python</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">Pytorch</span><span class="vp-arrow end"></span></button><!----></section></li></ul></section></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Llama Source Code Exploration</h1><div class="page-info"><span class="page-author-info" aria-label="AuthorðŸ–Š" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/liz-starfield" target="_blank" rel="noopener noreferrer">Liz</a></span><span property="author" content="Liz"></span></span><!----><span class="page-date-info" aria-label="Writing DateðŸ“…" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2024-06-01T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="Reading TimeâŒ›" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 5 min</span><meta property="timeRequired" content="PT5M"></span><span class="page-category-info" aria-label="CategoryðŸŒˆ" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category6 clickable" role="navigation">LLM</span><!--]--><meta property="articleSection" content="LLM"></span><span class="page-tag-info" aria-label="TagðŸ·" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag6 clickable" role="navigation">LLM</span><!--]--><meta property="keywords" content="LLM"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_1-about">1. About</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_2-llama-overall-architecture">2. Llama Overall Architecture</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_3-hyperparameters">3. Hyperparameters</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_4-tensor-dimensionality-transformation">4. Tensor Dimensionality Transformation</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_5-number-of-trainable-parameters">5. Number of Trainable Parameters</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_6-source-code">6. Source Code</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-1-entrance">6.1. Entrance</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-2-generationmixin">6.2. GenerationMixin</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-3-llamaforcausallm">6.3. LlamaForCausalLM</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-4-llamamodel">6.4. LlamaModel</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-5-llamadecoderlayer">6.5. LlamaDecoderLayer</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-6-llamarmsnorm">6.6. LlamaRMSNorm</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-7-llamasdpaattention">6.7. LlamaSdpaAttention</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-8-llamarotaryembedding">6.8. LlamaRotaryEmbedding</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-9-llamamlp">6.9. LlamaMLP</a></li><!----><!--]--></ul></li><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h1 id="llama-source-code-exploration" tabindex="-1"><a class="header-anchor" href="#llama-source-code-exploration" aria-hidden="true">#</a> Llama Source Code Exploration</h1><ul><li><ol><li>About</li></ol></li><li><ol start="2"><li>Llama Overall Architecture</li></ol></li><li><ol start="3"><li>Hyperparameters</li></ol></li><li><ol start="4"><li>Tensor Dimensionality Transformation</li></ol></li><li><ol start="5"><li>Number of Trainable Parameters</li></ol></li><li><ol start="6"><li>Source Code</li></ol></li></ul><!-- more --><h2 id="_1-about" tabindex="-1"><a class="header-anchor" href="#_1-about" aria-hidden="true">#</a> 1. About</h2><p>Sourceï¼šhttps://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct</p><p>Dateï¼š2024.04.18</p><p>Companyï¼šMeta</p><p>Source Codeï¼š</p><p>https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py</p><h2 id="_2-llama-overall-architecture" tabindex="-1"><a class="header-anchor" href="#_2-llama-overall-architecture" aria-hidden="true">#</a> 2. Llama Overall Architecture</h2><figure><img src="/blog/assets/Llama_Overall_Architecture-1gLi0eGW.png" alt="Llama Overall Architecture" tabindex="0" loading="lazy"><figcaption>Llama Overall Architecture</figcaption></figure><figure><img src="/blog/assets/Llama_Source_Code_Architecture-ungM-IWo.png" alt="Source Code Corresponding to Model Architecture" tabindex="0" loading="lazy"><figcaption>Source Code Corresponding to Model Architecture</figcaption></figure><figure><img src="/blog/assets/llama_vs_transformer-o2oE5na1.png" alt="Llama vs Transformer" tabindex="0" loading="lazy"><figcaption>Llama vs Transformer</figcaption></figure><h2 id="_3-hyperparameters" tabindex="-1"><a class="header-anchor" href="#_3-hyperparameters" aria-hidden="true">#</a> 3. Hyperparameters</h2><figure><img src="/blog/assets/llama_hyperparameters-jzdAS33W.png" alt="Hyperparameters" tabindex="0" loading="lazy"><figcaption>Hyperparameters</figcaption></figure><h2 id="_4-tensor-dimensionality-transformation" tabindex="-1"><a class="header-anchor" href="#_4-tensor-dimensionality-transformation" aria-hidden="true">#</a> 4. Tensor Dimensionality Transformation</h2><figure><img src="/blog/assets/llama_dim_trans-WzKa4GNP.png" alt="Tensor Dimension Transformation" tabindex="0" loading="lazy"><figcaption>Tensor Dimension Transformation</figcaption></figure><figure><img src="/blog/assets/LlamaForCausalLM-Cibak_R7.png" alt="Tensor Dimension Transformation Details" tabindex="0" loading="lazy"><figcaption>Tensor Dimension Transformation Details</figcaption></figure><h2 id="_5-number-of-trainable-parameters" tabindex="-1"><a class="header-anchor" href="#_5-number-of-trainable-parameters" aria-hidden="true">#</a> 5. Number of Trainable Parameters</h2><figure><img src="/blog/assets/llama_trainable_parameters-WRl7ZcvJ.png" alt="Number of Trainable Parameters" tabindex="0" loading="lazy"><figcaption>Number of Trainable Parameters</figcaption></figure><h2 id="_6-source-code" tabindex="-1"><a class="header-anchor" href="#_6-source-code" aria-hidden="true">#</a> 6. Source Code</h2><h3 id="_6-1-entrance" tabindex="-1"><a class="header-anchor" href="#_6-1-entrance" aria-hidden="true">#</a> 6.1. Entrance</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># Download the vocabulary file tokenizer.json from the model_id path and instantiate the tokenizer class</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&quot;&quot;&quot;
The main steps are divided into two parts:
1.Download the configuration file config.json from the model_id path and instantiate the LlamaConfig class
2.Download the model-related information from the model_id path and instantiate the LlamaForCausalLM class
&quot;&quot;&quot;</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_id<span class="token punctuation">,</span>
    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16<span class="token punctuation">,</span>
    device_map<span class="token operator">=</span><span class="token string">&quot;auto&quot;</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># prompt</span>
messages <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{</span><span class="token string">&quot;role&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;system&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;content&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;You are a pirate chatbot who always responds in pirate speak!&quot;</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span><span class="token string">&quot;role&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;user&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;content&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;Who are you?&quot;</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>

<span class="token comment"># Convert messages into tokens</span>
input_ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>apply_chat_template<span class="token punctuation">(</span>
    messages<span class="token punctuation">,</span>
    add_generation_prompt<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    return_tensors<span class="token operator">=</span><span class="token string">&quot;pt&quot;</span>
<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>model<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

terminators <span class="token operator">=</span> <span class="token punctuation">[</span>
    tokenizer<span class="token punctuation">.</span>eos_token_id<span class="token punctuation">,</span>
    tokenizer<span class="token punctuation">.</span>convert_tokens_to_ids<span class="token punctuation">(</span><span class="token string">&quot;&lt;|eot_id|&gt;&quot;</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span>

<span class="token comment"># GenerationMixin&#39;s generate</span>
<span class="token comment"># Generation Strategyï¼š*multinomial sampling* if `num_beams=1` and `do_sample=True`</span>
outputs <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>
    input_ids<span class="token punctuation">,</span>
    max_new_tokens<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>
    eos_token_id<span class="token operator">=</span>terminators<span class="token punctuation">,</span>
    do_sample<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    temperature<span class="token operator">=</span><span class="token number">0.6</span><span class="token punctuation">,</span> <span class="token comment"># The value used to modulate the next token probabilities.defaults to 1.0</span>
    top_p<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token comment"># defaults to 1.0</span>
<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">)</span>
response <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>input_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment"># Retrieve the part of the outputs that excludes the original output of input_ids (prompt)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>response<span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># Convert tokens back into characters, ignoring special tokens</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-2-generationmixin" tabindex="-1"><a class="header-anchor" href="#_6-2-generationmixin" aria-hidden="true">#</a> 6.2. GenerationMixin</h3><figure><img src="/blog/assets/Inheritance-MMzuMZev.png" alt="Inherit Relation of LlamaForCausalLM and GenerationMixin" tabindex="0" loading="lazy"><figcaption>Inherit Relation of LlamaForCausalLM and GenerationMixin</figcaption></figure><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">GenerationMixin</span><span class="token punctuation">:</span>

    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">generate</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        inputs<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        generation_config<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>GenerationConfig<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        logits_processor<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>LogitsProcessorList<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        stopping_criteria<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>StoppingCriteriaList<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        prefix_allowed_tokens_fn<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Callable<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        synced_gpus<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        assistant_model<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token string">&quot;PreTrainedModel&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        streamer<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token string">&quot;BaseStreamer&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        negative_prompt_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        negative_prompt_attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Union<span class="token punctuation">[</span>GenerateOutput<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span><span class="token punctuation">:</span>

        <span class="token comment"># 13. run sample</span>
        result <span class="token operator">=</span> self<span class="token punctuation">.</span>_sample<span class="token punctuation">(</span>
            input_ids<span class="token punctuation">,</span>
            logits_processor<span class="token operator">=</span>prepared_logits_processor<span class="token punctuation">,</span>
            logits_warper<span class="token operator">=</span>prepared_logits_warper<span class="token punctuation">,</span>
            stopping_criteria<span class="token operator">=</span>prepared_stopping_criteria<span class="token punctuation">,</span>
            generation_config<span class="token operator">=</span>generation_config<span class="token punctuation">,</span>
            synced_gpus<span class="token operator">=</span>synced_gpus<span class="token punctuation">,</span>
            streamer<span class="token operator">=</span>streamer<span class="token punctuation">,</span>
            <span class="token operator">**</span>model_kwargs<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_sample</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        input_ids<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">,</span>
        logits_processor<span class="token punctuation">:</span> LogitsProcessorList<span class="token punctuation">,</span>
        stopping_criteria<span class="token punctuation">:</span> StoppingCriteriaList<span class="token punctuation">,</span>
        generation_config<span class="token punctuation">:</span> GenerationConfig<span class="token punctuation">,</span>
        synced_gpus<span class="token punctuation">:</span> <span class="token builtin">bool</span><span class="token punctuation">,</span>
        streamer<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token string">&quot;BaseStreamer&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        logits_warper<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>LogitsProcessorList<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        <span class="token operator">**</span>model_kwargs<span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Union<span class="token punctuation">[</span>GenerateNonBeamOutput<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span><span class="token punctuation">:</span>

        <span class="token keyword">while</span> self<span class="token punctuation">.</span>_has_unfinished_sequences<span class="token punctuation">(</span>this_peer_finished<span class="token punctuation">,</span> synced_gpus<span class="token punctuation">,</span> device<span class="token operator">=</span>input_ids<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">:</span>            

            <span class="token comment"># forward pass to get next token</span>
            outputs <span class="token operator">=</span> self<span class="token punctuation">(</span>
                <span class="token operator">**</span>model_inputs<span class="token punctuation">,</span>
                return_dict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span>
                output_hidden_states<span class="token operator">=</span>output_hidden_states<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

            next_token_logits <span class="token operator">=</span> outputs<span class="token punctuation">.</span>logits<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>

            next_token_scores <span class="token operator">=</span> logits_warper<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> next_token_scores<span class="token punctuation">)</span>

            <span class="token comment"># token selection        </span>
            probs <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>next_token_scores<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            next_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>multinomial<span class="token punctuation">(</span>probs<span class="token punctuation">,</span> num_samples<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>

            input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>input_ids<span class="token punctuation">,</span> next_tokens<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> GenerateDecoderOnlyOutput<span class="token punctuation">(</span>
                    sequences<span class="token operator">=</span>input_ids<span class="token punctuation">,</span>
                    scores<span class="token operator">=</span>scores<span class="token punctuation">,</span>
                    logits<span class="token operator">=</span>raw_logits<span class="token punctuation">,</span>
                    attentions<span class="token operator">=</span>decoder_attentions<span class="token punctuation">,</span>
                    hidden_states<span class="token operator">=</span>decoder_hidden_states<span class="token punctuation">,</span>
                    past_key_values<span class="token operator">=</span>model_kwargs<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">&quot;past_key_values&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-3-llamaforcausallm" tabindex="-1"><a class="header-anchor" href="#_6-3-llamaforcausallm" aria-hidden="true">#</a> 6.3. LlamaForCausalLM</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LlamaForCausalLM</span><span class="token punctuation">(</span>LlamaPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> LlamaModel<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> config<span class="token punctuation">.</span>vocab_size
        self<span class="token punctuation">.</span>lm_head <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

        <span class="token comment"># Initialize weights and apply final processing</span>
        self<span class="token punctuation">.</span>post_init<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        input_ids<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>LongTensor <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        past_key_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>Cache<span class="token punctuation">,</span> List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        inputs_embeds<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        labels<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        output_attentions<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        output_hidden_states<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        return_dict<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        cache_position<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Union<span class="token punctuation">[</span>Tuple<span class="token punctuation">,</span> CausalLMOutputWithPast<span class="token punctuation">]</span><span class="token punctuation">:</span>

        <span class="token comment"># decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)</span>
        outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>
            input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">,</span>
            attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
            position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
            inputs_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span>
            use_cache<span class="token operator">=</span>use_cache<span class="token punctuation">,</span>
            output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span>
            output_hidden_states<span class="token operator">=</span>output_hidden_states<span class="token punctuation">,</span>
            return_dict<span class="token operator">=</span>return_dict<span class="token punctuation">,</span>
            cache_position<span class="token operator">=</span>cache_position<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

        hidden_states <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

        logits <span class="token operator">=</span> self<span class="token punctuation">.</span>lm_head<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

        <span class="token keyword">return</span> CausalLMOutputWithPast<span class="token punctuation">(</span>
            loss<span class="token operator">=</span>loss<span class="token punctuation">,</span>
            logits<span class="token operator">=</span>logits<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>outputs<span class="token punctuation">.</span>past_key_values<span class="token punctuation">,</span>
            hidden_states<span class="token operator">=</span>outputs<span class="token punctuation">.</span>hidden_states<span class="token punctuation">,</span>
            attentions<span class="token operator">=</span>outputs<span class="token punctuation">.</span>attentions<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-4-llamamodel" tabindex="-1"><a class="header-anchor" href="#_6-4-llamamodel" aria-hidden="true">#</a> 6.4. LlamaModel</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LlamaModel</span><span class="token punctuation">(</span>LlamaPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> LlamaConfig<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>padding_idx <span class="token operator">=</span> config<span class="token punctuation">.</span>pad_token_id
        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> config<span class="token punctuation">.</span>vocab_size

        self<span class="token punctuation">.</span>embed_tokens <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>padding_idx<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>
            <span class="token punctuation">[</span>LlamaDecoderLayer<span class="token punctuation">(</span>config<span class="token punctuation">,</span> layer_idx<span class="token punctuation">)</span> <span class="token keyword">for</span> layer_idx <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>num_hidden_layers<span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> LlamaRMSNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>rms_norm_eps<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gradient_checkpointing <span class="token operator">=</span> <span class="token boolean">False</span>

        <span class="token comment"># Initialize weights and apply final processing</span>
        self<span class="token punctuation">.</span>post_init<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        input_ids<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>LongTensor <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        past_key_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>Cache<span class="token punctuation">,</span> List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        inputs_embeds<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        output_attentions<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        output_hidden_states<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        return_dict<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        cache_position<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Union<span class="token punctuation">[</span>Tuple<span class="token punctuation">,</span> BaseModelOutputWithPast<span class="token punctuation">]</span><span class="token punctuation">:</span>

        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>embed_tokens<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>

        <span class="token keyword">for</span> decoder_layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
            layer_outputs <span class="token operator">=</span> decoder_layer<span class="token punctuation">(</span>
                    hidden_states<span class="token punctuation">,</span>
                    attention_mask<span class="token operator">=</span>causal_mask<span class="token punctuation">,</span>
                    position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">,</span>
                    past_key_value<span class="token operator">=</span>past_key_values<span class="token punctuation">,</span>
                    output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span>
                    use_cache<span class="token operator">=</span>use_cache<span class="token punctuation">,</span>
                    cache_position<span class="token operator">=</span>cache_position<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
        
        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>layer_outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 

        <span class="token keyword">return</span> BaseModelOutputWithPast<span class="token punctuation">(</span>
            last_hidden_state<span class="token operator">=</span>hidden_states<span class="token punctuation">,</span>
            past_key_values<span class="token operator">=</span>next_cache<span class="token punctuation">,</span>
            hidden_states<span class="token operator">=</span>all_hidden_states<span class="token punctuation">,</span>
            attentions<span class="token operator">=</span>all_self_attns<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-5-llamadecoderlayer" tabindex="-1"><a class="header-anchor" href="#_6-5-llamadecoderlayer" aria-hidden="true">#</a> 6.5. LlamaDecoderLayer</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LlamaDecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> LlamaConfig<span class="token punctuation">,</span> layer_idx<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size

        self<span class="token punctuation">.</span>self_attn <span class="token operator">=</span> LLAMA_ATTENTION_CLASSES<span class="token punctuation">[</span>config<span class="token punctuation">.</span>_attn_implementation<span class="token punctuation">]</span><span class="token punctuation">(</span>config<span class="token operator">=</span>config<span class="token punctuation">,</span> layer_idx<span class="token operator">=</span>layer_idx<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> LlamaMLP<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>input_layernorm <span class="token operator">=</span> LlamaRMSNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>rms_norm_eps<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>post_attention_layernorm <span class="token operator">=</span> LlamaRMSNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>rms_norm_eps<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>
        attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        past_key_value<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Cache<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        output_attentions<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        cache_position<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>

        residual <span class="token operator">=</span> hidden_states

        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>input_layernorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

        <span class="token comment"># Self Attention</span>
        hidden_states<span class="token punctuation">,</span> self_attn_weights<span class="token punctuation">,</span> present_key_value <span class="token operator">=</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>
            hidden_states<span class="token operator">=</span>hidden_states<span class="token punctuation">,</span>
            attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
            position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">,</span>
            past_key_value<span class="token operator">=</span>past_key_value<span class="token punctuation">,</span>
            output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span>
            use_cache<span class="token operator">=</span>use_cache<span class="token punctuation">,</span>
            cache_position<span class="token operator">=</span>cache_position<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

        hidden_states <span class="token operator">=</span> residual <span class="token operator">+</span> hidden_states

        <span class="token comment"># Fully Connected</span>
        residual <span class="token operator">=</span> hidden_states
        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
        hidden_states <span class="token operator">=</span> residual <span class="token operator">+</span> hidden_states

        outputs <span class="token operator">=</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>
            outputs <span class="token operator">+=</span> <span class="token punctuation">(</span>present_key_value<span class="token punctuation">,</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> outputs
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-6-llamarmsnorm" tabindex="-1"><a class="header-anchor" href="#_6-6-llamarmsnorm" aria-hidden="true">#</a> 6.6. LlamaRMSNorm</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LlamaRMSNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        LlamaRMSNorm is equivalent to T5LayerNorm
        &quot;&quot;&quot;</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>variance_epsilon <span class="token operator">=</span> eps

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_states<span class="token punctuation">)</span><span class="token punctuation">:</span>
        input_dtype <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>dtype
        hidden_states <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
        variance <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        hidden_states <span class="token operator">=</span> hidden_states <span class="token operator">*</span> torch<span class="token punctuation">.</span>rsqrt<span class="token punctuation">(</span>variance <span class="token operator">+</span> self<span class="token punctuation">.</span>variance_epsilon<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>weight <span class="token operator">*</span> hidden_states<span class="token punctuation">.</span>to<span class="token punctuation">(</span>input_dtype<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-7-llamasdpaattention" tabindex="-1"><a class="header-anchor" href="#_6-7-llamasdpaattention" aria-hidden="true">#</a> 6.7. LlamaSdpaAttention</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LlamaSdpaAttention</span><span class="token punctuation">(</span>LlamaAttention<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from
    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to
    SDPA API.
    &quot;&quot;&quot;</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> LlamaConfig<span class="token punctuation">,</span> layer_idx<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config
        self<span class="token punctuation">.</span>layer_idx <span class="token operator">=</span> layer_idx
        self<span class="token punctuation">.</span>attention_dropout <span class="token operator">=</span> config<span class="token punctuation">.</span>attention_dropout
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> config<span class="token punctuation">.</span>num_attention_heads
        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> self<span class="token punctuation">.</span>hidden_size <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads
        self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">=</span> config<span class="token punctuation">.</span>num_key_value_heads
        self<span class="token punctuation">.</span>num_key_value_groups <span class="token operator">=</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">//</span> self<span class="token punctuation">.</span>num_key_value_heads
        self<span class="token punctuation">.</span>max_position_embeddings <span class="token operator">=</span> config<span class="token punctuation">.</span>max_position_embeddings
        self<span class="token punctuation">.</span>rope_theta <span class="token operator">=</span> config<span class="token punctuation">.</span>rope_theta
        self<span class="token punctuation">.</span>is_causal <span class="token operator">=</span> <span class="token boolean">True</span>

        self<span class="token punctuation">.</span>q_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>k_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>v_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>o_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>rotary_emb <span class="token operator">=</span> LlamaRotaryEmbedding<span class="token punctuation">(</span>
                self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>
                max_position_embeddings<span class="token operator">=</span>self<span class="token punctuation">.</span>max_position_embeddings<span class="token punctuation">,</span>
                base<span class="token operator">=</span>self<span class="token punctuation">.</span>rope_theta<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

    <span class="token comment"># Adapted from LlamaAttention.forward</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>
        attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        past_key_value<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Cache<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        output_attentions<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        use_cache<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        cache_position<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        

        bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> _ <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>

        query_states <span class="token operator">=</span> self<span class="token punctuation">.</span>q_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
        key_states <span class="token operator">=</span> self<span class="token punctuation">.</span>k_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
        value_states <span class="token operator">=</span> self<span class="token punctuation">.</span>v_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

        query_states <span class="token operator">=</span> query_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        key_states <span class="token operator">=</span> key_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        value_states <span class="token operator">=</span> value_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

        cos<span class="token punctuation">,</span> sin <span class="token operator">=</span> self<span class="token punctuation">.</span>rotary_emb<span class="token punctuation">(</span>value_states<span class="token punctuation">,</span> position_ids<span class="token punctuation">)</span>
        query_states<span class="token punctuation">,</span> key_states <span class="token operator">=</span> apply_rotary_pos_emb<span class="token punctuation">(</span>query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">,</span> cos<span class="token punctuation">,</span> sin<span class="token punctuation">)</span>

        <span class="token keyword">if</span> past_key_value <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token comment"># sin and cos are specific to RoPE models; cache_position needed for the static cache</span>
            cache_kwargs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&quot;sin&quot;</span><span class="token punctuation">:</span> sin<span class="token punctuation">,</span> <span class="token string">&quot;cos&quot;</span><span class="token punctuation">:</span> cos<span class="token punctuation">,</span> <span class="token string">&quot;cache_position&quot;</span><span class="token punctuation">:</span> cache_position<span class="token punctuation">}</span>
            key_states<span class="token punctuation">,</span> value_states <span class="token operator">=</span> past_key_value<span class="token punctuation">.</span>update<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> value_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>layer_idx<span class="token punctuation">,</span> cache_kwargs<span class="token punctuation">)</span>

        key_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_groups<span class="token punctuation">)</span>
        value_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>value_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_groups<span class="token punctuation">)</span>

        causal_mask <span class="token operator">=</span> attention_mask
        <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            causal_mask <span class="token operator">=</span> causal_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span> key_states<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span>

        <span class="token comment"># SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,</span>
        <span class="token comment"># Reference: https://github.com/pytorch/pytorch/issues/112577.</span>
        <span class="token keyword">if</span> query_states<span class="token punctuation">.</span>device<span class="token punctuation">.</span><span class="token builtin">type</span> <span class="token operator">==</span> <span class="token string">&quot;cuda&quot;</span> <span class="token keyword">and</span> causal_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            query_states <span class="token operator">=</span> query_states<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
            key_states <span class="token operator">=</span> key_states<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
            value_states <span class="token operator">=</span> value_states<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># We dispatch to SDPA&#39;s Flash Attention or Efficient kernels via this if statement instead of an</span>
        <span class="token comment"># inline conditional assignment to support both torch.compile&#39;s `dynamic=True` and `fullgraph=True`</span>
        is_causal <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token keyword">if</span> causal_mask <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">and</span> q_len <span class="token operator">&gt;</span> <span class="token number">1</span> <span class="token keyword">else</span> <span class="token boolean">False</span>

        attn_output <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>scaled_dot_product_attention<span class="token punctuation">(</span>
            query_states<span class="token punctuation">,</span>
            key_states<span class="token punctuation">,</span>
            value_states<span class="token punctuation">,</span>
            attn_mask<span class="token operator">=</span>causal_mask<span class="token punctuation">,</span>
            dropout_p<span class="token operator">=</span>self<span class="token punctuation">.</span>attention_dropout <span class="token keyword">if</span> self<span class="token punctuation">.</span>training <span class="token keyword">else</span> <span class="token number">0.0</span><span class="token punctuation">,</span>
            is_causal<span class="token operator">=</span>is_causal<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

        attn_output <span class="token operator">=</span> attn_output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
        attn_output <span class="token operator">=</span> attn_output<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>

        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">(</span>attn_output<span class="token punctuation">)</span>

        <span class="token keyword">return</span> attn_output<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> past_key_value
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-8-llamarotaryembedding" tabindex="-1"><a class="header-anchor" href="#_6-8-llamarotaryembedding" aria-hidden="true">#</a> 6.8. LlamaRotaryEmbedding</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LlamaRotaryEmbedding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> max_position_embeddings<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> base<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> scaling_factor<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>scaling_factor <span class="token operator">=</span> scaling_factor <span class="token comment"># used to adjust the magnitude of positional encoding</span>
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> dim
        self<span class="token punctuation">.</span>max_position_embeddings <span class="token operator">=</span> max_position_embeddings <span class="token comment"># Indicates the maximum sequence length, i.e., the maximum number of positional encodings that the model can handle</span>
        self<span class="token punctuation">.</span>base <span class="token operator">=</span> base <span class="token comment"># used for calculating frequencies</span>
        inv_freq <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>base <span class="token operator">**</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>dim<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># Inverse frequency; the shape of inv_freq is (dim/2), and each element of inv_freq represents how quickly the encoding for that dimension will cycle through the sinusoidal functions. Smaller inv_freq leads to lower frequency (slower cycling), whereas a larger inv_freq results in higher frequency (faster cycling). In this way, the feature vector for any position will be unique, allowing the model to understand and exploit the positional information of elements in the sequence</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">&quot;inv_freq&quot;</span><span class="token punctuation">,</span> inv_freq<span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token comment"># For BC we register cos and sin cached</span>
        self<span class="token punctuation">.</span>max_seq_len_cached <span class="token operator">=</span> max_position_embeddings

    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> position_ids<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># x: [bs, num_attention_heads, seq_len, head_size]</span>
        inv_freq_expanded <span class="token operator">=</span> self<span class="token punctuation">.</span>inv_freq<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>position_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># ï¼ˆ64ï¼‰-&gt;(batch.size, 64, 1)</span>
        position_ids_expanded <span class="token operator">=</span> position_ids<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># (1, seq.len)-&gt;(batch.size, 1, seq.len)</span>
        <span class="token comment"># Force float32 since bfloat16 loses precision on long contexts</span>
        <span class="token comment"># See https://github.com/huggingface/transformers/pull/29285</span>
        device_type <span class="token operator">=</span> x<span class="token punctuation">.</span>device<span class="token punctuation">.</span><span class="token builtin">type</span>
        device_type <span class="token operator">=</span> device_type <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>device_type<span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token keyword">and</span> device_type <span class="token operator">!=</span> <span class="token string">&quot;mps&quot;</span> <span class="token keyword">else</span> <span class="token string">&quot;cpu&quot;</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>autocast<span class="token punctuation">(</span>device_type<span class="token operator">=</span>device_type<span class="token punctuation">,</span> enabled<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            freqs <span class="token operator">=</span> <span class="token punctuation">(</span>inv_freq_expanded<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> @ position_ids_expanded<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment"># (batch.size, 64, 1) * (batch.size, 1, seq.len) -&gt; (batch.size, 64, seq.len) -&gt; transpose(1, 2) -&gt; (batch.size, seq.len, 64)</span>
            emb <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>freqs<span class="token punctuation">,</span> freqs<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># (batch.size, seq.len, 128)</span>
            cos <span class="token operator">=</span> emb<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># (batch.size, seq.len, 128)</span>
            sin <span class="token operator">=</span> emb<span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># (batch.size, seq.len, 128)</span>
        <span class="token keyword">return</span> cos<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> sin<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-9-llamamlp" tabindex="-1"><a class="header-anchor" href="#_6-9-llamamlp" aria-hidden="true">#</a> 6.9. LlamaMLP</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LlamaMLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size
        self<span class="token punctuation">.</span>intermediate_size <span class="token operator">=</span> config<span class="token punctuation">.</span>intermediate_size
        self<span class="token punctuation">.</span>gate_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>mlp_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>mlp_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>down_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>mlp_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act_fn <span class="token operator">=</span> ACT2FN<span class="token punctuation">[</span>config<span class="token punctuation">.</span>hidden_act<span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token builtin">slice</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>intermediate_size <span class="token operator">//</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp
            gate_proj_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
            up_proj_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
            down_proj_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

            gate_proj <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>
                <span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> gate_proj_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>
            <span class="token punctuation">)</span>
            up_proj <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> up_proj_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

            intermediate_states <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>act_fn<span class="token punctuation">(</span>gate_proj<span class="token punctuation">)</span> <span class="token operator">*</span> up_proj<span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
            down_proj <span class="token operator">=</span> <span class="token punctuation">[</span>
                F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>intermediate_states<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> down_proj_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span>
            <span class="token punctuation">]</span>
            down_proj <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>down_proj<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            down_proj <span class="token operator">=</span> self<span class="token punctuation">.</span>down_proj<span class="token punctuation">(</span>self<span class="token punctuation">.</span>act_fn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>gate_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>up_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> down_proj
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><!--[--><!----><!--]--><footer class="page-meta"><!----><div class="meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a aria-label="Transformer Source Code Exploration" class="vp-link nav-link prev nav-link prev" href="/blog/posts/LLM/transformer.html"><div class="hint"><span class="arrow start"></span>Prev</div><div class="link"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Transformer Source Code Exploration</div></a><a aria-label="LLM Summary" class="vp-link nav-link next nav-link next" href="/blog/posts/LLM/llm_summary.html"><div class="hint">Next<span class="arrow end"></span></div><div class="link">LLM Summary<span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span></div></a></nav><!----><!--[--><!----><!--]--><!--]--></main><!--]--><!----></div><!--]--><!--]--><!----><!--]--></div>
    <script type="module" src="/blog/assets/app-7SIUZpS0.js" defer></script>
  </body>
</html>
